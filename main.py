#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Google SSR è®¢é˜…é“¾æ¥æœç´¢å™¨ - ä¸»ç¨‹åºå…¥å£
é›†æˆæ™ºèƒ½å¯†é’¥ç®¡ç†ã€é’‰é’‰é€šçŸ¥ã€URLæå–å’Œå»é‡åŠŸèƒ½
"""

import sys
import os
import json
import time
import logging
from datetime import datetime

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from google_api_scraper_enhanced import EnhancedGoogleAPIScraper
from logger_config import get_scraper_logger

def main():
    """ä¸»ç¨‹åºå…¥å£"""
    print("ğŸš€ Google SSR è®¢é˜…é“¾æ¥æœç´¢å™¨")
    print("=" * 50)
    print(f"â° å¯åŠ¨æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 50)
    
    try:
        # åˆå§‹åŒ–æœç´¢å™¨
        scraper = EnhancedGoogleAPIScraper()
        
        # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
        print(f"ğŸ” æœç´¢æŸ¥è¯¢: {scraper.search_query}")
        print(f"ğŸŒ æ”¯æŒåœ°åŒºæ•°é‡: {len(scraper.regions)} ä¸ªåœ°åŒº (å…¨çƒå¤šè¯­è¨€è¦†ç›–)")
        
        # æ˜¾ç¤ºæ‰¹é‡åœ°åŒºæœç´¢é…ç½®
        regions_config = scraper.config.get('regions', {})
        batch_count = regions_config.get('batch_count', 4)
        inter_region_delay = regions_config.get('inter_region_delay', 15)
        priority_regions = regions_config.get('priority_regions', [])
        use_priority_only = regions_config.get('use_priority_only', False)
        
        print(f"ğŸ“ æœç´¢æ¨¡å¼: {'æ‰¹é‡åœ°åŒºæœç´¢' if batch_count > 1 else 'å•åœ°åŒºæœç´¢'}")
        if batch_count > 1:
            print(f"   æ¯æ¬¡æœç´¢åœ°åŒºæ•°: {batch_count}")
            print(f"   åœ°åŒºé—´å»¶è¿Ÿ: {inter_region_delay} ç§’")
            if use_priority_only and priority_regions:
                print(f"   ä¼˜å…ˆåœ°åŒº: {', '.join(priority_regions)} (ä»…æœç´¢ä¼˜å…ˆåœ°åŒº)")
            elif priority_regions:
                print(f"   ä¼˜å…ˆåœ°åŒº: {', '.join(priority_regions)} (ä¼˜å…ˆä½†ä¸é™åˆ¶)")
        
        print(f"â±ï¸  æœç´¢æ—¶é—´èŒƒå›´: {scraper.config['search']['time_range']}")
        print(f"ğŸ“Š æ¯é¡µæœ€å¤§ç»“æœæ•°: {scraper.config['search']['max_results_per_query']}")
        print(f"ğŸ“„ æœ€å¤§å¤„ç†é¡µé¢æ•°: {scraper.config['search']['max_pages_to_process']}")
        print(f"ğŸ”„ å®šæ—¶ä»»åŠ¡é—´éš”: {scraper.config['schedule']['interval_hours']} å°æ—¶")
        print(f"ğŸ”” é’‰é’‰é€šçŸ¥: {'å¯ç”¨' if scraper.config['validation']['send_notifications'] else 'ç¦ç”¨'}")
        print(f"ğŸŒ ä»£ç†è®¾ç½®: {'å¯ç”¨' if scraper.config['proxy']['enabled'] else 'ç¦ç”¨'}")
        if scraper.config['proxy']['enabled']:
            proxy_url = f"{scraper.config['proxy']['protocol']}://{scraper.config['proxy']['host']}:{scraper.config['proxy']['port']}"
            print(f"   ä»£ç†åœ°å€: {proxy_url}")
        print("=" * 50)
        
        # å¯åŠ¨å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨
        scraper.start_scheduler()
        
    except KeyboardInterrupt:
        print("\nğŸ›‘ æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨åœæ­¢...")
        sys.exit(0)
    except Exception as e:
        print(f"âŒ ç¨‹åºå¯åŠ¨å¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
