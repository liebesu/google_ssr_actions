#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Google SSR è®¢é˜…é“¾æ¥æœç´¢å™¨ - ä¸»ç¨‹åºå…¥å£
é›†æˆæ™ºèƒ½å¯†é’¥ç®¡ç†ã€é’‰é’‰é€šçŸ¥ã€URLæå–å’Œå»é‡åŠŸèƒ½
"""

import sys
import os
import json
import time
import logging
from datetime import datetime

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from google_api_scraper_enhanced import EnhancedGoogleAPIScraper
from logger_config import get_scraper_logger
from config import config
from error_handler import ErrorHandler, safe_execute

def main():
    """ä¸»ç¨‹åºå…¥å£"""
    print("ğŸš€ Google SSR è®¢é˜…é“¾æ¥æœç´¢å™¨")
    print("=" * 50)
    print(f"â° å¯åŠ¨æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 50)
    
    try:
        # åˆå§‹åŒ–é”™è¯¯å¤„ç†å™¨
        error_handler = ErrorHandler()
        
        # å®‰å…¨åˆå§‹åŒ–æœç´¢å™¨
        scraper = safe_execute(
            EnhancedGoogleAPIScraper,
            default_return=None,
            logger=logging.getLogger(__name__)
        )
        
        if scraper is None:
            print("âŒ æœç´¢å™¨åˆå§‹åŒ–å¤±è´¥")
            sys.exit(1)
        
        # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
        print(f"ğŸ” æœç´¢æŸ¥è¯¢: {scraper.search_query}")
        print(f"ğŸŒ æ”¯æŒåœ°åŒºæ•°é‡: {len(scraper.regions)} ä¸ªåœ°åŒº (å…¨çƒå¤šè¯­è¨€è¦†ç›–)")
        
        # ä½¿ç”¨ç»Ÿä¸€é…ç½®ç®¡ç†å™¨è·å–é…ç½®ä¿¡æ¯
        batch_count = config.get('regions.batch_count', 4)
        inter_region_delay = config.get('regions.inter_region_delay', 15)
        priority_regions = config.get('regions.priority_regions', [])
        use_priority_only = config.get('regions.use_priority_only', False)
        
        print(f"ğŸ“ æœç´¢æ¨¡å¼: {'æ‰¹é‡åœ°åŒºæœç´¢' if batch_count > 1 else 'å•åœ°åŒºæœç´¢'}")
        if batch_count > 1:
            print(f"   æ¯æ¬¡æœç´¢åœ°åŒºæ•°: {batch_count}")
            print(f"   åœ°åŒºé—´å»¶è¿Ÿ: {inter_region_delay} ç§’")
            if use_priority_only and priority_regions:
                print(f"   ä¼˜å…ˆåœ°åŒº: {', '.join(priority_regions)} (ä»…æœç´¢ä¼˜å…ˆåœ°åŒº)")
            elif priority_regions:
                print(f"   ä¼˜å…ˆåœ°åŒº: {', '.join(priority_regions)} (ä¼˜å…ˆä½†ä¸é™åˆ¶)")
        
        print(f"â±ï¸  æœç´¢æ—¶é—´èŒƒå›´: {config.get('search.time_range', 'past_24_hours')}")
        print(f"ğŸ“Š æ¯é¡µæœ€å¤§ç»“æœæ•°: {config.get('search.max_results_per_query', 100)}")
        print(f"ğŸ“„ æœ€å¤§å¤„ç†é¡µé¢æ•°: {config.get('search.max_pages_to_process', 30)}")
        print(f"ğŸ”„ å®šæ—¶ä»»åŠ¡é—´éš”: {config.get('schedule.interval_hours', 2)} å°æ—¶")
        print(f"ğŸ”” é’‰é’‰é€šçŸ¥: {'å¯ç”¨' if config.get('validation.send_notifications', True) else 'ç¦ç”¨'}")
        print(f"ğŸŒ ä»£ç†è®¾ç½®: {'å¯ç”¨' if config.is_proxy_enabled() else 'ç¦ç”¨'}")
        
        proxy_config = config.get_proxy_config()
        if proxy_config:
            print(f"   ä»£ç†åœ°å€: {proxy_config.get('http', 'N/A')}")
        print("=" * 50)
        
        # å¯åŠ¨å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨
        scraper.start_scheduler()
        
    except KeyboardInterrupt:
        print("\nğŸ›‘ æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨åœæ­¢...")
        sys.exit(0)
    except Exception as e:
        print(f"âŒ ç¨‹åºå¯åŠ¨å¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
