#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®æ¸…ç†å·¥å…·
æ¸…ç†å’ŒéªŒè¯é¡¹ç›®ä¸­çš„URLæ•°æ®æ–‡ä»¶
"""

import json
import os
import re
from typing import List, Set, Dict
from urllib.parse import urlparse


class DataCleaner:
    """æ•°æ®æ¸…ç†å™¨"""
    
    def __init__(self):
        self.valid_url_pattern = re.compile(r'^https?://[^\s"\'<>]+api/v1/client/subscribe\?token=[A-Za-z0-9]+')
        self.placeholder_patterns = [
            r'your-provider\.com',
            r'xxxxx',
            r'xxxxxxxx',
            r'example\.com',
            r'test\.com'
        ]
    
    def is_valid_subscription_url(self, url: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆçš„è®¢é˜…URL"""
        if not isinstance(url, str):
            return False
        
        # åŸºæœ¬æ ¼å¼æ£€æŸ¥
        if not self.valid_url_pattern.match(url):
            return False
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºå ä½ç¬¦URL
        for pattern in self.placeholder_patterns:
            if re.search(pattern, url, re.IGNORECASE):
                return False
        
        # URLè§£ææ£€æŸ¥
        try:
            parsed = urlparse(url)
            if not parsed.scheme or not parsed.netloc:
                return False
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦çš„tokenå‚æ•°
            if 'token=' not in parsed.query:
                return False
            
            return True
        except Exception:
            return False
    
    def normalize_url(self, url: str) -> str:
        """æ ‡å‡†åŒ–URL"""
        # ç§»é™¤å°¾éƒ¨çš„é¢å¤–ä¿¡æ¯
        url = re.sub(r'è®¢é˜…æµé‡ï¼š[^&]*', '', url)
        url = re.sub(r'æ€»æµé‡:[^&]*', '', url)
        url = re.sub(r'å‰©ä½™æµé‡:[^&]*', '', url)
        url = re.sub(r'å·²ä¸Šä¼ :[^&]*', '', url)
        url = re.sub(r'å·²ä¸‹è½½:[^&]*', '', url)
        url = re.sub(r'è¯¥è®¢é˜…å°†äº[^&]*', '', url)
        
        return url.strip()
    
    def clean_url_list(self, urls: List) -> List[str]:
        """æ¸…ç†URLåˆ—è¡¨"""
        clean_urls = set()
        
        for item in urls:
            # è·³è¿‡éå­—ç¬¦ä¸²é¡¹
            if not isinstance(item, str):
                continue
            
            # è·³è¿‡æ˜æ˜¾çš„éURLé¡¹
            if not item.startswith(('http://', 'https://')):
                continue
            
            # æ ‡å‡†åŒ–URL
            normalized_url = self.normalize_url(item)
            
            # éªŒè¯URL
            if self.is_valid_subscription_url(normalized_url):
                clean_urls.add(normalized_url)
        
        return sorted(list(clean_urls))
    
    def clean_discovered_urls_file(self, file_path: str) -> Dict:
        """æ¸…ç†discovered_urls.jsonæ–‡ä»¶"""
        print(f"æ­£åœ¨æ¸…ç†æ–‡ä»¶: {file_path}")
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            original_count = len(data)
            print(f"åŸå§‹æ¡ç›®æ•°: {original_count}")
            
            # æ¸…ç†æ•°æ®
            clean_urls = self.clean_url_list(data)
            
            print(f"æ¸…ç†åURLæ•°: {len(clean_urls)}")
            print(f"ç§»é™¤æ¡ç›®æ•°: {original_count - len(clean_urls)}")
            
            # å¤‡ä»½åŸæ–‡ä»¶
            backup_path = file_path + '.backup'
            with open(backup_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"åŸæ–‡ä»¶å·²å¤‡ä»½åˆ°: {backup_path}")
            
            # å†™å…¥æ¸…ç†åçš„æ•°æ®
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(clean_urls, f, ensure_ascii=False, indent=2)
            
            return {
                'original_count': original_count,
                'clean_count': len(clean_urls),
                'removed_count': original_count - len(clean_urls),
                'backup_path': backup_path
            }
            
        except Exception as e:
            print(f"æ¸…ç†æ–‡ä»¶å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def validate_data_files(self, base_dir: str = '.') -> Dict:
        """éªŒè¯é¡¹ç›®ä¸­çš„æ•°æ®æ–‡ä»¶"""
        results = {}
        
        files_to_check = [
            'discovered_urls.json',
            'api_urls_results.json',
            'data/history_urls.json',
            'data/live_urls.json'
        ]
        
        for file_path in files_to_check:
            full_path = os.path.join(base_dir, file_path)
            if os.path.exists(full_path):
                print(f"\næ£€æŸ¥æ–‡ä»¶: {file_path}")
                try:
                    with open(full_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    if isinstance(data, list):
                        valid_urls = [item for item in data if isinstance(item, str) and self.is_valid_subscription_url(item)]
                        invalid_items = [item for item in data if not (isinstance(item, str) and self.is_valid_subscription_url(item))]
                        
                        results[file_path] = {
                            'total_items': len(data),
                            'valid_urls': len(valid_urls),
                            'invalid_items': len(invalid_items),
                            'invalid_examples': invalid_items[:5] if invalid_items else []
                        }
                        
                        print(f"  æ€»æ¡ç›®: {len(data)}")
                        print(f"  æœ‰æ•ˆURL: {len(valid_urls)}")
                        print(f"  æ— æ•ˆæ¡ç›®: {len(invalid_items)}")
                        if invalid_items:
                            print(f"  æ— æ•ˆç¤ºä¾‹: {invalid_items[:3]}")
                    
                    elif isinstance(data, dict):
                        # å¤„ç†å­—å…¸æ ¼å¼çš„æ–‡ä»¶ï¼ˆå¦‚api_urls_results.jsonï¼‰
                        urls = data.get('urls', [])
                        if urls:
                            valid_urls = [url for url in urls if self.is_valid_subscription_url(url)]
                            results[file_path] = {
                                'total_items': len(urls),
                                'valid_urls': len(valid_urls),
                                'invalid_items': len(urls) - len(valid_urls)
                            }
                            print(f"  æ€»URL: {len(urls)}")
                            print(f"  æœ‰æ•ˆURL: {len(valid_urls)}")
                        else:
                            results[file_path] = {'note': 'No URLs found in dict'}
                    
                except Exception as e:
                    results[file_path] = {'error': str(e)}
                    print(f"  é”™è¯¯: {e}")
            else:
                results[file_path] = {'status': 'file_not_found'}
                print(f"  æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        return results


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§¹ æ•°æ®æ¸…ç†å·¥å…·")
    print("=" * 50)
    
    cleaner = DataCleaner()
    
    # éªŒè¯æ•°æ®æ–‡ä»¶
    print("\nğŸ“Š éªŒè¯æ•°æ®æ–‡ä»¶...")
    results = cleaner.validate_data_files()
    
    # æ¸…ç†discovered_urls.json
    print("\nğŸ”§ æ¸…ç†discovered_urls.json...")
    if os.path.exists('discovered_urls.json'):
        clean_result = cleaner.clean_discovered_urls_file('discovered_urls.json')
        if 'error' not in clean_result:
            print("âœ… æ¸…ç†å®Œæˆ")
            print(f"   åŸå§‹æ¡ç›®: {clean_result['original_count']}")
            print(f"   æ¸…ç†å: {clean_result['clean_count']}")
            print(f"   ç§»é™¤: {clean_result['removed_count']}")
        else:
            print(f"âŒ æ¸…ç†å¤±è´¥: {clean_result['error']}")
    else:
        print("âš ï¸ discovered_urls.json æ–‡ä»¶ä¸å­˜åœ¨")
    
    print("\nâœ… æ•°æ®æ¸…ç†å®Œæˆ")


if __name__ == "__main__":
    main()

