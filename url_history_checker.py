#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è®¢é˜…URLå†å²æ£€æŸ¥å™¨
ç”¨äºæ£€æŸ¥è®¢é˜…URLæ˜¯å¦åœ¨å†å²è®°å½•ä¸­å­˜åœ¨è¿‡
"""

import json
import re
import os
from typing import Set, List, Dict, Optional


class URLHistoryChecker:
    """è®¢é˜…URLå†å²æ£€æŸ¥å™¨"""
    
    def __init__(self, discovered_urls_file: str = 'discovered_urls.json', 
                 notified_urls_file: str = 'notified_urls.txt'):
        """
        åˆå§‹åŒ–å†å²æ£€æŸ¥å™¨
        
        Args:
            discovered_urls_file: å·²å‘ç°URLæ–‡ä»¶è·¯å¾„
            notified_urls_file: å·²é€šçŸ¥URLæ–‡ä»¶è·¯å¾„
        """
        self.discovered_urls_file = discovered_urls_file
        self.notified_urls_file = notified_urls_file
        self.discovered_urls = self.load_discovered_urls()
        self.notified_urls = self.load_notified_urls()
        
        # ä¸ºäº†æé«˜æŸ¥è¯¢æ•ˆç‡ï¼Œé¢„å¤„ç†åŸºç¡€URLæ˜ å°„
        self.base_url_mapping = self._build_base_url_mapping()
        
    def load_discovered_urls(self) -> Set[str]:
        """åŠ è½½å·²å‘ç°çš„è®¢é˜…é“¾æ¥"""
        try:
            if os.path.exists(self.discovered_urls_file):
                with open(self.discovered_urls_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    # è¿‡æ»¤æ‰éå­—ç¬¦ä¸²é¡¹ï¼ˆå¦‚æ—¶é—´æˆ³ï¼‰
                    return set([url for url in data if isinstance(url, str) and url.startswith('http')])
            return set()
        except Exception as e:
            print(f"âŒ åŠ è½½å·²å‘ç°URLå¤±è´¥: {e}")
            return set()
    
    def load_notified_urls(self) -> Set[str]:
        """åŠ è½½å·²é€šçŸ¥çš„è®¢é˜…é“¾æ¥"""
        try:
            if os.path.exists(self.notified_urls_file):
                with open(self.notified_urls_file, 'r', encoding='utf-8') as f:
                    return set([line.strip() for line in f if line.strip() and line.strip().startswith('http')])
            return set()
        except Exception as e:
            print(f"âŒ åŠ è½½å·²é€šçŸ¥URLå¤±è´¥: {e}")
            return set()
    
    def extract_base_subscription_url(self, url: str) -> str:
        """
        æå–è®¢é˜…URLçš„åŸºç¡€éƒ¨åˆ†ï¼Œç”¨äºå»é‡æ¯”è¾ƒ
        ç§»é™¤æµé‡ä¿¡æ¯ã€é¢å¤–å‚æ•°ç­‰ï¼Œåªä¿ç•™æ ¸å¿ƒçš„è®¢é˜…åœ°å€
        """
        # ç§»é™¤æµé‡ä¿¡æ¯ç­‰é¢å¤–æ–‡æœ¬
        url = re.sub(r'è®¢é˜…æµé‡ï¼š[^&]*', '', url)
        url = re.sub(r'æ€»æµé‡:[^&]*', '', url)
        url = re.sub(r'å‰©ä½™æµé‡:[^&]*', '', url)
        url = re.sub(r'å·²ä¸Šä¼ :[^&]*', '', url)
        url = re.sub(r'å·²ä¸‹è½½:[^&]*', '', url)
        url = re.sub(r'è¯¥è®¢é˜…å°†äº[^&]*', '', url)
        
        # åˆ†ç¦»åŸºç¡€URLå’Œå‚æ•°
        if '?' in url:
            base_part, params = url.split('?', 1)
            # åªä¿ç•™tokenå‚æ•°
            if 'token=' in params:
                token_match = re.search(r'token=([^&]+)', params)
                if token_match:
                    return f"{base_part}?token={token_match.group(1)}"
        
        return url.strip()
    
    def _build_base_url_mapping(self) -> Dict[str, str]:
        """æ„å»ºåŸºç¡€URLåˆ°åŸå§‹URLçš„æ˜ å°„ï¼Œç”¨äºå¿«é€ŸæŸ¥è¯¢"""
        mapping = {}
        
        # å¤„ç†å·²å‘ç°çš„URL
        for url in self.discovered_urls:
            base_url = self.extract_base_subscription_url(url)
            if base_url not in mapping:
                mapping[base_url] = url
        
        # å¤„ç†å·²é€šçŸ¥çš„URL  
        for url in self.notified_urls:
            base_url = self.extract_base_subscription_url(url)
            if base_url not in mapping:
                mapping[base_url] = url
                
        return mapping
    
    def check_url_exists(self, url: str) -> Dict[str, any]:
        """
        æ£€æŸ¥URLæ˜¯å¦åœ¨å†å²ä¸­å­˜åœ¨
        
        Args:
            url: è¦æ£€æŸ¥çš„è®¢é˜…URL
            
        Returns:
            dict: æ£€æŸ¥ç»“æœ
                - exists: æ˜¯å¦å­˜åœ¨
                - in_discovered: æ˜¯å¦åœ¨å·²å‘ç°åˆ—è¡¨ä¸­
                - in_notified: æ˜¯å¦åœ¨å·²é€šçŸ¥åˆ—è¡¨ä¸­
                - base_url: åŸºç¡€URL
                - matched_url: åŒ¹é…åˆ°çš„å†å²URLï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        """
        base_url = self.extract_base_subscription_url(url)
        
        # æ£€æŸ¥æ˜¯å¦åœ¨åŸºç¡€URLæ˜ å°„ä¸­
        exists_in_base = base_url in self.base_url_mapping
        matched_url = self.base_url_mapping.get(base_url)
        
        # è¯¦ç»†æ£€æŸ¥åœ¨å“ªä¸ªåˆ—è¡¨ä¸­
        in_discovered = any(
            self.extract_base_subscription_url(discovered_url) == base_url 
            for discovered_url in self.discovered_urls
        )
        
        in_notified = any(
            self.extract_base_subscription_url(notified_url) == base_url 
            for notified_url in self.notified_urls
        )
        
        return {
            'exists': exists_in_base or in_discovered or in_notified,
            'in_discovered': in_discovered,
            'in_notified': in_notified,
            'base_url': base_url,
            'matched_url': matched_url,
            'original_url': url
        }
    
    def check_multiple_urls(self, urls: List[str]) -> List[Dict[str, any]]:
        """
        æ‰¹é‡æ£€æŸ¥å¤šä¸ªURL
        
        Args:
            urls: URLåˆ—è¡¨
            
        Returns:
            List[dict]: æ£€æŸ¥ç»“æœåˆ—è¡¨
        """
        return [self.check_url_exists(url) for url in urls]
    
    def get_statistics(self) -> Dict[str, int]:
        """è·å–å†å²è®°å½•ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'discovered_count': len(self.discovered_urls),
            'notified_count': len(self.notified_urls),
            'unique_base_urls': len(self.base_url_mapping),
            'total_unique_urls': len(self.discovered_urls | self.notified_urls)
        }
    
    def find_similar_urls(self, url: str) -> List[str]:
        """
        æŸ¥æ‰¾ç›¸ä¼¼çš„URLï¼ˆç›¸åŒåŸºç¡€URLä½†å‚æ•°ä¸åŒï¼‰
        
        Args:
            url: æŸ¥è¯¢URL
            
        Returns:
            List[str]: ç›¸ä¼¼URLåˆ—è¡¨
        """
        base_url = self.extract_base_subscription_url(url)
        similar_urls = []
        
        # åœ¨å·²å‘ç°URLä¸­æŸ¥æ‰¾
        for discovered_url in self.discovered_urls:
            if self.extract_base_subscription_url(discovered_url) == base_url:
                similar_urls.append(discovered_url)
        
        # åœ¨å·²é€šçŸ¥URLä¸­æŸ¥æ‰¾
        for notified_url in self.notified_urls:
            if self.extract_base_subscription_url(notified_url) == base_url:
                similar_urls.append(notified_url)
        
        return list(set(similar_urls))


def main():
    """ä¸»å‡½æ•° - äº¤äº’å¼å‘½ä»¤è¡Œå·¥å…·"""
    print("ğŸ” è®¢é˜…URLå†å²æ£€æŸ¥å™¨")
    print("=" * 50)
    
    # åˆå§‹åŒ–æ£€æŸ¥å™¨
    try:
        checker = URLHistoryChecker()
        stats = checker.get_statistics()
        print(f"ğŸ“Š å†å²è®°å½•ç»Ÿè®¡:")
        print(f"   å·²å‘ç°URLæ•°é‡: {stats['discovered_count']}")
        print(f"   å·²é€šçŸ¥URLæ•°é‡: {stats['notified_count']}")
        print(f"   å”¯ä¸€åŸºç¡€URLæ•°: {stats['unique_base_urls']}")
        print(f"   æ€»å”¯ä¸€URLæ•°: {stats['total_unique_urls']}")
        print("=" * 50)
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        return
    
    while True:
        print("\nè¯·é€‰æ‹©æ“ä½œ:")
        print("1. æ£€æŸ¥å•ä¸ªURL")
        print("2. æ‰¹é‡æ£€æŸ¥URL")
        print("3. æŸ¥æ‰¾ç›¸ä¼¼URL")
        print("4. æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯")
        print("5. é€€å‡º")
        
        choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1-5): ").strip()
        
        if choice == '1':
            # æ£€æŸ¥å•ä¸ªURL
            url = input("è¯·è¾“å…¥è¦æ£€æŸ¥çš„è®¢é˜…URL: ").strip()
            if url:
                result = checker.check_url_exists(url)
                print(f"\nğŸ“‹ æ£€æŸ¥ç»“æœ:")
                print(f"   åŸå§‹URL: {result['original_url']}")
                print(f"   åŸºç¡€URL: {result['base_url']}")
                print(f"   æ˜¯å¦å­˜åœ¨: {'âœ… æ˜¯' if result['exists'] else 'âŒ å¦'}")
                
                if result['exists']:
                    print(f"   åœ¨å·²å‘ç°åˆ—è¡¨: {'âœ… æ˜¯' if result['in_discovered'] else 'âŒ å¦'}")
                    print(f"   åœ¨å·²é€šçŸ¥åˆ—è¡¨: {'âœ… æ˜¯' if result['in_notified'] else 'âŒ å¦'}")
                    if result['matched_url']:
                        print(f"   åŒ¹é…çš„URL: {result['matched_url']}")
                else:
                    print("   ğŸ†• è¿™æ˜¯ä¸€ä¸ªæ–°çš„è®¢é˜…URL")
        
        elif choice == '2':
            # æ‰¹é‡æ£€æŸ¥URL
            print("è¯·è¾“å…¥å¤šä¸ªURLï¼Œæ¯è¡Œä¸€ä¸ªï¼Œè¾“å…¥ç©ºè¡Œç»“æŸ:")
            urls = []
            while True:
                url = input().strip()
                if not url:
                    break
                urls.append(url)
            
            if urls:
                results = checker.check_multiple_urls(urls)
                print(f"\nğŸ“‹ æ‰¹é‡æ£€æŸ¥ç»“æœ (å…±{len(urls)}ä¸ªURL):")
                print("-" * 80)
                
                existing_count = 0
                for i, result in enumerate(results, 1):
                    status = "âœ… å­˜åœ¨" if result['exists'] else "ğŸ†• æ–°URL"
                    if result['exists']:
                        existing_count += 1
                    print(f"{i:2d}. {status} - {result['original_url'][:60]}...")
                
                print("-" * 80)
                print(f"ç»Ÿè®¡: {existing_count}/{len(urls)} ä¸ªURLå·²å­˜åœ¨, {len(urls)-existing_count} ä¸ªæ–°URL")
        
        elif choice == '3':
            # æŸ¥æ‰¾ç›¸ä¼¼URL
            url = input("è¯·è¾“å…¥è¦æŸ¥æ‰¾ç›¸ä¼¼URLçš„è®¢é˜…åœ°å€: ").strip()
            if url:
                similar_urls = checker.find_similar_urls(url)
                print(f"\nğŸ“‹ ç›¸ä¼¼URLæŸ¥æ‰¾ç»“æœ:")
                print(f"   æŸ¥è¯¢URL: {url}")
                print(f"   åŸºç¡€URL: {checker.extract_base_subscription_url(url)}")
                
                if similar_urls:
                    print(f"   æ‰¾åˆ° {len(similar_urls)} ä¸ªç›¸ä¼¼URL:")
                    for i, similar_url in enumerate(similar_urls, 1):
                        print(f"     {i}. {similar_url}")
                else:
                    print("   ğŸ†• æ²¡æœ‰æ‰¾åˆ°ç›¸ä¼¼çš„URL")
        
        elif choice == '4':
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            stats = checker.get_statistics()
            print(f"\nğŸ“Š è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯:")
            print(f"   å·²å‘ç°URLæ•°é‡: {stats['discovered_count']}")
            print(f"   å·²é€šçŸ¥URLæ•°é‡: {stats['notified_count']}")
            print(f"   å”¯ä¸€åŸºç¡€URLæ•°: {stats['unique_base_urls']}")
            print(f"   æ€»å”¯ä¸€URLæ•°: {stats['total_unique_urls']}")
            
            # è®¡ç®—é‡å¤ç‡
            total_urls = stats['discovered_count'] + stats['notified_count']
            if total_urls > 0:
                duplicate_rate = (total_urls - stats['total_unique_urls']) / total_urls * 100
                print(f"   é‡å¤ç‡: {duplicate_rate:.1f}%")
        
        elif choice == '5':
            print("ğŸ‘‹ å†è§ï¼")
            break
        
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")


# ä¾¿æ·å‡½æ•°ï¼Œå¯ä»¥è¢«å…¶ä»–è„šæœ¬å¯¼å…¥ä½¿ç”¨
def quick_check(url: str, discovered_file: str = 'discovered_urls.json', 
                notified_file: str = 'notified_urls.txt') -> bool:
    """
    å¿«é€Ÿæ£€æŸ¥URLæ˜¯å¦å­˜åœ¨
    
    Args:
        url: è¦æ£€æŸ¥çš„URL
        discovered_file: å·²å‘ç°URLæ–‡ä»¶
        notified_file: å·²é€šçŸ¥URLæ–‡ä»¶
        
    Returns:
        bool: æ˜¯å¦å­˜åœ¨
    """
    checker = URLHistoryChecker(discovered_file, notified_file)
    result = checker.check_url_exists(url)
    return result['exists']


if __name__ == "__main__":
    main()
