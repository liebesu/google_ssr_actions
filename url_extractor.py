#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
URLæå–å’Œæ¸…ç†å·¥å…·
ä¸“é—¨å¤„ç†åŒ…å«HTMLæ ‡ç­¾çš„ä¼ªURLï¼Œæå–çœŸæ­£çš„è®¢é˜…é“¾æ¥
"""

import re
import urllib.parse
from urllib.parse import urlparse, unquote
from typing import List, Set
import logging
import html

class URLExtractor:
    """URLæå–å’Œæ¸…ç†å™¨"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        # è®¢é˜…é“¾æ¥çš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
        self.subscription_patterns = [
            # æ ‡å‡†æ ¼å¼ï¼šhttps://domain.com/api/v1/client/subscribe?token=xxx
            r'https?://[^\s"\'<>]+api/v1/client/subscribe\?token=[A-Za-z0-9]+',
            # åŒ…å«HTMLæ ‡ç­¾çš„æ ¼å¼
            r'<code>https?://[^\s"\'<>]+api/v1/client/subscribe\?token=[A-Za-z0-9]+</code>',
            # åŒ…å«å¼•å·çš„æ ¼å¼
            r'["\']https?://[^\s"\'<>]+api/v1/client/subscribe\?token=[A-Za-z0-9]+["\']',
            # æ›´å®½æ¾çš„åŒ¹é…ï¼ŒåŒ…å«å¯èƒ½çš„é¢å¤–å‚æ•°ï¼Œä½†é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
            r'https?://[^\s"\'<>]+api/v1/client/subscribe\?token=[A-Za-z0-9]+(?:&[^=\s"\'<>]*=[^=\s"\'<>]*)*',
            # ä¸“é—¨å¤„ç†åŒ…å«flagå‚æ•°çš„URL
            r'https?://[^\s"\'<>]+api/v1/client/subscribe\?token=[A-Za-z0-9]+&flag=[A-Za-z0-9]+',
            
            # æ–°å¢ï¼šå…¶ä»–å¸¸è§è®¢é˜…æ ¼å¼
            r'https?://[^\s"\'<>]+/subscribe/link\?token=[A-Za-z0-9]+',
            r'https?://[^\s"\'<>]+/getSubscribe\?token=[A-Za-z0-9]+',
            r'https?://[^\s"\'<>]+/sub\?target=[A-Za-z0-9]+&url=[^\s"\'<>]+',
            r'https?://[^\s"\'<>]+/link/[A-Za-z0-9]+(?:\?[^\s"\'<>]*)?',
            r'https?://[^\s"\'<>]+/s/[A-Za-z0-9]+',
            
            # Base64è®¢é˜…é“¾æ¥
            r'(?:vmess|vless|trojan|ss|ssr|hysteria2?)://[A-Za-z0-9+/=]+',
            
            # çŸ­é“¾æ¥æœåŠ¡
            r'https?://(?:bit\.ly|goo\.gl|tinyurl\.com|t\.co|short\.link)/[A-Za-z0-9]+',
        ]
        
        # éœ€è¦æ¸…ç†çš„HTMLæ ‡ç­¾å’Œå±æ€§
        self.html_cleanup_patterns = [
            r'<[^>]+>',  # ç§»é™¤æ‰€æœ‰HTMLæ ‡ç­¾
            r'&[a-zA-Z0-9#]+;',  # ç§»é™¤HTMLå®ä½“
            r'%[0-9A-Fa-f]{2}',  # ç§»é™¤URLç¼–ç 
        ]
    
    def extract_subscription_urls(self, text: str) -> List[str]:
        """
        ä»æ–‡æœ¬ä¸­æå–æ‰€æœ‰è®¢é˜…é“¾æ¥
        
        Args:
            text: åŒ…å«è®¢é˜…é“¾æ¥çš„æ–‡æœ¬
            
        Returns:
            List[str]: æå–åˆ°çš„è®¢é˜…é“¾æ¥åˆ—è¡¨
        """
        urls = set()
        
        # ä½¿ç”¨å¤šç§æ¨¡å¼åŒ¹é…
        for pattern in self.subscription_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                # æ¸…ç†HTMLæ ‡ç­¾
                clean_url = self.clean_html_tags(match)
                # è¿›ä¸€æ­¥æ¸…ç†å’ŒéªŒè¯
                clean_url = self.clean_and_validate_url(clean_url)
                if clean_url:
                    urls.add(clean_url)
        
        return list(urls)
    
    def clean_html_tags(self, text: str) -> str:
        """
        æ¸…ç†HTMLæ ‡ç­¾å’Œå®ä½“
        
        Args:
            text: åŒ…å«HTMLçš„æ–‡æœ¬
            
        Returns:
            str: æ¸…ç†åçš„æ–‡æœ¬
        """
        # å…ˆå¤„ç†åµŒå¥—çš„HTMLå®ä½“ç¼–ç ï¼ˆå¦‚ &amp;amp; -> &ï¼‰
        while '&amp;' in text:
            text = text.replace('&amp;', '&')
        
        # å†ä½¿ç”¨html.unescapeå¤„ç†å…¶ä»–HTMLå®ä½“
        text = html.unescape(text)
        
        # å†ä½¿ç”¨unquoteå¤„ç†URLç¼–ç 
        text = unquote(text)
        
        # ç§»é™¤HTMLæ ‡ç­¾
        for pattern in self.html_cleanup_patterns:
            text = re.sub(pattern, '', text)
        
        return text.strip()
    
    def clean_and_validate_url(self, url: str) -> str:
        """
        æ¸…ç†å’ŒéªŒè¯URL
        
        Args:
            url: åŸå§‹URL
            
        Returns:
            str: æ¸…ç†åçš„æœ‰æ•ˆURLï¼Œå¦‚æœæ— æ•ˆåˆ™è¿”å›None
        """
        try:
            # ç§»é™¤å‰åç©ºç™½å­—ç¬¦
            url = url.strip()
            
            # ç§»é™¤å¯èƒ½çš„HTMLæ ‡ç­¾æ®‹ç•™
            url = re.sub(r'<[^>]*>', '', url)
            
            # ç§»é™¤å¯èƒ½çš„å¼•å·
            url = url.strip('"\'')
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦çš„éƒ¨åˆ†
            if 'api/v1/client/subscribe?token=' not in url:
                return None
            
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–çº¯URLéƒ¨åˆ†ï¼Œå»é™¤åé¢çš„é¢å¤–æ–‡æœ¬
            # åŒ¹é…åˆ°ç¬¬ä¸€ä¸ªä¸­æ–‡å­—ç¬¦æˆ–ç©ºæ ¼ä¹‹å‰ï¼Œä½†å…è®¸&å‚æ•°
            url_match = re.match(r'(https?://[^\s"\'<>]+api/v1/client/subscribe\?token=[A-Za-z0-9]+(?:&[^ä¸€-é¾¯\s]*)?)', url)
            if url_match:
                url = url_match.group(1)
            else:
                # å¦‚æœæ²¡æœ‰å‚æ•°ï¼ŒåŒ¹é…åŸºæœ¬URL
                url_match = re.match(r'(https?://[^\s"\'<>]+api/v1/client/subscribe\?token=[A-Za-z0-9]+)', url)
                if url_match:
                    url = url_match.group(1)
            
            # è§£æURL
            parsed = urlparse(url)
            
            # éªŒè¯URLæ ¼å¼
            if not parsed.scheme or not parsed.netloc:
                return None
            
            # ç¡®ä¿æ˜¯httpæˆ–https
            if parsed.scheme not in ['http', 'https']:
                return None
            
            # é‡æ–°æ„å»ºURLï¼Œç¡®ä¿æ ¼å¼æ­£ç¡®
            clean_url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
            if parsed.query:
                clean_url += f"?{parsed.query}"
            
            return clean_url
            
        except Exception as e:
            self.logger.debug(f"URLæ¸…ç†å¤±è´¥: {url} - {e}")
            return None
    
    def extract_from_search_results(self, search_results: List[dict]) -> List[str]:
        """
        ä»æœç´¢ç»“æœä¸­æå–è®¢é˜…é“¾æ¥
        
        Args:
            search_results: æœç´¢ç»“æœåˆ—è¡¨
            
        Returns:
            List[str]: æå–åˆ°çš„è®¢é˜…é“¾æ¥åˆ—è¡¨
        """
        all_urls = set()
        
        for result in search_results:
            # ä»é“¾æ¥å­—æ®µæå–
            link = result.get('link', '')
            if link:
                urls = self.extract_subscription_urls(link)
                all_urls.update(urls)
            
            # ä»æ ‡é¢˜å­—æ®µæå–
            title = result.get('title', '')
            if title:
                urls = self.extract_subscription_urls(title)
                all_urls.update(urls)
            
            # ä»æ‘˜è¦å­—æ®µæå–
            snippet = result.get('snippet', '')
            if snippet:
                urls = self.extract_subscription_urls(snippet)
                all_urls.update(urls)
        
        return list(all_urls)
    
    def process_mixed_urls(self, urls: List[str]) -> List[str]:
        """
        å¤„ç†æ··åˆçš„URLåˆ—è¡¨ï¼ˆåŒ…å«çœŸå®URLå’Œä¼ªURLï¼‰
        
        Args:
            urls: æ··åˆçš„URLåˆ—è¡¨
            
        Returns:
            List[str]: æ¸…ç†åçš„çœŸå®URLåˆ—è¡¨
        """
        clean_urls = set()
        
        for url in urls:
            # å¦‚æœå·²ç»æ˜¯å¹²å‡€çš„URLï¼Œç›´æ¥æ·»åŠ 
            if self.is_clean_url(url):
                clean_urls.add(url)
            else:
                # å°è¯•æå–è®¢é˜…é“¾æ¥
                extracted = self.extract_subscription_urls(url)
                clean_urls.update(extracted)
        
        return list(clean_urls)
    
    def is_clean_url(self, url: str) -> bool:
        """
        æ£€æŸ¥URLæ˜¯å¦æ˜¯å¹²å‡€çš„ï¼ˆä¸åŒ…å«HTMLæ ‡ç­¾ï¼‰
        
        Args:
            url: è¦æ£€æŸ¥çš„URL
            
        Returns:
            bool: æ˜¯å¦æ˜¯å¹²å‡€çš„URL
        """
        # æ£€æŸ¥æ˜¯å¦åŒ…å«HTMLæ ‡ç­¾
        if re.search(r'<[^>]+>', url):
            return False
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«HTMLå®ä½“
        if re.search(r'&[a-zA-Z0-9#]+;', url):
            return False
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ˜æ˜¾çš„HTMLå†…å®¹
        html_indicators = ['<code>', '</code>', '<br/>', '<div', '</div>', '&lt;', '&gt;']
        for indicator in html_indicators:
            if indicator in url:
                return False
        
        return True

def test_url_extractor():
    """æµ‹è¯•URLæå–å™¨"""
    print("ğŸ§ª æµ‹è¯•URLæå–å™¨")
    print("=" * 50)
    
    extractor = URLExtractor()
    
    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        # ä¼ªURLç¤ºä¾‹
        'https://t.me/>è®¢é˜…é“¾æ¥ï¼š<code>https://daka778.top/api/v1/client/subscribe?token=1b4d963259351e7719fdb6ce4276cf6d</code><br/>è®¢é˜…æµé‡ï¼š<code>100 GB</code>',
        # åŒ…å«å¼•å·çš„URL
        '"https://example.com/api/v1/client/subscribe?token=abc123"',
        # å¹²å‡€çš„URL
        'https://daka778.top/api/v1/client/subscribe?token=1b4d963259351e7719fdb6ce4276cf6d',
        # åŒ…å«HTMLå®ä½“çš„URL
        'https://example.com/api/v1/client/subscribe?token=abc123&amp;flag=clash',
        # æ··åˆå†…å®¹
        'Some text https://test.com/api/v1/client/subscribe?token=xyz789 more text',
    ]
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\næµ‹è¯•ç”¨ä¾‹ {i}:")
        print(f"è¾“å…¥: {test_case[:100]}...")
        
        urls = extractor.extract_subscription_urls(test_case)
        print(f"æå–ç»“æœ: {urls}")
        
        for url in urls:
            is_clean = extractor.is_clean_url(url)
            print(f"  - {url} (å¹²å‡€: {is_clean})")

if __name__ == "__main__":
    test_url_extractor()
