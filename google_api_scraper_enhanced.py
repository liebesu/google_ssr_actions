#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆGoogle APIç«¯ç‚¹æœç´¢å™¨
é›†æˆæ™ºèƒ½å¯†é’¥ç®¡ç†å’Œé’‰é’‰é¢åº¦é€šçŸ¥
"""

import requests
import time
import json
import logging
import re
from datetime import datetime, timedelta
from logger_config import get_scraper_logger
from urllib.parse import urljoin, urlparse, quote_plus
from bs4 import BeautifulSoup
import schedule
import os
import sys
from enhanced_key_manager import EnhancedSerpAPIKeyManager
from url_extractor import URLExtractor
from typing import List, Set, Dict

class EnhancedGoogleAPIScraper:
    def __init__(self, config_file='scraper_config.json'):
        self.config_file = config_file
        self.load_config()
        self.setup_logging()
        # ä½¿ç”¨æ ‡å‡†APIæ ¼å¼æœç´¢
        self.search_query = '"api/v1/client/subscribe?token="'
        
        # ç²¾ç®€åœ°åŒºé…ç½® - åªä¿ç•™æœ€é‡è¦çš„5ä¸ªåœ°åŒº
        self.regions = [
            # ä¸­æ–‡åœ°åŒº
            {'gl': 'cn', 'hl': 'zh-CN', 'lr': 'lang_zh-CN|lang_en', 'name': 'ä¸­å›½å¤§é™†'},
            {'gl': 'hk', 'hl': 'zh-HK', 'lr': 'lang_zh-HK|lang_en', 'name': 'é¦™æ¸¯'},
            
            # è‹±è¯­åœ°åŒº
            {'gl': 'us', 'hl': 'en', 'lr': 'lang_en|lang_zh-CN', 'name': 'ç¾å›½'},
            
            # äºšå¤ªåœ°åŒº
            {'gl': 'sg', 'hl': 'en', 'lr': 'lang_en|lang_zh-CN|lang_ms', 'name': 'æ–°åŠ å¡'},
            {'gl': 'jp', 'hl': 'ja', 'lr': 'lang_ja|lang_en|lang_zh-CN', 'name': 'æ—¥æœ¬'},
        ]
        # ä»çŠ¶æ€æ–‡ä»¶åŠ è½½åœ°åŒºç´¢å¼•ï¼Œç¡®ä¿è½®æ¢çŠ¶æ€æŒä¹…åŒ–
        self.region_state_file = 'region_state.json'
        self.current_region_index = self.load_region_index()
        self.results_file = 'api_urls_results.json'
        self.log_file = 'api_scraper.log'
        
        # åˆå§‹åŒ–å¢å¼ºç‰ˆSerpAPIå¯†é’¥ç®¡ç†å™¨
        dingtalk_webhook = "https://oapi.dingtalk.com/robot/send?access_token=afb2baa012da6b3ba990405167b8c1d924e6b489c9013589ab6f6323c4a8509a"
        self.key_manager = EnhancedSerpAPIKeyManager(dingtalk_webhook=dingtalk_webhook)
        
        # åˆå§‹åŒ–URLæå–å™¨
        self.url_extractor = URLExtractor()
        
        
        self.visited_urls_file = 'visited_urls.json'
        self.discovered_urls_file = 'discovered_urls.json'
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache',
            'Referer': 'https://www.google.com/'
        })
        # é¢„ç½®åŒæ„ Cookieï¼Œé¿å…åŒæ„é¡µå½±å“
        try:
            from datetime import datetime as _dt, timezone
            consent_val = f"YES+{_dt.now(timezone.utc).strftime('%Y%m%d')}"
            self.session.cookies.set('CONSENT', consent_val, domain='.google.com')
        except Exception:
            pass
        # åœ¨sessionåˆå§‹åŒ–åè®¾ç½®ä»£ç†
        self.setup_proxy()
        self.visited_urls = self.load_visited_urls()
        self.discovered_urls = self.load_discovered_urls()
        # å¯åŠ¨æ—¶æ¸…ç†é‡å¤çš„å·²å‘ç°URL
        self.cleanup_discovered_urls()
        self.subscription_checker = None
        self.setup_subscription_checker()
        
        # æ·»åŠ é¢åº¦ç›‘æ§ç›¸å…³å±æ€§
        self.last_quota_notification = None
        self.quota_notification_interval = 6 * 60 * 60  # 6å°æ—¶å‘é€ä¸€æ¬¡é¢åº¦é€šçŸ¥
        
        # æ·»åŠ é¢‘ç‡é™åˆ¶ç›¸å…³å±æ€§
        self.last_serpapi_request = None
        self.last_search_time = None
        self.hourly_request_count = 0
        self.hourly_request_reset_time = None
        
        # æ·»åŠ é’‰é’‰é€šçŸ¥é¢‘ç‡é™åˆ¶å±æ€§
        self.last_dingtalk_notification = None
        self.dingtalk_hourly_count = 0
        self.dingtalk_hourly_reset_time = None
        self.pending_subscription_notifications = []
    
    def load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        default_config = {
            "proxy": {
                "enabled": False,
                "host": "127.0.0.1",
                "port": 7897,
                "protocol": "http"
            },
            "search": {
                "time_range": "past_12_hours",
                "max_results_per_query": 100,
                "max_pages_to_process": 30
            },
            "validation": {
                "enabled": True,
                "save_only_available": True,
                "send_notifications": True
            },
            "schedule": {
                "interval_hours": 1,
                "immediate_run": True
            },
            "quota_monitoring": {
                "enabled": True,
                "notification_interval_hours": 6,
                "low_quota_threshold": 50
            },
            "rate_limiting": {
                "serpapi_request_delay": 10,
                "min_interval_minutes": 30,
                "max_requests_per_hour": 6
            },
            "dingtalk_notifications": {
                "enabled": True,
                "min_interval_minutes": 10,
                "batch_notifications": True,
                "max_notifications_per_hour": 6,
                "quota_report_interval_hours": 6,
                "subscription_notification_delay": 5
            },
            "regions": {
                "batch_count": 1,  # æ¯æ¬¡åªæœç´¢1ä¸ªåœ°åŒº
                "inter_region_delay": 15,
                "priority_regions": ["cn", "hk", "us", "sg", "jp"],
                "use_priority_only": True  # å¯ç”¨ä¼˜å…ˆåœ°åŒºæ¨¡å¼
            }
        }
        
        if os.path.exists(self.config_file):
            try:
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    self.config = json.load(f)
                # åˆå¹¶é»˜è®¤é…ç½®
                for key, value in default_config.items():
                    if key not in self.config:
                        self.config[key] = value
                    elif isinstance(value, dict):
                        for sub_key, sub_value in value.items():
                            if sub_key not in self.config[key]:
                                self.config[key][sub_key] = sub_value
            except Exception as e:
                print(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
                self.config = default_config
        else:
            self.config = default_config
    
    def load_region_index(self):
        """åŠ è½½åœ°åŒºç´¢å¼•çŠ¶æ€"""
        try:
            if os.path.exists(self.region_state_file):
                with open(self.region_state_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    index = data.get('current_region_index', 0)
                    # ç¡®ä¿ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
                    return index % len(self.regions) if index < len(self.regions) else 0
        except Exception as e:
            print(f"åŠ è½½åœ°åŒºç´¢å¼•çŠ¶æ€å¤±è´¥: {e}")
        return 0

    def save_region_index(self):
        """ä¿å­˜åœ°åŒºç´¢å¼•çŠ¶æ€"""
        try:
            data = {
                'current_region_index': self.current_region_index,
                'last_update': datetime.now().isoformat()
            }
            with open(self.region_state_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            self.logger.error(f"ä¿å­˜åœ°åŒºç´¢å¼•çŠ¶æ€å¤±è´¥: {e}")

    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        daily_logger = get_scraper_logger()
        self.logger = daily_logger.get_logger()
    
    def setup_proxy(self):
        """è®¾ç½®ä»£ç†ï¼ˆåœ¨CIæˆ–DISABLE_PROXY=1æ—¶ç¦ç”¨ï¼‰"""
        try:
            if os.getenv('GITHUB_ACTIONS') == 'true' or os.getenv('DISABLE_PROXY') == '1':
                self.logger.info("CIç¯å¢ƒæˆ–DISABLE_PROXY=1ï¼Œä»£ç†å·²ç¦ç”¨")
                return
        except Exception:
            pass
        if self.config.get('proxy', {}).get('enabled', False):
            proxy_host = self.config['proxy']['host']
            proxy_port = self.config['proxy']['port']
            proxy_protocol = self.config['proxy']['protocol']
            proxy_url = f"{proxy_protocol}://{proxy_host}:{proxy_port}"
            self.session.proxies.update({'http': proxy_url, 'https': proxy_url})
            self.logger.info(f"å·²è®¾ç½®ä»£ç†: {proxy_url}")
    
    def load_visited_urls(self) -> Set[str]:
        """åŠ è½½å·²è®¿é—®çš„URL"""
        try:
            if os.path.exists(self.visited_urls_file):
                with open(self.visited_urls_file, 'r', encoding='utf-8') as f:
                    return set(json.load(f))
        except Exception as e:
            self.logger.error(f"åŠ è½½å·²è®¿é—®URLå¤±è´¥: {e}")
        return set()
    
    def save_visited_urls(self):
        """ä¿å­˜å·²è®¿é—®çš„URL"""
        try:
            with open(self.visited_urls_file, 'w', encoding='utf-8') as f:
                json.dump(list(self.visited_urls), f, ensure_ascii=False, indent=2)
        except Exception as e:
            self.logger.error(f"ä¿å­˜å·²è®¿é—®URLå¤±è´¥: {e}")
    
    def load_discovered_urls(self) -> Set[str]:
        """åŠ è½½å·²å‘ç°çš„è®¢é˜…é“¾æ¥"""
        try:
            if os.path.exists(self.discovered_urls_file):
                with open(self.discovered_urls_file, 'r', encoding='utf-8') as f:
                    return set(json.load(f))
        except Exception as e:
            self.logger.error(f"åŠ è½½å·²å‘ç°è®¢é˜…é“¾æ¥å¤±è´¥: {e}")
        return set()
    
    def save_discovered_urls(self):
        """ä¿å­˜å·²å‘ç°çš„è®¢é˜…é“¾æ¥"""
        try:
            with open(self.discovered_urls_file, 'w', encoding='utf-8') as f:
                json.dump(list(self.discovered_urls), f, ensure_ascii=False, indent=2)
        except Exception as e:
            self.logger.error(f"ä¿å­˜å·²å‘ç°è®¢é˜…é“¾æ¥å¤±è´¥: {e}")
    
    def extract_base_subscription_url(self, url: str) -> str:
        """æå–è®¢é˜…URLçš„åŸºç¡€éƒ¨åˆ†ï¼Œç”¨äºå»é‡æ¯”è¾ƒ"""
        # ç§»é™¤é¢å¤–çš„å‚æ•°å’Œæµé‡ä¿¡æ¯
        import re
        
        # ç§»é™¤æµé‡ä¿¡æ¯ç­‰é¢å¤–æ–‡æœ¬
        url = re.sub(r'è®¢é˜…æµé‡ï¼š[^&]*', '', url)
        url = re.sub(r'æ€»æµé‡:[^&]*', '', url)
        url = re.sub(r'å‰©ä½™æµé‡:[^&]*', '', url)
        url = re.sub(r'å·²ä¸Šä¼ :[^&]*', '', url)
        url = re.sub(r'å·²ä¸‹è½½:[^&]*', '', url)
        url = re.sub(r'è¯¥è®¢é˜…å°†äº[^&]*', '', url)
        
        # åˆ†ç¦»åŸºç¡€URLå’Œå‚æ•°
        if '?' in url:
            base_part, params = url.split('?', 1)
            # åªä¿ç•™tokenå‚æ•°
            if 'token=' in params:
                token_match = re.search(r'token=([^&]+)', params)
                if token_match:
                    return f"{base_part}?token={token_match.group(1)}"
        
        return url.strip()
    
    def cleanup_discovered_urls(self):
        """æ¸…ç†discovered_urlsä¸­çš„é‡å¤é¡¹"""
        try:
            # æŒ‰åŸºç¡€URLå»é‡
            unique_urls = {}
            for url in self.discovered_urls:
                base_url = self.extract_base_subscription_url(url)
                if base_url not in unique_urls:
                    unique_urls[base_url] = url
            
            # æ›´æ–°discovered_urls
            old_count = len(self.discovered_urls)
            self.discovered_urls = set(unique_urls.values())
            new_count = len(self.discovered_urls)
            
            if old_count != new_count:
                self.logger.info(f"ğŸ“ æ¸…ç†å·²å‘ç°URL: {old_count} -> {new_count} (ç§»é™¤ {old_count - new_count} ä¸ªé‡å¤é¡¹)")
                self.save_discovered_urls()
        except Exception as e:
            self.logger.error(f"æ¸…ç†å·²å‘ç°URLå¤±è´¥: {e}")
    
    def setup_subscription_checker(self):
        """è®¾ç½®è®¢é˜…æ£€æµ‹å™¨"""
        try:
            from subscription_checker import SubscriptionChecker
            self.subscription_checker = SubscriptionChecker(use_proxy=self.config.get('proxy', {}).get('enabled', False))
            self.logger.info("è®¢é˜…æ£€æµ‹å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            self.logger.error(f"è®¢é˜…æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.subscription_checker = None
    
    def check_quota_silently(self):
        """é™é»˜æ£€æŸ¥é¢åº¦ï¼Œä¸å‘é€é€šçŸ¥"""
        try:
            # æ£€æŸ¥æ‰€æœ‰å¯†é’¥é¢åº¦
            self.logger.info("æ­£åœ¨æ£€æŸ¥å¯†é’¥é¢åº¦...")
            quotas = self.key_manager.check_all_quotas(force_refresh=True)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¯†é’¥é¢åº¦ä¸è¶³
            available_keys = [q for q in quotas if q['success'] and q['account_status'] == 'Active']
            low_quota_threshold = self.config.get('quota_monitoring', {}).get('low_quota_threshold', 50)
            
            low_quota_keys = [q for q in available_keys if q['total_searches_left'] < low_quota_threshold]
            if low_quota_keys:
                self.logger.warning(f"å‘ç° {len(low_quota_keys)} ä¸ªå¯†é’¥é¢åº¦ä¸è¶³ {low_quota_threshold} æ¬¡")
                for key in low_quota_keys:
                    self.logger.warning(f"å¯†é’¥ {key['api_key'][:10]}... å‰©ä½™ {key['total_searches_left']} æ¬¡")
            
        except Exception as e:
            self.logger.error(f"æ£€æŸ¥é¢åº¦æ—¶å‘ç”Ÿé”™è¯¯: {e}")
    
    def check_quota_and_notify(self):
        """æ£€æŸ¥é¢åº¦å¹¶å‘é€é€šçŸ¥"""
        try:
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥å‘é€é¢åº¦é€šçŸ¥
            now = datetime.now()
            if (self.last_quota_notification and 
                (now - self.last_quota_notification).seconds < self.quota_notification_interval):
                return
            
            # æ£€æŸ¥æ‰€æœ‰å¯†é’¥é¢åº¦
            self.logger.info("æ­£åœ¨æ£€æŸ¥å¯†é’¥é¢åº¦...")
            quotas = self.key_manager.check_all_quotas(force_refresh=True)
            
            # å‘é€é’‰é’‰é€šçŸ¥
            if self.config.get('quota_monitoring', {}).get('enabled', True):
                success = self.key_manager.send_quota_notification(quotas)
                if success:
                    self.last_quota_notification = now
                    self.logger.info("é¢åº¦é€šçŸ¥å‘é€æˆåŠŸ")
                else:
                    self.logger.warning("é¢åº¦é€šçŸ¥å‘é€å¤±è´¥")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¯†é’¥é¢åº¦ä¸è¶³
            available_keys = [q for q in quotas if q['success'] and q['account_status'] == 'Active']
            low_quota_threshold = self.config.get('quota_monitoring', {}).get('low_quota_threshold', 50)
            
            low_quota_keys = [q for q in available_keys if q['total_searches_left'] < low_quota_threshold]
            if low_quota_keys:
                self.logger.warning(f"å‘ç° {len(low_quota_keys)} ä¸ªå¯†é’¥é¢åº¦ä¸è¶³ {low_quota_threshold} æ¬¡")
                for key in low_quota_keys:
                    self.logger.warning(f"å¯†é’¥ {key['api_key'][:10]}... å‰©ä½™ {key['total_searches_left']} æ¬¡")
            
        except Exception as e:
            self.logger.error(f"æ£€æŸ¥é¢åº¦æ—¶å‘ç”Ÿé”™è¯¯: {e}")
    
    def send_round_completion_notification_if_needed(self):
        """æ£€æŸ¥SerpAPIä½¿ç”¨é‡æ˜¯å¦è¾¾åˆ°é˜ˆå€¼å¹¶å‘é€é€šçŸ¥"""
        try:
            self.logger.info("æ£€æŸ¥SerpAPIä½¿ç”¨é‡æ˜¯å¦è¾¾åˆ°é€šçŸ¥é˜ˆå€¼...")
            quotas = self.key_manager.check_all_quotas(force_refresh=True)
            if quotas:
                # åªå‘é€SerpAPIä½¿ç”¨é‡é˜ˆå€¼é€šçŸ¥ï¼Œä¸å‘é€è½®æ¬¡ç»“æŸæŠ¥å‘Š
                self._send_serpapi_usage_threshold_notification(quotas)
            else:
                self.logger.warning("æ— æ³•è·å–å¯†é’¥ä½¿ç”¨æƒ…å†µä¿¡æ¯")
        except Exception as e:
            self.logger.error(f"æ£€æŸ¥SerpAPIä½¿ç”¨é‡é˜ˆå€¼æ—¶å‡ºé”™: {e}")
    
    def _send_serpapi_usage_threshold_notification(self, quotas: List[Dict]):
        """
        å‘é€SerpAPIä½¿ç”¨é‡é˜ˆå€¼é€šçŸ¥ï¼ˆä»…åœ¨è¾¾åˆ°é˜ˆå€¼æ—¶å‘é€ï¼‰
        
        Args:
            quotas: å¯†é’¥é…é¢ä¿¡æ¯åˆ—è¡¨
        """
        try:
            self.logger.info("æ£€æŸ¥SerpAPIä½¿ç”¨é‡æ˜¯å¦è¾¾åˆ°é€šçŸ¥é˜ˆå€¼...")
            
            # å¯¼å…¥è®¢é˜…æ£€æµ‹å™¨
            from subscription_checker import SubscriptionChecker
            
            # è®¡ç®—æ€»ä½¿ç”¨é‡å’Œé…é¢
            total_used = 0
            total_quota = 0
            
            # æ„å»ºè¯¦ç»†çš„å¯†é’¥ä¿¡æ¯å­—å…¸ï¼ˆç”¨äºé€šçŸ¥æ˜¾ç¤ºï¼‰
            quotas_detail = {}
            
            for i, quota_info in enumerate(quotas, 1):
                if quota_info and isinstance(quota_info, dict) and quota_info.get('success'):
                    used = quota_info.get('this_month_usage', 0)
                    quota = quota_info.get('searches_per_month', 0)
                    if used is not None and quota and quota > 0:
                        total_used += used
                        total_quota += quota
                        
                        # æ·»åŠ åˆ°è¯¦ç»†ä¿¡æ¯å­—å…¸ä¸­
                        quotas_detail[f"å¯†é’¥{i}"] = {
                            'searches_used_today': used,
                            'quota_limit': quota
                        }
                    else:
                        # è®°å½•å¤±æ•ˆçš„å¯†é’¥
                        quotas_detail[f"å¯†é’¥{i}"] = None
            
            if total_used > 0 and total_quota > 0:
                self.logger.debug(f"SerpAPIæ€»ä½¿ç”¨é‡: {total_used}/{total_quota}")
                
                # åˆ›å»ºè®¢é˜…æ£€æµ‹å™¨å®ä¾‹å¹¶å‘é€é˜ˆå€¼é€šçŸ¥
                checker = SubscriptionChecker()
                success = checker.send_serpapi_usage_notification(total_used, total_quota, quotas)
                
                if success:
                    self.logger.info("âœ… SerpAPIä½¿ç”¨é‡é˜ˆå€¼æ£€æŸ¥å®Œæˆ")
                else:
                    self.logger.warning("âŒ SerpAPIä½¿ç”¨é‡é˜ˆå€¼é€šçŸ¥å‘é€å¤±è´¥")
            else:
                self.logger.debug("æ— æ³•è®¡ç®—SerpAPIä½¿ç”¨é‡æ•°æ®")
                
        except Exception as e:
            self.logger.error(f"SerpAPIä½¿ç”¨é‡é˜ˆå€¼æ£€æŸ¥å¤±è´¥: {e}")
    
    def search_google_with_serpapi(self, query: str, time_range: str = "past_12_hours", region: Dict = None) -> List[Dict]:
        """ä½¿ç”¨SerpAPIæœç´¢Google"""
        try:
            # è·å–æœ€ä¼˜å¯†é’¥
            api_key = self.key_manager.get_optimal_key()
            if not api_key:
                self.logger.error("æ²¡æœ‰å¯ç”¨çš„APIå¯†é’¥")
                return []
            
            # æ„å»ºæœç´¢å‚æ•°
            # ä½¿ç”¨åŠ¨æ€åœ°åŒºå‚æ•°ä»¥è·å¾—ä¸åŒåœ°åŒºçš„æœç´¢ç»“æœ
            if region is None:
                region = {'gl': 'cn', 'hl': 'zh-CN', 'lr': 'lang_zh-CN|lang_en'}  # é»˜è®¤ä¸­å›½åœ°åŒº
                
            params = {
                'api_key': api_key,
                'engine': 'google',
                'q': query,
                'google_domain': 'google.com',  
                'gl': region['gl'],  # åŠ¨æ€åœ°ç†ä½ç½®
                'hl': region['hl'],  # åŠ¨æ€ç•Œé¢è¯­è¨€
                'num': self.config['search']['max_results_per_query'],
                'lr': region.get('lr', 'lang_zh-CN|lang_en'),  # åŠ¨æ€è¯­è¨€é™åˆ¶
            }
            
            # æ·»åŠ æ—¶é—´èŒƒå›´
            time_mapping = {
                'past_hour': 'qdr:h',
                'past_12_hours': 'qdr:d',
                'past_24_hours': 'qdr:d',
                'past_week': 'qdr:w',
                'past_month': 'qdr:m',
                'past_year': 'qdr:y'
            }
            
            if time_range in time_mapping:
                params['tbs'] = time_mapping[time_range]
            
            # æ·»åŠ è¿‡æ»¤å‚æ•°
            params['filter'] = '0'  # ä¸è¿‡æ»¤ç»“æœ
            
            self.logger.info(f"SerpAPI æœç´¢å‚æ•°: {params}")
            
            # å‘é€è¯·æ±‚ - å…ˆå°è¯•ä½¿ç”¨sessionï¼ˆå¸¦ä»£ç†ï¼‰ï¼Œå¤±è´¥æ—¶è‡ªåŠ¨åˆ‡æ¢åˆ°ç›´è¿
            try:
                response = self.session.get('https://serpapi.com/search', params=params, timeout=30)
            except (requests.exceptions.Timeout, requests.exceptions.ConnectionError, 
                    requests.exceptions.ChunkedEncodingError) as e:
                error_str = str(e)
                if ("timed out" in error_str and "Connection to" in error_str) or \
                   "Unable to connect to proxy" in error_str or \
                   "ProxyError" in error_str or \
                   "ConnectTimeoutError" in error_str or \
                   "Connection aborted" in error_str or \
                   "RemoteDisconnected" in error_str:
                    self.logger.info(f"SerpAPIè¿æ¥é—®é¢˜ï¼Œåˆ‡æ¢åˆ°ç›´è¿æ¨¡å¼: {e}")
                    response = requests.get('https://serpapi.com/search', params=params, timeout=30)
                else:
                    raise
            
            if response.status_code == 200:
                data = response.json()
                organic_results = data.get('organic_results', [])
                self.logger.info(f"SerpAPI æœ‰æœºç»“æœ {len(organic_results)} æ¡")
                return organic_results
            elif response.status_code == 401:
                self.logger.error("SerpAPIè®¤è¯å¤±è´¥ï¼Œå¯†é’¥å¯èƒ½æ— æ•ˆ")
                self.key_manager.mark_key_failed(api_key)
                return []
            elif response.status_code == 429:
                self.logger.error("SerpAPIè¯·æ±‚é¢‘ç‡é™åˆ¶")
                self.key_manager.mark_key_failed(api_key)
                return []
            else:
                self.logger.error(f"SerpAPIè¯·æ±‚å¤±è´¥: HTTP {response.status_code}")
                return []
                
        except Exception as e:
            self.logger.error(f"SerpAPIæœç´¢å¼‚å¸¸: {e}")
            return []
    
    def extract_api_urls_from_page(self, url: str) -> List[str]:
        """ä»ç½‘é¡µä¸­æå–API URL"""
        try:
            # æ£€æŸ¥æ˜¯å¦å·²è®¿é—®è¿‡
            if url in self.visited_urls:
                return []
            
            self.visited_urls.add(url)
            
            # è·³è¿‡æŸäº›å·²çŸ¥éš¾ä»¥è®¿é—®çš„ç½‘ç«™
            skip_domains = ['telemetr.io', 'facebook.com', 'x.com', 'twitter.com']
            if any(domain in url for domain in skip_domains):
                self.logger.info(f"è·³è¿‡å·²çŸ¥éš¾ä»¥è®¿é—®çš„åŸŸå: {url}")
                return []
            
            # å‘é€è¯·æ±‚ï¼Œå¢åŠ é‡è¯•æœºåˆ¶
            max_retries = 2
            for attempt in range(max_retries):
                try:
                    # è®¾ç½®æ›´å®½æ¾çš„headersï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨
                    headers = {
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                        'Accept-Language': 'en-US,en;q=0.5',
                        'Accept-Encoding': 'gzip, deflate',
                        'Connection': 'keep-alive',
                        'Upgrade-Insecure-Requests': '1',
                    }
                    
                    response = self.session.get(url, timeout=15, headers=headers, allow_redirects=True)
                    
                    # å¯¹äºæŸäº›é”™è¯¯çŠ¶æ€ï¼Œä¸æŠ›å‡ºå¼‚å¸¸ï¼Œè€Œæ˜¯ç»§ç»­å°è¯•è§£æ
                    if response.status_code in [200, 301, 302]:
                        break
                    elif response.status_code in [403, 404, 429]:
                        self.logger.warning(f"é¡µé¢è®¿é—®å—é™ {url}, çŠ¶æ€ç : {response.status_code}")
                        return []
                    else:
                        response.raise_for_status()
                        
                except (requests.exceptions.Timeout, requests.exceptions.ConnectionError) as e:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ä»£ç†è¿æ¥é—®é¢˜
                    error_str = str(e)
                    if ("timed out" in error_str and "Connection to" in error_str) or \
                       "Unable to connect to proxy" in error_str or \
                       "ProxyError" in error_str or \
                       "ConnectTimeoutError" in error_str:
                        # ä»£ç†è¿æ¥å¤±è´¥ï¼Œå°è¯•ç›´è¿
                        try:
                            self.logger.info(f"ä»£ç†è¿æ¥å¤±è´¥ï¼Œå°è¯•ç›´è¿è®¿é—®: {url}")
                            direct_session = requests.Session()
                            direct_session.headers.update(headers)
                            response = direct_session.get(url, timeout=15, allow_redirects=True)
                            
                            if response.status_code in [200, 301, 302]:
                                self.logger.info(f"ç›´è¿è®¿é—®æˆåŠŸ: {url}")
                                break
                            elif response.status_code in [403, 404, 429]:
                                self.logger.warning(f"é¡µé¢è®¿é—®å—é™ {url}, çŠ¶æ€ç : {response.status_code}")
                                return []
                            else:
                                response.raise_for_status()
                                
                        except Exception as direct_error:
                            self.logger.warning(f"ç›´è¿è®¿é—®ä¹Ÿå¤±è´¥ {url}: {direct_error}")
                            if attempt == max_retries - 1:
                                self.logger.warning(f"é¡µé¢è®¿é—®å½»åº•å¤±è´¥ {url}: ä»£ç†å’Œç›´è¿éƒ½å¤±è´¥")
                                return []
                            time.sleep(2)
                            continue
                    else:
                        # å…¶ä»–ç±»å‹çš„è¿æ¥é”™è¯¯
                        if attempt == max_retries - 1:
                            self.logger.warning(f"é¡µé¢è®¿é—®è¶…æ—¶/è¿æ¥å¤±è´¥ {url}: {e}")
                            return []
                        time.sleep(2)  # é‡è¯•å‰ç­‰å¾…2ç§’
                        continue
            
            # è§£æHTMLå†…å®¹
            try:
                # å°è¯•ä¸åŒçš„ç¼–ç 
                content = response.content
                if response.encoding:
                    content = response.text
                
                soup = BeautifulSoup(content, 'html.parser')
                
                # è·å–é¡µé¢æ–‡æœ¬å†…å®¹
                text = soup.get_text()
                
                # åŒæ—¶ä¹Ÿæœç´¢HTMLæºç ä¸­çš„é“¾æ¥ï¼ˆæœ‰äº›é“¾æ¥å¯èƒ½åœ¨æ³¨é‡Šæˆ–scriptä¸­ï¼‰
                html_text = str(soup)
                
                # åˆå¹¶æ–‡æœ¬å†…å®¹
                full_text = text + "\n" + html_text
                
                # ä½¿ç”¨URLæå–å™¨æå–è®¢é˜…é“¾æ¥
                api_urls = self.url_extractor.extract_subscription_urls(full_text)
                
                if api_urls:
                    self.logger.info(f"âœ… ä» {url} æå–åˆ° {len(api_urls)} ä¸ªè®¢é˜…é“¾æ¥")
                    for api_url in api_urls:
                        self.logger.info(f"   å‘ç°: {api_url}")
                else:
                    self.logger.debug(f"ä» {url} æœªæ‰¾åˆ°è®¢é˜…é“¾æ¥")
                
                return api_urls
                
            except Exception as parse_error:
                self.logger.warning(f"è§£æé¡µé¢å†…å®¹å¤±è´¥ {url}: {parse_error}")
                return []
            
        except Exception as e:
            self.logger.error(f"æå– {url} ä¸­çš„API URLå¤±è´¥: {e}")
            return []
    
    def scrape_api_urls(self) -> List[str]:
        """ä¸»è¦æœç´¢é€»è¾‘ - æ”¯æŒæ‰¹é‡å¤šåœ°åŒºæœç´¢"""
        self.logger.info("å¼€å§‹æœç´¢API URL")
        
        # åªæ£€æŸ¥é¢åº¦ï¼Œä¸å‘é€é€šçŸ¥
        self.check_quota_silently()
        
        all_api_urls = []
        time_range = self.config['search']['time_range']
        
        # æ¯æ¬¡åªæœç´¢ä¸€ä¸ªåœ°åŒºï¼Œè½®æ¢æ‰§è¡Œ
        inter_region_delay = self.config.get('regions', {}).get('inter_region_delay', 15)
        priority_regions = self.config.get('regions', {}).get('priority_regions', [])
        use_priority_only = self.config.get('regions', {}).get('use_priority_only', False)
        
        # ç¡®å®šæœ¬æ¬¡è¦æœç´¢çš„åœ°åŒºï¼ˆåªé€‰æ‹©ä¸€ä¸ªï¼‰
        regions_to_search = []
        executed_regions = []  # è®°å½•å®é™…æ‰§è¡Œçš„åœ°åŒºä¿¡æ¯ï¼Œç”¨äºä¿å­˜ç»“æœ
        
        if use_priority_only and priority_regions:
            # ä»ä¼˜å…ˆåœ°åŒºä¸­è½®æ¢é€‰æ‹©
            priority_region_configs = []
            for region_code in priority_regions:
                for region in self.regions:
                    if region['gl'] == region_code:
                        priority_region_configs.append(region)
                        break
            # åªé€‰æ‹©å½“å‰ç´¢å¼•å¯¹åº”çš„ä¼˜å…ˆåœ°åŒº
            if priority_region_configs:
                region_index = self.current_region_index % len(priority_region_configs)
                regions_to_search = [priority_region_configs[region_index]]
        else:
            # ä»æ‰€æœ‰åœ°åŒºä¸­è½®æ¢é€‰æ‹©
            total_regions = len(self.regions)
            region_index = self.current_region_index % total_regions
            regions_to_search = [self.regions[region_index]]
        
        try:
            query = self.search_query
            self.logger.info(f"ä½¿ç”¨æœç´¢æŸ¥è¯¢: {query}")
            self.logger.info(f"æœ¬æ¬¡æœç´¢åœ°åŒºæ•°é‡: {len(regions_to_search)}")
            
            # å¾ªç¯æœç´¢å„ä¸ªåœ°åŒº
            for i, current_region in enumerate(regions_to_search):
                self.logger.info(f"[{i+1}/{len(regions_to_search)}] æœç´¢åœ°åŒº: {current_region['name']} (gl={current_region['gl']}, hl={current_region['hl']}, lr={current_region.get('lr', 'lang_zh-CN|lang_en')})")
                
                try:
                    # ä½¿ç”¨SerpAPIæœç´¢å½“å‰åœ°åŒº
                    organic_results = self.search_google_with_serpapi(query, time_range, current_region)
                    
                    if not organic_results:
                        self.logger.warning(f"åœ°åŒº {current_region['name']} æœªè·å–åˆ°æœç´¢ç»“æœ")
                        executed_regions.append(current_region)  # å³ä½¿æ²¡ç»“æœä¹Ÿè®°å½•æ‰§è¡Œè¿‡
                        continue
                    
                    # è®°å½•æ‰§è¡Œçš„åœ°åŒº
                    executed_regions.append(current_region)
                    
                    # æå–ç›´æ¥å‘½ä¸­çš„å®Œæ•´è®¢é˜…URLï¼ˆåŒ…æ‹¬ä¼ªURLï¼‰
                    direct_urls = []
                    for result in organic_results:
                        # ä»é“¾æ¥å­—æ®µæå–
                        link = result.get('link', '')
                        if link:
                            extracted_urls = self.url_extractor.extract_subscription_urls(link)
                            direct_urls.extend(extracted_urls)
                        
                        # ä»æ ‡é¢˜å­—æ®µæå–
                        title = result.get('title', '')
                        if title:
                            extracted_urls = self.url_extractor.extract_subscription_urls(title)
                            direct_urls.extend(extracted_urls)
                        
                        # ä»æ‘˜è¦å­—æ®µæå–
                        snippet = result.get('snippet', '')
                        if snippet:
                            extracted_urls = self.url_extractor.extract_subscription_urls(snippet)
                            direct_urls.extend(extracted_urls)
                    
                    # å»é‡å¹¶è®°å½•
                    direct_urls = list(set(direct_urls))
                    for url in direct_urls:
                        self.logger.info(f"[{current_region['name']}] ç›´æ¥å‘½ä¸­å®Œæ•´è®¢é˜…URL: {url}")
                    
                    # ç«‹å³éªŒè¯ç›´æ¥å‘½ä¸­çš„è®¢é˜…é“¾æ¥ï¼ˆæ™ºèƒ½å»é‡ï¼‰
                    for url in direct_urls:
                        # æå–URLåŸºç¡€éƒ¨åˆ†ç”¨äºå»é‡æ£€æŸ¥
                        base_url = self.extract_base_subscription_url(url)
                        
                        # æ£€æŸ¥æ˜¯å¦å·²ç»éªŒè¯è¿‡è¿™ä¸ªåŸºç¡€URL  
                        already_discovered = any(
                            self.extract_base_subscription_url(discovered_url) == base_url 
                            for discovered_url in self.discovered_urls
                        )
                        
                        if already_discovered:
                            self.logger.info(f"â­ï¸ [{current_region['name']}] è·³è¿‡å·²éªŒè¯çš„è®¢é˜…é“¾æ¥: {url}")
                            continue
                        
                        # åŒé‡æ£€æŸ¥ï¼šç¡®ä¿åŸºç¡€URLä¸é‡å¤
                        if base_url in {self.extract_base_subscription_url(u) for u in self.discovered_urls}:
                            self.logger.debug(f"âš ï¸ [{current_region['name']}] åŸºç¡€URLå·²å­˜åœ¨ï¼Œè·³è¿‡: {base_url}")
                            continue
                            
                        self.discovered_urls.add(url)  # æ·»åŠ åˆ°å·²å‘ç°åˆ—è¡¨
                        if self.subscription_checker:
                            self.logger.info(f"ğŸ” [{current_region['name']}] éªŒè¯æ–°å‘ç°çš„è®¢é˜…é“¾æ¥: {url}")
                            result = self.subscription_checker.check_subscription_url(url)
                            if result['available']:
                                self.logger.info(f"âœ… [{current_region['name']}] ç›´æ¥å‘½ä¸­çš„è®¢é˜…é“¾æ¥å¯ç”¨: {url}")
                            else:
                                self.logger.info(f"âŒ [{current_region['name']}] ç›´æ¥å‘½ä¸­çš„è®¢é˜…é“¾æ¥ä¸å¯ç”¨: {url}")
                    
                    all_api_urls.extend(direct_urls)
                    
                    # å¤„ç†éœ€è¦è®¿é—®çš„é¡µé¢
                    pages_to_process = organic_results[:self.config['search']['max_pages_to_process']]
                    for result in pages_to_process:
                        link = result.get('link', '')
                        if link and 'api/v1/client/subscribe?token=' not in link:
                            page_urls = self.extract_api_urls_from_page(link)
                            for url in page_urls:
                                # æå–URLåŸºç¡€éƒ¨åˆ†ç”¨äºå»é‡æ£€æŸ¥
                                base_url = self.extract_base_subscription_url(url)
                                
                                # æ£€æŸ¥æ˜¯å¦å·²ç»éªŒè¯è¿‡è¿™ä¸ªåŸºç¡€URL
                                already_discovered = any(
                                    self.extract_base_subscription_url(discovered_url) == base_url 
                                    for discovered_url in self.discovered_urls
                                )
                                
                                if not already_discovered:
                                    # åŒé‡æ£€æŸ¥ï¼šç¡®ä¿åŸºç¡€URLä¸é‡å¤  
                                    if base_url not in {self.extract_base_subscription_url(u) for u in self.discovered_urls}:
                                        self.discovered_urls.add(url)
                                        if self.subscription_checker:
                                            self.logger.info(f"ğŸ” [{current_region['name']}] éªŒè¯é¡µé¢å‘ç°çš„è®¢é˜…é“¾æ¥: {url}")
                                            result = self.subscription_checker.check_subscription_url(url)
                                            if result['available']:
                                                self.logger.info(f"âœ… [{current_region['name']}] å‘ç°çš„è®¢é˜…é“¾æ¥å¯ç”¨: {url}")
                                            else:
                                                self.logger.info(f"âŒ [{current_region['name']}] å‘ç°çš„è®¢é˜…é“¾æ¥ä¸å¯ç”¨: {url}")
                                else:
                                    self.logger.info(f"â­ï¸ [{current_region['name']}] è·³è¿‡å·²éªŒè¯çš„é¡µé¢è®¢é˜…é“¾æ¥: {url}")
                            all_api_urls.extend(page_urls)
                    
                    self.logger.info(f"[{current_region['name']}] åœ°åŒºæœç´¢å®Œæˆï¼Œå‘ç° {len(direct_urls)} ä¸ªURL")
                    
                except Exception as region_error:
                    self.logger.error(f"åœ°åŒº {current_region['name']} æœç´¢å¤±è´¥: {region_error}")
                    # å³ä½¿å¤±è´¥ä¹Ÿè®°å½•æ‰§è¡Œè¿‡çš„åœ°åŒº
                    executed_regions.append(current_region)
                
                # ç”±äºåªæœç´¢ä¸€ä¸ªåœ°åŒºï¼Œä¸éœ€è¦åœ°åŒºé—´å»¶è¿Ÿ
            
            # æ›´æ–°åœ°åŒºç´¢å¼•ï¼ˆæ¯æ¬¡æ¨è¿›1ä¸ªä½ç½®ï¼‰
            if not use_priority_only:
                self.current_region_index = (self.current_region_index + 1) % len(self.regions)
                self.save_region_index()  # ä¿å­˜åœ°åŒºç´¢å¼•çŠ¶æ€
            else:
                # å¦‚æœæ˜¯ä¼˜å…ˆåœ°åŒºæ¨¡å¼ï¼Œä¹Ÿè¦æ›´æ–°ç´¢å¼•
                self.current_region_index = (self.current_region_index + 1) % len(priority_regions)
                self.save_region_index()
            
            # ä¿å­˜æ‰§è¡Œçš„åœ°åŒºåˆ—è¡¨ï¼Œä¾›ç»“æœä¿å­˜æ—¶ä½¿ç”¨
            self.last_executed_regions = executed_regions
            
            # ä¿å­˜çŠ¶æ€
            self.save_visited_urls()
            self.save_discovered_urls()
            
            self.logger.info(f"æ‰¹é‡æœç´¢å®Œæˆï¼Œå…±æœç´¢ {len(executed_regions)} ä¸ªåœ°åŒºï¼Œæ‰¾åˆ° {len(all_api_urls)} ä¸ªAPI URL")
            return all_api_urls
            
        except Exception as e:
            self.logger.error(f"æ‰¹é‡æœç´¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return []
    
    def save_results(self, api_urls: List[str]):
        """ä¿å­˜æœç´¢ç»“æœ - æ”¯æŒæ‰¹é‡åœ°åŒºä¿¡æ¯"""
        try:
            # è·å–æ‰§è¡Œçš„åœ°åŒºä¿¡æ¯
            executed_regions = getattr(self, 'last_executed_regions', [])
            
            # æœ€ç»ˆå»é‡å¤„ç†ï¼ˆä½¿ç”¨åŸºç¡€URLå»é‡ï¼‰
            unique_urls = {}
            for url in api_urls:
                base_url = self.extract_base_subscription_url(url)
                if base_url not in unique_urls:
                    unique_urls[base_url] = url
            
            # è½¬æ¢ä¸ºå»é‡åçš„åˆ—è¡¨
            deduplicated_urls = list(unique_urls.values())
            original_count = len(api_urls)
            deduplicated_count = len(deduplicated_urls)
            
            if original_count != deduplicated_count:
                self.logger.info(f"ğŸ”„ æœ€ç»ˆå»é‡: {original_count} -> {deduplicated_count} (ç§»é™¤ {original_count - deduplicated_count} ä¸ªé‡å¤URL)")
            
            if executed_regions:
                # æ„å»ºåœ°åŒºä¿¡æ¯åˆ—è¡¨
                regions_info = []
                for region in executed_regions:
                    region_str = f"{region['name']} (gl={region['gl']}, hl={region['hl']}, lr={region.get('lr', 'lang_zh-CN|lang_en')})"
                    regions_info.append(region_str)
                
                # æ„å»ºåœ°åŒºæ‘˜è¦
                if len(executed_regions) == 1:
                    regions_summary = regions_info[0]
                else:
                    regions_summary = f"æ‰¹é‡æœç´¢ {len(executed_regions)} ä¸ªåœ°åŒº: " + ", ".join([r['name'] for r in executed_regions])
            else:
                # å›é€€åˆ°å•åœ°åŒºæ¨¡å¼
                current_region = self.regions[(self.current_region_index - 1) % len(self.regions)]
                regions_info = [f"{current_region['name']} (gl={current_region['gl']}, hl={current_region['hl']}, lr={current_region.get('lr', 'lang_zh-CN|lang_en')})"]
                regions_summary = regions_info[0]
            
            # ä¿å­˜åˆ°JSONæ–‡ä»¶
            result_data = {
                'timestamp': datetime.now().isoformat(),
                'query': self.search_query,
                'search_type': 'batch_regions' if len(executed_regions) > 1 else 'single_region',
                'regions_summary': regions_summary,
                'regions_detail': regions_info,
                'regions_count': len(executed_regions) if executed_regions else 1,
                'total_urls': deduplicated_count,
                'original_urls': original_count,
                'urls': deduplicated_urls
            }
            
            with open(self.results_file, 'w', encoding='utf-8') as f:
                json.dump(result_data, f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜åˆ°æ–‡æœ¬æ–‡ä»¶
            with open('api_urls.txt', 'w', encoding='utf-8') as f:
                for url in deduplicated_urls:
                    f.write(url + '\n')
            
            self.logger.info(f"ç»“æœå·²ä¿å­˜åˆ° {self.results_file} å’Œ api_urls.txt")
            self.logger.info(f"æœç´¢æ¨¡å¼: {result_data['search_type']}, åœ°åŒº: {regions_summary}")
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")
    
    def run_scraping_task(self):
        """æ‰§è¡Œæœç´¢ä»»åŠ¡"""
        self.logger.info("=" * 50)
        self.logger.info("å¼€å§‹æ‰§è¡Œæœç´¢ä»»åŠ¡")
        self.logger.info("=" * 50)
        
        try:
            # æ‰§è¡Œæœç´¢
            api_urls = self.scrape_api_urls()
            
            if api_urls:
                # ä¿å­˜ç»“æœ
                self.save_results(api_urls)
                self.logger.info(f"ä»»åŠ¡å®Œæˆï¼Œæ‰¾åˆ° {len(api_urls)} ä¸ªAPI URL")
            else:
                self.logger.info("ä»»åŠ¡å®Œæˆï¼Œæœªæ‰¾åˆ°æ–°çš„API URL")
            
            # ä»»åŠ¡å®Œæˆåæ£€æŸ¥SerpAPIä½¿ç”¨é‡é˜ˆå€¼
            self.send_round_completion_notification_if_needed()
                
        except Exception as e:
            self.logger.error(f"æ‰§è¡Œæœç´¢ä»»åŠ¡æ—¶å‘ç”Ÿé”™è¯¯: {e}")
    
    def start_scheduler(self):
        """å¯åŠ¨å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨"""
        interval_hours = self.config['schedule']['interval_hours']
        
        # è®¾ç½®å®šæ—¶ä»»åŠ¡
        schedule.every(interval_hours).hours.do(self.run_scraping_task)
        
        # å¦‚æœé…ç½®äº†ç«‹å³è¿è¡Œ
        if self.config['schedule']['immediate_run']:
            self.logger.info("ç«‹å³æ‰§è¡Œä¸€æ¬¡æœç´¢ä»»åŠ¡")
            self.run_scraping_task()
        
        self.logger.info(f"å®šæ—¶ä»»åŠ¡å·²å¯åŠ¨ï¼Œæ¯ {interval_hours} å°æ—¶æ‰§è¡Œä¸€æ¬¡")
        
        # è¿è¡Œè°ƒåº¦å™¨
        try:
            while True:
                schedule.run_pending()
                time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
        except KeyboardInterrupt:
            self.logger.info("æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨åœæ­¢...")
        except Exception as e:
            self.logger.error(f"è°ƒåº¦å™¨è¿è¡Œå¼‚å¸¸: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨å¢å¼ºç‰ˆGoogle APIæœç´¢å™¨")
    print("=" * 50)
    
    try:
        scraper = EnhancedGoogleAPIScraper()
        scraper.start_scheduler()
    except Exception as e:
        print(f"å¯åŠ¨å¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
