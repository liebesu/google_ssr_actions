# é¡¹ç›®åˆ†æä¸æ”¹è¿›å»ºè®®

## ğŸ“Š é¡¹ç›®æ¦‚è¿°

è¿™æ˜¯ä¸€ä¸ªåŸºäº GitHub Actions çš„**è®¢é˜…èšåˆç³»ç»Ÿ**ï¼Œé€šè¿‡è‡ªåŠ¨åŒ–çˆ¬å–ã€éªŒè¯ã€èšåˆè®¢é˜…é“¾æ¥ï¼Œç”Ÿæˆå¤šç§æ ¼å¼çš„è®¢é˜…æ–‡ä»¶å¹¶éƒ¨ç½²åˆ° GitHub Pagesã€‚

### æ ¸å¿ƒåŠŸèƒ½
- âœ… Google/GitHub æœç´¢å‘ç°è®¢é˜…é“¾æ¥
- âœ… è®¢é˜…å¯ç”¨æ€§éªŒè¯å’Œæµé‡æ£€æµ‹
- âœ… èŠ‚ç‚¹å»é‡ã€åˆ†ç±»ï¼ˆåè®®/åœ°åŒºï¼‰
- âœ… å¤šæ ¼å¼è®¢é˜…æ–‡ä»¶ç”Ÿæˆï¼ˆTXT/YAML/Clashï¼‰
- âœ… è‡ªåŠ¨åŒ–å®šæ—¶æ„å»ºå’Œéƒ¨ç½²

---

## ğŸ” ä»£ç è´¨é‡åˆ†æ

### âœ… ä¼˜ç‚¹

1. **åŠŸèƒ½å®Œæ•´æ€§å¼º**
   - è¦†ç›–äº†è®¢é˜…èšåˆçš„å®Œæ•´æµç¨‹
   - æ”¯æŒå¤šç§åè®®å’Œæ ¼å¼
   - æœ‰å®Œå–„çš„æ•°æ®æŒä¹…åŒ–æœºåˆ¶

2. **è‡ªåŠ¨åŒ–ç¨‹åº¦é«˜**
   - GitHub Actions è‡ªåŠ¨åŒ–æ„å»º
   - å®šæ—¶ä»»åŠ¡ï¼ˆæ¯3å°æ—¶ï¼‰
   - è‡ªåŠ¨éƒ¨ç½²åˆ° GitHub Pages

3. **ç›‘æ§å’Œé€šçŸ¥**
   - é›†æˆé’‰é’‰é€šçŸ¥
   - å¥åº·çŠ¶æ€æ£€æŸ¥
   - SerpAPI é…é¢ç®¡ç†

### âŒ é—®é¢˜ç‚¹

#### 1. **ä»£ç æ¶æ„é—®é¢˜**

**é—®é¢˜1ï¼šæ–‡ä»¶è¿‡å¤§ï¼ŒèŒè´£ä¸æ¸…**
- `aggregator_cli.py` æœ‰ **2453 è¡Œ**ï¼ŒåŒ…å«å¤ªå¤šèŒè´£
- `subscription_checker.py` æœ‰ **2312 è¡Œ**
- è¿åå•ä¸€èŒè´£åŸåˆ™

**æ”¹è¿›å»ºè®®**ï¼š
```python
# å»ºè®®æ‹†åˆ†ä¸ºï¼š
aggregator_cli.py          # ä¸»å…¥å£ï¼ˆ<200è¡Œï¼‰
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ url_discovery.py   # URLå‘ç°æœåŠ¡
â”‚   â”œâ”€â”€ subscription_validator.py  # è®¢é˜…éªŒè¯æœåŠ¡
â”‚   â”œâ”€â”€ node_processor.py  # èŠ‚ç‚¹å¤„ç†æœåŠ¡
â”‚   â”œâ”€â”€ subscription_generator.py  # è®¢é˜…ç”ŸæˆæœåŠ¡
â”‚   â””â”€â”€ health_monitor.py  # å¥åº·ç›‘æ§æœåŠ¡
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ subscription.py    # è®¢é˜…æ•°æ®æ¨¡å‹
â”‚   â””â”€â”€ node.py           # èŠ‚ç‚¹æ•°æ®æ¨¡å‹
â””â”€â”€ utils/
    â”œâ”€â”€ protocol_classifier.py  # åè®®åˆ†ç±»å™¨
    â””â”€â”€ region_classifier.py    # åœ°åŒºåˆ†ç±»å™¨
```

**é—®é¢˜2ï¼šä»£ç é‡å¤**
- å¤šä¸ªæ–‡ä»¶ä¸­æœ‰é‡å¤çš„ URL è§£æé€»è¾‘
- é‡å¤çš„ Base64 è§£ç é€»è¾‘
- é‡å¤çš„åè®®åˆ†ç±»é€»è¾‘

**æ”¹è¿›å»ºè®®**ï¼š
```python
# ç»Ÿä¸€å·¥å…·ç±»
class URLParser:
    @staticmethod
    def normalize_subscribe_url(url: str) -> Optional[str]:
        # ç»Ÿä¸€å®ç°
        
class ProtocolClassifier:
    @staticmethod
    def classify(line: str) -> Optional[str]:
        # ç»Ÿä¸€å®ç°
```

#### 2. **æ€§èƒ½ä¼˜åŒ–**

**é—®é¢˜1ï¼šåŒæ­¥å¤„ç†ï¼Œæ•ˆç‡ä½**
```python
# å½“å‰ä»£ç ï¼ˆaggregator_cli.py:1250-1301ï¼‰
for u in candidates:
    if should_skip_due_to_backoff(rate_state, u, now_ts):
        continue
    ok, code, err, lat_ms = validate_subscription_url(u)  # åŒæ­¥é˜»å¡
    if ok:
        alive_urls.append(u)
```

**æ”¹è¿›å»ºè®®**ï¼š
```python
# ä½¿ç”¨å¹¶å‘å¤„ç†
from concurrent.futures import ThreadPoolExecutor, as_completed

def validate_urls_parallel(urls: List[str], max_workers: int = 10):
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(validate_subscription_url, url): url 
                   for url in urls}
        results = []
        for future in as_completed(futures):
            url = futures[future]
            try:
                result = future.result()
                if result[0]:  # ok
                    results.append(url)
            except Exception as e:
                logger.error(f"Validation failed for {url}: {e}")
    return results
```

**é—®é¢˜2ï¼šé‡å¤è¯·æ±‚**
- åŒä¸€ä¸ª URL å¯èƒ½è¢«å¤šæ¬¡è¯·æ±‚
- ç¼ºå°‘è¯·æ±‚ç¼“å­˜æœºåˆ¶

**æ”¹è¿›å»ºè®®**ï¼š
```python
# æ·»åŠ è¯·æ±‚ç¼“å­˜
from functools import lru_cache
from datetime import datetime, timedelta

class RequestCache:
    def __init__(self, ttl_seconds: int = 300):
        self.cache = {}
        self.ttl = timedelta(seconds=ttl_seconds)
    
    def get(self, url: str):
        if url in self.cache:
            data, timestamp = self.cache[url]
            if datetime.now() - timestamp < self.ttl:
                return data
        return None
    
    def set(self, url: str, data):
        self.cache[url] = (data, datetime.now())
```

#### 3. **é”™è¯¯å¤„ç†**

**é—®é¢˜1ï¼šå¼‚å¸¸å¤„ç†ä¸å¤Ÿå®Œå–„**
```python
# å½“å‰ä»£ç ï¼šå¤§é‡ bare except
except Exception:
    return None
```

**æ”¹è¿›å»ºè®®**ï¼š
```python
# æ˜ç¡®å¼‚å¸¸ç±»å‹å’Œæ—¥å¿—è®°å½•
import logging

logger = logging.getLogger(__name__)

def safe_b64_decode(data: str) -> Optional[str]:
    try:
        # ...
    except ValueError as e:
        logger.debug(f"Base64 decode failed: {e}")
        return None
    except Exception as e:
        logger.error(f"Unexpected error in base64 decode: {e}", exc_info=True)
        return None
```

**é—®é¢˜2ï¼šç¼ºå°‘é‡è¯•æœºåˆ¶**
- ç½‘ç»œè¯·æ±‚å¤±è´¥æ—¶ç›´æ¥è·³è¿‡
- æ²¡æœ‰æŒ‡æ•°é€€é¿é‡è¯•

**æ”¹è¿›å»ºè®®**ï¼š
```python
# å·²ç»æœ‰ error_handler.pyï¼Œä½†ä½¿ç”¨ä¸å¤Ÿ
from error_handler import retry_with_backoff

@retry_with_backoff(max_retries=3, initial_delay=1.0)
def fetch_subscription(url: str) -> Tuple[Optional[bytes], ...]:
    # ...
```

#### 4. **é…ç½®ç®¡ç†**

**é—®é¢˜1ï¼šç¡¬ç¼–ç é…ç½®**
```python
# aggregator_cli.py:2287-2289
next_dt = build_dt + timedelta(hours=3)  # ç¡¬ç¼–ç 3å°æ—¶
```

**æ”¹è¿›å»ºè®®**ï¼š
```python
# ä½¿ç”¨é…ç½®æ–‡ä»¶æˆ–ç¯å¢ƒå˜é‡
class Config:
    BUILD_INTERVAL_HOURS = int(os.getenv("BUILD_INTERVAL_HOURS", "3"))
    MAX_NODES = int(os.getenv("MAX_NODES", "1200"))
    MAX_WORKERS = int(os.getenv("MAX_WORKERS", "10"))
```

**é—®é¢˜2ï¼šç¯å¢ƒå˜é‡å¤„ç†æ··ä¹±**
- GitHub Actions workflow ä¸­æœ‰å¤§é‡é‡å¤çš„ç¯å¢ƒå˜é‡è®¾ç½®
- ç¼ºå°‘ç»Ÿä¸€çš„é…ç½®ç®¡ç†

**æ”¹è¿›å»ºè®®**ï¼š
```python
# åˆ›å»ºç»Ÿä¸€çš„é…ç½®åŠ è½½å™¨
class ConfigLoader:
    @staticmethod
    def load_serpapi_keys():
        keys = []
        # ä¼˜å…ˆä» SCRAPER_KEYS
        if scraper_keys := os.getenv("SCRAPER_KEYS"):
            keys.extend(scraper_keys.split('\n'))
        # å†ä» SERPAPI_KEY_1-10
        for i in range(1, 11):
            if key := os.getenv(f"SERPAPI_KEY_{i}"):
                keys.append(key)
        return keys
```

#### 5. **æµ‹è¯•è¦†ç›–**

**é—®é¢˜**ï¼š
- ç¼ºå°‘å•å…ƒæµ‹è¯•
- é›†æˆæµ‹è¯•ä¸å®Œæ•´
- æµ‹è¯•è¦†ç›–ç‡ä½

**æ”¹è¿›å»ºè®®**ï¼š
```python
# æ·»åŠ  pytest æµ‹è¯•
# tests/test_protocol_classifier.py
def test_classify_protocol():
    assert classify_protocol("ss://...") == "ss"
    assert classify_protocol("vmess://...") == "vmess"

# tests/test_url_validator.py
def test_normalize_subscribe_url():
    assert normalize_subscribe_url("https://example.com/api/v1/client/subscribe?token=abc") is not None
    assert normalize_subscribe_url("invalid") is None
```

#### 6. **å®‰å…¨æ€§**

**é—®é¢˜1ï¼šæ•æ„Ÿä¿¡æ¯å¯èƒ½æ³„éœ²**
```python
# aggregator_cli.py:1174
"key_masked": (api_key[:4] + "*" * min(8, max(0, len(api_key) - 8)) + api_key[-4:])
```
- è™½ç„¶åšäº†æ©ç ï¼Œä½†å¯èƒ½ä¸å¤Ÿå®‰å…¨

**æ”¹è¿›å»ºè®®**ï¼š
```python
# æ›´å®‰å…¨çš„æ©ç 
def mask_key(key: str) -> str:
    if len(key) <= 8:
        return "*" * len(key)
    return key[:2] + "*" * (len(key) - 4) + key[-2:]
```

**é—®é¢˜2ï¼šè¾“å…¥éªŒè¯ä¸è¶³**
- URL éªŒè¯å¯èƒ½ä¸å¤Ÿä¸¥æ ¼
- ç¼ºå°‘å¯¹æ¶æ„å†…å®¹çš„æ£€æŸ¥

**æ”¹è¿›å»ºè®®**ï¼š
```python
import re

def validate_url_format(url: str) -> bool:
    # URL æ ¼å¼éªŒè¯
    url_pattern = re.compile(
        r'^https?://'  # http:// or https://
        r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\.)+[A-Z]{2,6}\.?|'  # domain
        r'localhost|'  # localhost
        r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})'  # IP
        r'(?::\d+)?'  # optional port
        r'(?:/?|[/?]\S+)$', re.IGNORECASE)
    return bool(url_pattern.match(url))
```

#### 7. **ä»£ç å¯ç»´æŠ¤æ€§**

**é—®é¢˜1ï¼šç¼ºå°‘ç±»å‹æç¤º**
```python
# å¾ˆå¤šå‡½æ•°ç¼ºå°‘å®Œæ•´çš„ç±»å‹æç¤º
def normalize_node_line(line: str) -> Optional[str]:
    # ...
```

**æ”¹è¿›å»ºè®®**ï¼š
```python
# ä½¿ç”¨å®Œæ•´çš„ç±»å‹æç¤º
from typing import Optional, Dict, List, Tuple, Union

def normalize_node_line(line: str) -> Optional[str]:
    """
    æ ‡å‡†åŒ–èŠ‚ç‚¹è¡Œ
    
    Args:
        line: åŸå§‹èŠ‚ç‚¹é…ç½®è¡Œ
        
    Returns:
        æ ‡å‡†åŒ–åçš„èŠ‚ç‚¹è¡Œï¼Œå¦‚æœæ— æ•ˆåˆ™è¿”å› None
    """
    # ...
```

**é—®é¢˜2ï¼šæ–‡æ¡£å­—ç¬¦ä¸²ä¸è¶³**
- å¾ˆå¤šå‡½æ•°ç¼ºå°‘æ–‡æ¡£å­—ç¬¦ä¸²
- ç¼ºå°‘æ¨¡å—çº§åˆ«çš„æ–‡æ¡£

**æ”¹è¿›å»ºè®®**ï¼š
```python
"""
è®¢é˜…èšåˆå™¨æ¨¡å—

è¯¥æ¨¡å—è´Ÿè´£ï¼š
1. å‘ç°è®¢é˜…é“¾æ¥
2. éªŒè¯è®¢é˜…å¯ç”¨æ€§
3. å¤„ç†èŠ‚ç‚¹æ•°æ®
4. ç”Ÿæˆè®¢é˜…æ–‡ä»¶
"""

def process_subscriptions(urls: List[str]) -> Dict[str, Any]:
    """
    å¤„ç†è®¢é˜…åˆ—è¡¨
    
    Args:
        urls: è®¢é˜… URL åˆ—è¡¨
        
    Returns:
        å¤„ç†ç»“æœå­—å…¸ï¼ŒåŒ…å«ï¼š
        - alive_urls: å¯ç”¨ URL åˆ—è¡¨
        - nodes: èŠ‚ç‚¹åˆ—è¡¨
        - stats: ç»Ÿè®¡ä¿¡æ¯
    """
    # ...
```

#### 8. **GitHub Actions ä¼˜åŒ–**

**é—®é¢˜1ï¼šæ„å»ºæ­¥éª¤è¿‡å¤š**
- ç¯å¢ƒå˜é‡è®¾ç½®æ­¥éª¤è¿‡é•¿ï¼ˆ10ä¸ª SERPAPI_KEYï¼‰
- ç¼ºå°‘æ„å»ºç¼“å­˜

**æ”¹è¿›å»ºè®®**ï¼š
```yaml
# ä½¿ç”¨çŸ©é˜µç­–ç•¥ç®€åŒ–å¯†é’¥ç®¡ç†
- name: Prepare SerpAPI keys
  run: |
    python scripts/prepare_keys.py
    
# æ·»åŠ æ„å»ºç¼“å­˜
- name: Cache Python dependencies
  uses: actions/cache@v3
  with:
    path: ~/.cache/pip
    key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements_scraper.txt') }}
```

**é—®é¢˜2ï¼šç¼ºå°‘æ„å»ºå¤±è´¥é€šçŸ¥**
- æ„å»ºå¤±è´¥æ—¶æ²¡æœ‰é€šçŸ¥æœºåˆ¶

**æ”¹è¿›å»ºè®®**ï¼š
```yaml
- name: Notify on failure
  if: failure()
  run: |
    # å‘é€å¤±è´¥é€šçŸ¥
```

#### 9. **æ•°æ®åº“/å­˜å‚¨ä¼˜åŒ–**

**é—®é¢˜**ï¼š
- ä½¿ç”¨ JSON æ–‡ä»¶å­˜å‚¨å†å²æ•°æ®
- éšç€æ•°æ®å¢é•¿ï¼Œæ€§èƒ½ä¼šä¸‹é™

**æ”¹è¿›å»ºè®®**ï¼š
```python
# è€ƒè™‘ä½¿ç”¨ SQLiteï¼ˆè½»é‡çº§ï¼‰
import sqlite3

class SubscriptionDB:
    def __init__(self, db_path: str = "data/subscriptions.db"):
        self.conn = sqlite3.connect(db_path)
        self._init_schema()
    
    def _init_schema(self):
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS subscriptions (
                url TEXT PRIMARY KEY,
                first_seen TEXT,
                last_seen TEXT,
                status TEXT,
                nodes_count INTEGER
            )
        """)
```

#### 10. **ç›‘æ§å’Œæ—¥å¿—**

**é—®é¢˜**ï¼š
- æ—¥å¿—è®°å½•ä¸å¤Ÿè¯¦ç»†
- ç¼ºå°‘æ€§èƒ½ç›‘æ§

**æ”¹è¿›å»ºè®®**ï¼š
```python
import time
import logging
from functools import wraps

def log_performance(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        start = time.perf_counter()
        result = func(*args, **kwargs)
        elapsed = time.perf_counter() - start
        logger.info(f"{func.__name__} took {elapsed:.2f}s")
        return result
    return wrapper

@log_performance
def validate_subscription_url(url: str):
    # ...
```

---

## ğŸš€ æ”¹è¿›ä¼˜å…ˆçº§

### ğŸ”´ é«˜ä¼˜å…ˆçº§ï¼ˆç«‹å³æ”¹è¿›ï¼‰

1. **æ‹†åˆ†å¤§æ–‡ä»¶**
   - `aggregator_cli.py` æ‹†åˆ†
   - `subscription_checker.py` æ‹†åˆ†

2. **æ·»åŠ å¹¶å‘å¤„ç†**
   - URL éªŒè¯å¹¶å‘åŒ–
   - è®¢é˜…ä¸‹è½½å¹¶å‘åŒ–

3. **å®Œå–„é”™è¯¯å¤„ç†**
   - æ˜ç¡®å¼‚å¸¸ç±»å‹
   - æ·»åŠ é‡è¯•æœºåˆ¶

4. **ç»Ÿä¸€é…ç½®ç®¡ç†**
   - åˆ›å»ºé…ç½®ç±»
   - ç®€åŒ–ç¯å¢ƒå˜é‡å¤„ç†

### ğŸŸ¡ ä¸­ä¼˜å…ˆçº§ï¼ˆè¿‘æœŸæ”¹è¿›ï¼‰

5. **æ·»åŠ æµ‹è¯•è¦†ç›–**
   - å•å…ƒæµ‹è¯•
   - é›†æˆæµ‹è¯•

6. **æ€§èƒ½ä¼˜åŒ–**
   - æ·»åŠ è¯·æ±‚ç¼“å­˜
   - å‡å°‘é‡å¤è¯·æ±‚

7. **ä»£ç æ–‡æ¡£åŒ–**
   - æ·»åŠ æ–‡æ¡£å­—ç¬¦ä¸²
   - å®Œå–„ README

### ğŸŸ¢ ä½ä¼˜å…ˆçº§ï¼ˆé•¿æœŸæ”¹è¿›ï¼‰

8. **æ•°æ®åº“è¿ç§»**
   - JSON â†’ SQLite

9. **ç›‘æ§å¢å¼º**
   - æ€§èƒ½ç›‘æ§
   - è¯¦ç»†æ—¥å¿—

10. **å®‰å…¨æ€§å¢å¼º**
    - è¾“å…¥éªŒè¯
    - æ•æ„Ÿä¿¡æ¯ä¿æŠ¤

---

## ğŸ“ å…·ä½“æ”¹è¿›ç¤ºä¾‹

### ç¤ºä¾‹1ï¼šæ‹†åˆ† aggregator_cli.py

```python
# aggregator_cli.py (ä¸»å…¥å£ï¼Œ<200è¡Œ)
from services.subscription_aggregator import SubscriptionAggregator

def main():
    parser = argparse.ArgumentParser()
    # ... å‚æ•°è§£æ
    
    aggregator = SubscriptionAggregator(
        output_dir=args.output_dir,
        max_nodes=args.max,
        dedup=args.dedup
    )
    
    aggregator.run()

# services/subscription_aggregator.py
class SubscriptionAggregator:
    def __init__(self, output_dir: str, max_nodes: int = None, dedup: bool = False):
        self.output_dir = output_dir
        self.max_nodes = max_nodes
        self.dedup = dedup
        self.url_discovery = URLDiscoveryService()
        self.validator = SubscriptionValidator()
        self.processor = NodeProcessor()
        self.generator = SubscriptionGenerator()
    
    def run(self):
        # 1. å‘ç° URL
        urls = self.url_discovery.discover()
        
        # 2. éªŒè¯ URL
        alive_urls = self.validator.validate(urls)
        
        # 3. å¤„ç†èŠ‚ç‚¹
        nodes = self.processor.process(alive_urls)
        
        # 4. ç”Ÿæˆè®¢é˜…
        self.generator.generate(nodes, self.output_dir)
```

### ç¤ºä¾‹2ï¼šå¹¶å‘å¤„ç†

```python
# utils/concurrent_validator.py
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Tuple

class ConcurrentValidator:
    def __init__(self, max_workers: int = 10):
        self.max_workers = max_workers
    
    def validate_urls(self, urls: List[str]) -> List[str]:
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {
                executor.submit(validate_subscription_url, url): url
                for url in urls
            }
            
            alive_urls = []
            for future in as_completed(futures):
                url = futures[future]
                try:
                    ok, code, err, lat_ms = future.result()
                    if ok:
                        alive_urls.append(url)
                except Exception as e:
                    logger.error(f"Validation failed for {url}: {e}")
            
            return alive_urls
```

---

## ğŸ“Š é¢„æœŸæ”¹è¿›æ•ˆæœ

| æ”¹è¿›é¡¹ | å½“å‰çŠ¶æ€ | æ”¹è¿›å | æå‡ |
|--------|---------|--------|------|
| ä»£ç è¡Œæ•° | 2453è¡Œ/æ–‡ä»¶ | <500è¡Œ/æ–‡ä»¶ | -80% |
| æ„å»ºæ—¶é—´ | ~5åˆ†é’Ÿ | ~2åˆ†é’Ÿ | -60% |
| é”™è¯¯ç‡ | ~5% | <1% | -80% |
| æµ‹è¯•è¦†ç›–ç‡ | ~10% | >80% | +700% |
| ä»£ç å¯ç»´æŠ¤æ€§ | ä½ | é«˜ | â¬†ï¸ |

---

## ğŸ¯ æ€»ç»“

è¿™ä¸ªé¡¹ç›®**åŠŸèƒ½å®Œæ•´**ï¼Œä½†å­˜åœ¨**ä»£ç ç»“æ„é—®é¢˜**å’Œ**æ€§èƒ½ç“¶é¢ˆ**ã€‚é€šè¿‡ï¼š
1. æ‹†åˆ†å¤§æ–‡ä»¶
2. æ·»åŠ å¹¶å‘å¤„ç†
3. å®Œå–„é”™è¯¯å¤„ç†
4. æ·»åŠ æµ‹è¯•è¦†ç›–

å¯ä»¥å¤§å¹…æå‡ä»£ç è´¨é‡å’Œæ€§èƒ½ã€‚

å»ºè®®ä¼˜å…ˆå¤„ç†**é«˜ä¼˜å…ˆçº§**é—®é¢˜ï¼Œè¿™äº›æ”¹è¿›å¯ä»¥å¸¦æ¥ç«‹ç«¿è§å½±çš„æ•ˆæœã€‚



