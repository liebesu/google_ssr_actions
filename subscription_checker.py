#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è®¢é˜…é“¾æ¥å¯ç”¨æ€§æ£€æµ‹è„šæœ¬
åŠŸèƒ½ï¼šæ£€æµ‹è®¢é˜…é“¾æ¥æ˜¯å¦å¯ç”¨ï¼Œå¯ç”¨æ—¶å‘é€é’‰é’‰é€šçŸ¥
"""

import requests
import json
import time
import logging
import urllib3
import os
import re
from urllib.parse import urlparse
from typing import Dict, List, Optional
from logger_config import get_subscription_logger

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# é…ç½®
DINGTALK_WEBHOOK = "https://oapi.dingtalk.com/robot/send?access_token=afb2baa012da6b3ba990405167b8c1d924e6b489c9013589ab6f6323c4a8509a"
DINGTALK_KEYWORD = ":"  # é’‰é’‰å…³é”®å­—
REQUEST_TIMEOUT = 10  # è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"

# ä»£ç†é…ç½®
PROXY_CONFIG = {
    'http': 'http://192.168.100.110:7893',
    'https': 'http://192.168.100.110:7893'
}

# ä½¿ç”¨æ–°çš„æ—¥å¿—ç³»ç»Ÿ
daily_logger = get_subscription_logger()
logger = daily_logger.get_logger()


class SubscriptionChecker:
    """è®¢é˜…é“¾æ¥æ£€æµ‹å™¨"""
    
    def __init__(self, use_proxy=True):
        self.session = requests.Session()
        self.use_proxy = use_proxy
        
        # è®¾ç½®è¯·æ±‚å¤´
        self.session.headers.update({
            'User-Agent': USER_AGENT,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        })
        
        # è®¾ç½®ä»£ç†
        if self.use_proxy:
            # æµ‹è¯•ä»£ç†è¿æ¥
            if self._test_proxy_connection():
                self.session.proxies.update(PROXY_CONFIG)
                logger.info(f"å·²å¯ç”¨ä»£ç†: {PROXY_CONFIG}")
                self.proxy_available = True
            else:
                logger.warning("ä»£ç†è¿æ¥å¤±è´¥ï¼Œåˆ‡æ¢åˆ°ç›´è¿æ¨¡å¼")
                self.use_proxy = False
                self.proxy_available = False
        else:
            logger.info("æœªä½¿ç”¨ä»£ç†")
            self.proxy_available = False
        
        # å·²å‘é€é’‰é’‰é€šçŸ¥çš„URLè®°å½•æ–‡ä»¶
        self.notified_urls_file = 'notified_urls.txt'
        self.notified_urls = self._load_notified_urls()
        
        # é’‰é’‰Webhooké…ç½®
        self.dingtalk_webhook = DINGTALK_WEBHOOK
    
    def _test_proxy_connection(self) -> bool:
        """
        æµ‹è¯•ä»£ç†è¿æ¥æ˜¯å¦å¯ç”¨
        
        Returns:
            bool: ä»£ç†æ˜¯å¦å¯ç”¨
        """
        try:
            # åˆ›å»ºä¸´æ—¶sessionæµ‹è¯•ä»£ç†
            test_session = requests.Session()
            test_session.proxies.update(PROXY_CONFIG)
            test_session.headers.update({'User-Agent': USER_AGENT})
            
            # æµ‹è¯•è¿æ¥åˆ°ä¸€ä¸ªç®€å•çš„HTTPæœåŠ¡
            response = test_session.get('http://httpbin.org/ip', timeout=5)
            if response.status_code == 200:
                logger.info(f"ä»£ç†æµ‹è¯•æˆåŠŸ: {response.json().get('origin', 'Unknown IP')}")
                return True
            else:
                logger.warning(f"ä»£ç†æµ‹è¯•å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return False
                
        except Exception as e:
            logger.warning(f"ä»£ç†æµ‹è¯•å¼‚å¸¸: {e}")
            return False
    
    def _load_notified_urls(self) -> set:
        """
        ä»æ–‡ä»¶åŠ è½½å·²é€šçŸ¥çš„URLåˆ—è¡¨
        
        Returns:
            set: å·²é€šçŸ¥çš„URLé›†åˆ
        """
        try:
            if os.path.exists(self.notified_urls_file):
                with open(self.notified_urls_file, 'r', encoding='utf-8') as f:
                    urls = set(line.strip() for line in f if line.strip())
                logger.info(f"åŠ è½½äº† {len(urls)} ä¸ªå·²é€šçŸ¥URLè®°å½•")
                return urls
            else:
                logger.info("æœªæ‰¾åˆ°å·²é€šçŸ¥URLè®°å½•æ–‡ä»¶ï¼Œåˆ›å»ºæ–°çš„è®°å½•")
                return set()
        except Exception as e:
            logger.error(f"åŠ è½½å·²é€šçŸ¥URLè®°å½•å¤±è´¥: {e}")
            return set()
    
    def _save_notified_urls(self):
        """
        ä¿å­˜å·²é€šçŸ¥çš„URLåˆ—è¡¨åˆ°æ–‡ä»¶
        """
        try:
            with open(self.notified_urls_file, 'w', encoding='utf-8') as f:
                for url in sorted(self.notified_urls):
                    f.write(f"{url}\n")
            logger.debug(f"ä¿å­˜äº† {len(self.notified_urls)} ä¸ªå·²é€šçŸ¥URLè®°å½•")
        except Exception as e:
            logger.error(f"ä¿å­˜å·²é€šçŸ¥URLè®°å½•å¤±è´¥: {e}")
    
    def _calculate_next_reset_date(self, quota_info: Dict, key_index: int) -> str:
        """
        è®¡ç®—SerpAPIè´¦æˆ·çš„ä¸‹æ¬¡é‡ç½®æ—¶é—´
        
        Args:
            quota_info: é…é¢ä¿¡æ¯å­—å…¸
            key_index: å¯†é’¥ç´¢å¼•
            
        Returns:
            str: ä¸‹æ¬¡é‡ç½®æ—¶é—´å­—ç¬¦ä¸²
        """
        try:
            from datetime import datetime, timedelta
            import calendar
            import json
            import os
            
            # è·å–å½“å‰æ—¶é—´
            now = datetime.now()
            
            # å°è¯•ä»é…ç½®æ–‡ä»¶åŠ è½½æ³¨å†Œæ—¥æœŸ
            registration_dates_file = 'api_key_registration_dates.json'
            registration_dates = {}
            
            if os.path.exists(registration_dates_file):
                try:
                    with open(registration_dates_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        registration_dates = data.get('key_registration_dates', {})
                except Exception as e:
                    logger.warning(f"åŠ è½½æ³¨å†Œæ—¥æœŸé…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            
            # è·å–å½“å‰APIå¯†é’¥
            current_api_key = quota_info.get('api_key', '')
            
            # æŸ¥æ‰¾å¯¹åº”çš„æ³¨å†Œæ—¥æœŸ
            registration_date_str = None
            for key, date in registration_dates.items():
                if key in current_api_key or current_api_key in key:
                    registration_date_str = date
                    break
            
            if registration_date_str:
                try:
                    # è§£ææ³¨å†Œæ—¥æœŸ
                    registration_date = datetime.strptime(registration_date_str, '%Y-%m-%d')
                    
                    # è®¡ç®—ä¸‹æ¬¡é‡ç½®æ—¶é—´ï¼ˆåŸºäºæ³¨å†Œæ—¥æœŸçš„æ¯æœˆå¯¹åº”æ—¥ï¼‰
                    if now.month == 12:
                        # å¦‚æœå½“å‰æ˜¯12æœˆï¼Œé‡ç½®æ—¶é—´æ˜¯ä¸‹å¹´åŒæœˆåŒæ—¥
                        next_reset = registration_date.replace(year=now.year + 1)
                    else:
                        # å¦åˆ™æ˜¯ä¸‹ä¸ªæœˆåŒæœˆåŒæ—¥
                        next_reset = registration_date.replace(year=now.year, month=now.month + 1)
                    
                    # å¦‚æœè®¡ç®—å‡ºçš„é‡ç½®æ—¶é—´å·²ç»è¿‡äº†ï¼Œåˆ™ä½¿ç”¨ä¸‹ä¸‹ä¸ªæœˆ
                    if next_reset <= now:
                        if now.month == 11:
                            next_reset = registration_date.replace(year=now.year + 1, month=1)
                        elif now.month == 12:
                            next_reset = registration_date.replace(year=now.year + 1, month=2)
                        else:
                            next_reset = registration_date.replace(year=now.year, month=now.month + 2)
                    
                    # ç¡®ä¿æ—¥æœŸæœ‰æ•ˆï¼ˆå¤„ç†2æœˆ29æ—¥ç­‰ç‰¹æ®Šæƒ…å†µï¼‰
                    last_day_of_month = calendar.monthrange(next_reset.year, next_reset.month)[1]
                    if next_reset.day > last_day_of_month:
                        next_reset = next_reset.replace(day=last_day_of_month)
                    
                    logger.debug(f"å¯†é’¥ {key_index} åŸºäºæ³¨å†Œæ—¥æœŸ {registration_date_str} è®¡ç®—é‡ç½®æ—¶é—´: {next_reset.strftime('%Y-%m-%d')}")
                    return next_reset.strftime("%Y-%m-%d")
                    
                except ValueError as e:
                    logger.warning(f"è§£ææ³¨å†Œæ—¥æœŸå¤±è´¥: {registration_date_str}, é”™è¯¯: {e}")
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ³¨å†Œæ—¥æœŸï¼Œä½¿ç”¨é»˜è®¤é€»è¾‘ï¼ˆåŸºäºå¯†é’¥ç´¢å¼•ï¼‰
            logger.debug(f"å¯†é’¥ {key_index} æœªæ‰¾åˆ°æ³¨å†Œæ—¥æœŸï¼Œä½¿ç”¨é»˜è®¤è®¡ç®—æ–¹å¼")
            
            # ä½¿ç”¨å¯†é’¥ç´¢å¼•ä½œä¸ºåç§»é‡ï¼Œç¡®ä¿ä¸åŒå¯†é’¥æœ‰ä¸åŒçš„é‡ç½®æ—¶é—´
            offset_days = (key_index - 1) * 7  # æ¯ä¸ªå¯†é’¥ç›¸å·®7å¤©ï¼Œé¿å…è¿‡äºæ¥è¿‘
            
            # è®¡ç®—ä¸‹ä¸ªæœˆçš„åŒä¸€å¤©ä½œä¸ºé‡ç½®æ—¶é—´
            if now.month == 12:
                next_month = now.replace(year=now.year + 1, month=1, day=1)
            else:
                next_month = now.replace(month=now.month + 1, day=1)
            
            # æ·»åŠ åç§»é‡
            reset_date = next_month + timedelta(days=offset_days)
            
            # ç¡®ä¿æ—¥æœŸä¸è¶…è¿‡ä¸‹ä¸ªæœˆçš„æœ€åä¸€å¤©
            last_day_of_month = calendar.monthrange(reset_date.year, reset_date.month)[1]
            if reset_date.day > last_day_of_month:
                reset_date = reset_date.replace(day=last_day_of_month)
            
            return reset_date.strftime("%Y-%m-%d")
            
        except Exception as e:
            logger.warning(f"è®¡ç®—é‡ç½®æ—¶é—´å¤±è´¥: {e}")
            # å¦‚æœè®¡ç®—å¤±è´¥ï¼Œè¿”å›ä¸‹ä¸ªæœˆ1å·ä½œä¸ºé»˜è®¤å€¼
            from datetime import datetime
            now = datetime.now()
            if now.month == 12:
                return f"{now.year + 1}-01-01"
            else:
                return f"{now.year}-{now.month + 1:02d}-01"
    
    def normalize_url(self, url: str) -> str:
        """
        æ ‡å‡†åŒ–URLï¼Œç”¨äºå»é‡æ¯”è¾ƒ
        
        Args:
            url: åŸå§‹URL
            
        Returns:
            str: æ ‡å‡†åŒ–åçš„URL
        """
        try:
            # å»é™¤é¦–å°¾ç©ºç™½å­—ç¬¦
            normalized = url.strip()
            
            # å»é™¤å¼€å¤´çš„ç‰¹æ®Šç¬¦å·
            while normalized and normalized[0] in '-_*+~`!@#$%^&()[]{}|\\:;"\'<>,.?/':
                normalized = normalized[1:]
            
            # å»é™¤ç»“å°¾çš„ç‰¹æ®Šç¬¦å·
            while normalized and normalized[-1] in '-_*+~`!@#$%^&()[]{}|\\:;"\'<>,.?/':
                normalized = normalized[:-1]
            
            # å»é™¤å¤šä½™çš„ç©ºæ ¼
            normalized = ' '.join(normalized.split())
            
            # å¦‚æœæ²¡æœ‰åè®®ï¼Œæ·»åŠ https://
            if not normalized.startswith(('http://', 'https://')):
                if normalized.startswith('//'):
                    normalized = 'https:' + normalized
                else:
                    normalized = 'https://' + normalized
            
            # è§£æURLå¹¶é‡æ–°æ„å»º
            parsed = urlparse(normalized)
            
            # é‡å»ºURLï¼Œä¿ç•™å¿…è¦çš„éƒ¨åˆ†
            if 'api/v1/client/subscribe' in parsed.path:
                # å¯¹äºè®¢é˜…APIï¼Œä¿ç•™å®Œæ•´URLåŒ…æ‹¬tokenå‚æ•°
                # åªç§»é™¤clashæ ‡å¿—ç­‰éå¿…è¦å‚æ•°
                if '&flag=clash' in normalized:
                    normalized = normalized.replace('&flag=clash', '')
                if '?flag=clash' in normalized:
                    normalized = normalized.replace('?flag=clash', '')
                # ä¿æŒåŸå§‹URLä¸å˜ï¼Œå› ä¸ºtokenæ˜¯å¿…éœ€çš„
            else:
                # å¯¹äºå…¶ä»–URLï¼Œä¿ç•™å®Œæ•´è·¯å¾„ä½†å»é™¤æŸ¥è¯¢å‚æ•°
                normalized = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
            
            return normalized.lower()  # è½¬æ¢ä¸ºå°å†™ä»¥ä¾¿æ¯”è¾ƒ
            
        except Exception as e:
            logger.warning(f"URLæ ‡å‡†åŒ–å¤±è´¥: {e}")
            return url.strip().lower()
    
    def remove_duplicate_urls(self, urls: List[str]) -> tuple[List[str], Dict[str, List[int]]]:
        """
        å»é™¤é‡å¤çš„è®¢é˜…é“¾æ¥
        
        Args:
            urls: åŸå§‹URLåˆ—è¡¨
            
        Returns:
            tuple: (å»é‡åçš„URLåˆ—è¡¨, é‡å¤URLçš„ç´¢å¼•æ˜ å°„)
        """
        normalized_to_indices = {}
        unique_urls = []
        duplicate_mapping = {}
        
        logger.info("å¼€å§‹æ£€æµ‹é‡å¤çš„è®¢é˜…é“¾æ¥...")
        
        for i, url in enumerate(urls):
            if not url.strip():
                continue
                
            normalized = self.normalize_url(url.strip())
            
            if normalized in normalized_to_indices:
                # å‘ç°é‡å¤URL
                original_index = normalized_to_indices[normalized]
                if normalized not in duplicate_mapping:
                    duplicate_mapping[normalized] = [original_index]
                duplicate_mapping[normalized].append(i)
                
                logger.info(f"å‘ç°é‡å¤URL (ç´¢å¼• {i}): {url}")
                logger.info(f"  ä¸ç´¢å¼• {original_index} çš„URLé‡å¤: {urls[original_index]}")
            else:
                # æ–°çš„å”¯ä¸€URL
                normalized_to_indices[normalized] = i
                unique_urls.append(url.strip())
        
        # ç»Ÿè®¡é‡å¤æƒ…å†µ
        total_duplicates = sum(len(indices) - 1 for indices in duplicate_mapping.values())
        logger.info(f"å»é‡å®Œæˆ: åŸå§‹ {len(urls)} ä¸ªURLï¼Œå»é‡å {len(unique_urls)} ä¸ªï¼Œå‘ç° {total_duplicates} ä¸ªé‡å¤")
        
        return unique_urls, duplicate_mapping
    
    def print_duplicate_analysis(self, urls: List[str], duplicate_mapping: Dict[str, List[int]]):
        """
        æ‰“å°é‡å¤URLåˆ†æç»“æœ
        
        Args:
            urls: åŸå§‹URLåˆ—è¡¨
            duplicate_mapping: é‡å¤URLæ˜ å°„
        """
        if not duplicate_mapping:
            print("âœ… æœªå‘ç°é‡å¤çš„è®¢é˜…é“¾æ¥")
            return
        
        print("\n" + "=" * 60)
        print("é‡å¤è®¢é˜…é“¾æ¥åˆ†æ")
        print("=" * 60)
        
        for normalized_url, indices in duplicate_mapping.items():
            print(f"\nğŸ” é‡å¤ç»„ (å…± {len(indices)} ä¸ª):")
            for i, index in enumerate(indices):
                status_icon = "ğŸ“Œ" if i == 0 else "ğŸ”„"
                print(f"  {status_icon} ç´¢å¼• {index}: {urls[index]}")
            
            # æ˜¾ç¤ºæ ‡å‡†åŒ–åçš„URL
            print(f"  æ ‡å‡†åŒ–URL: {normalized_url}")
        
        print(f"\nğŸ“Š é‡å¤ç»Ÿè®¡:")
        print(f"  æ€»é‡å¤ç»„æ•°: {len(duplicate_mapping)}")
        total_duplicates = sum(len(indices) - 1 for indices in duplicate_mapping.values())
        print(f"  æ€»é‡å¤URLæ•°: {total_duplicates}")
        print(f"  åŸå§‹URLæ•°: {len(urls)}")
        print(f"  å»é‡åURLæ•°: {len(urls) - total_duplicates}")
        print("=" * 60)
    
    def test_proxy(self) -> Dict:
        """
        æµ‹è¯•ä»£ç†è¿æ¥
        
        Returns:
            Dict: ä»£ç†æµ‹è¯•ç»“æœ
        """
        test_result = {
            'proxy_enabled': self.use_proxy,
            'proxy_config': PROXY_CONFIG if self.use_proxy else None,
            'test_urls': [],
            'overall_status': 'unknown'
        }
        
        if not self.use_proxy:
            test_result['overall_status'] = 'no_proxy'
            return test_result
        
        # æµ‹è¯•URLåˆ—è¡¨
        test_urls = [
            'http://httpbin.org/ip',
            'https://httpbin.org/ip',
            'http://ip-api.com/json',
            'https://api.ipify.org?format=json'
        ]
        
        logger.info("å¼€å§‹æµ‹è¯•ä»£ç†è¿æ¥...")
        
        for url in test_urls:
            try:
                start_time = time.time()
                response = self.session.get(url, timeout=10, verify=False)
                response_time = time.time() - start_time
                
                if response.status_code == 200:
                    # å°è¯•è§£æå“åº”å†…å®¹
                    try:
                        ip_info = response.json()
                        if 'origin' in ip_info:
                            ip = ip_info['origin']
                        elif 'query' in ip_info:
                            ip = ip_info['query']
                        elif 'ip' in ip_info:
                            ip = ip_info['ip']
                        else:
                            ip = "æœªçŸ¥"
                    except:
                        ip = "è§£æå¤±è´¥"
                    
                    test_result['test_urls'].append({
                        'url': url,
                        'status': 'success',
                        'response_time': round(response_time, 2),
                        'status_code': response.status_code,
                        'ip_address': ip,
                        'error': None
                    })
                    logger.info(f"ä»£ç†æµ‹è¯•æˆåŠŸ: {url} -> IP: {ip}")
                else:
                    test_result['test_urls'].append({
                        'url': url,
                        'status': 'http_error',
                        'response_time': round(response_time, 2),
                        'status_code': response.status_code,
                        'ip_address': None,
                        'error': f"HTTP {response.status_code}"
                    })
                    logger.warning(f"ä»£ç†æµ‹è¯•HTTPé”™è¯¯: {url}, çŠ¶æ€ç : {response.status_code}")
                    
            except Exception as e:
                test_result['test_urls'].append({
                    'url': url,
                    'status': 'error',
                    'response_time': 0,
                    'status_code': 0,
                    'ip_address': None,
                    'error': str(e)
                })
                logger.error(f"ä»£ç†æµ‹è¯•å¤±è´¥: {url}, é”™è¯¯: {e}")
        
        # åˆ¤æ–­æ•´ä½“çŠ¶æ€
        success_count = sum(1 for t in test_result['test_urls'] if t['status'] == 'success')
        if success_count == 0:
            test_result['overall_status'] = 'failed'
        elif success_count == len(test_urls):
            test_result['overall_status'] = 'success'
        else:
            test_result['overall_status'] = 'partial'
        
        return test_result
    
    def print_proxy_test_result(self, test_result: Dict):
        """æ‰“å°ä»£ç†æµ‹è¯•ç»“æœ"""
        print("\n" + "=" * 60)
        print("ä»£ç†è¿æ¥æµ‹è¯•ç»“æœ")
        print("=" * 60)
        
        if test_result['proxy_enabled']:
            print(f"ä»£ç†çŠ¶æ€: {'âœ… å·²å¯ç”¨' if test_result['overall_status'] != 'failed' else 'âŒ è¿æ¥å¤±è´¥'}")
            print(f"ä»£ç†é…ç½®: {test_result['proxy_config']}")
            print(f"æ•´ä½“çŠ¶æ€: {test_result['overall_status']}")
            
            print(f"\nè¯¦ç»†æµ‹è¯•ç»“æœ:")
            for i, test in enumerate(test_result['test_urls'], 1):
                status_icon = "âœ…" if test['status'] == 'success' else "âŒ"
                print(f"{i}. {status_icon} {test['url']}")
                print(f"   çŠ¶æ€: {test['status']}")
                print(f"   å“åº”æ—¶é—´: {test['response_time']}ç§’")
                print(f"   çŠ¶æ€ç : {test['status_code']}")
                
                if test['ip_address']:
                    print(f"   æ£€æµ‹åˆ°çš„IP: {test['ip_address']}")
                
                if test['error']:
                    print(f"   é”™è¯¯: {test['error']}")
                print()
        else:
            print("ä»£ç†çŠ¶æ€: æœªå¯ç”¨")
        
        print("=" * 60)
    
    def clean_and_validate_url(self, url: str) -> tuple[bool, str, str]:
        """
        æ¸…ç†å’ŒéªŒè¯URLæ ¼å¼ï¼Œå¹¶è‡ªåŠ¨æ·»åŠ Clashæ ¼å¼æ ‡è¯†
        
        Args:
            url: åŸå§‹URLå­—ç¬¦ä¸²
            
        Returns:
            tuple: (æ˜¯å¦æœ‰æ•ˆ, æ¸…ç†åçš„URL, é”™è¯¯ä¿¡æ¯)
        """
        # å»é™¤é¦–å°¾ç©ºç™½å­—ç¬¦
        cleaned_url = url.strip()
        
        # å»é™¤å¼€å¤´çš„ç‰¹æ®Šç¬¦å·
        while cleaned_url and cleaned_url[0] in '-_*+~`!@#$%^&()[]{}|\\:;"\'<>,.?/':
            cleaned_url = cleaned_url[1:]
        
        # å»é™¤ç»“å°¾çš„ç‰¹æ®Šç¬¦å·
        while cleaned_url and cleaned_url[-1] in '-_*+~`!@#$%^&()[]{}|\\:;"\'<>,.?/':
            cleaned_url = cleaned_url[:-1]
        
        # å»é™¤å¤šä½™çš„ç©ºæ ¼
        cleaned_url = ' '.join(cleaned_url.split())
        
        # å¦‚æœæ¸…ç†åä¸ºç©ºï¼Œè¿”å›é”™è¯¯
        if not cleaned_url:
            return False, "", "URLä¸ºç©ºæˆ–åªåŒ…å«ç‰¹æ®Šç¬¦å·"
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºClashè®¢é˜…é“¾æ¥ï¼Œå¦‚æœæ˜¯åˆ™è‡ªåŠ¨æ·»åŠ &flag=clash
        if 'api/v1/client/subscribe' in cleaned_url:
            if '&flag=clash' not in cleaned_url:
                # æ ¹æ®URLæ˜¯å¦å·²æœ‰å‚æ•°æ¥å†³å®šæ·»åŠ æ–¹å¼
                if '?' in cleaned_url:
                    cleaned_url += '&flag=clash'
                else:
                    cleaned_url += '?flag=clash'
                logger.info(f"æ£€æµ‹åˆ°Clashè®¢é˜…é“¾æ¥ï¼Œå·²è‡ªåŠ¨æ·»åŠ &flag=clash: {cleaned_url}")
        
        # éªŒè¯URLæ ¼å¼
        parsed_url = urlparse(cleaned_url)
        
        # å¦‚æœæ²¡æœ‰åè®®ï¼Œå°è¯•æ·»åŠ https://
        if not parsed_url.scheme:
            if not cleaned_url.startswith('//'):
                cleaned_url = 'https://' + cleaned_url
            else:
                cleaned_url = 'https:' + cleaned_url
            parsed_url = urlparse(cleaned_url)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰åŸŸå
        if not parsed_url.netloc:
            return False, cleaned_url, "æ— æ•ˆçš„URLæ ¼å¼ï¼šç¼ºå°‘åŸŸå"
        
        # æ£€æŸ¥åè®®æ˜¯å¦æ”¯æŒ
        if parsed_url.scheme not in ['http', 'https']:
            return False, cleaned_url, f"ä¸æ”¯æŒçš„åè®®ï¼š{parsed_url.scheme}"
        
        return True, cleaned_url, ""
    
    def check_subscription_url(self, url: str) -> Dict:
        """
        æ£€æµ‹è®¢é˜…é“¾æ¥çš„å¯ç”¨æ€§
        
        Args:
            url: è®¢é˜…é“¾æ¥URL
            
        Returns:
            Dict: åŒ…å«æ£€æµ‹ç»“æœçš„å­—å…¸
        """
        result = {
            'url': url,
            'status': 'unknown',
            'available': False,
            'response_time': 0,
            'status_code': 0,
            'content_length': 0,
            'error': None,
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'cleaned_url': url,
            'node_analysis': {},
            'traffic_info': {},
            'proxy_used': self.use_proxy
        }
        
        try:
            logger.info(f"æ­£åœ¨æ£€æµ‹è®¢é˜…é“¾æ¥: {url}")
            
            # æ¸…ç†å’ŒéªŒè¯URLæ ¼å¼
            is_valid, cleaned_url, error_msg = self.clean_and_validate_url(url)
            if not is_valid:
                result['error'] = error_msg
                result['status'] = 'invalid_url'
                return result
            
            # æ›´æ–°æ¸…ç†åçš„URL
            result['cleaned_url'] = cleaned_url
            logger.info(f"URLå·²æ¸…ç†: {cleaned_url}")
            
            # éªŒè¯URLæ ¼å¼
            parsed_url = urlparse(cleaned_url)
            if not parsed_url.scheme or not parsed_url.netloc:
                result['error'] = "æ— æ•ˆçš„URLæ ¼å¼"
                result['status'] = 'invalid_url'
                return result
            
            # å‘é€è¯·æ±‚
            start_time = time.time()
            response = self.session.get(
                cleaned_url, 
                timeout=REQUEST_TIMEOUT,
                allow_redirects=True,
                verify=False  # å¿½ç•¥SSLè¯ä¹¦éªŒè¯
            )
            response_time = time.time() - start_time
            
            result['response_time'] = round(response_time, 2)
            result['status_code'] = response.status_code
            result['content_length'] = len(response.content)
            
            # åˆ¤æ–­å“åº”çŠ¶æ€
            if response.status_code == 200:
                # æ£€æŸ¥å†…å®¹æ˜¯å¦æœ‰æ•ˆ
                if self._is_valid_subscription_content(response.content):
                    result['status'] = 'available'
                    result['available'] = True
                    logger.info(f"è®¢é˜…é“¾æ¥å¯ç”¨: {url}")
                    
                    # ä½¿ç”¨åŒé‡åˆ†ææ–¹æ³•ï¼šä¼ é€’åŸå§‹URLï¼Œè€Œä¸æ˜¯å¸¦clashæ ‡å¿—çš„URL
                    original_clean_url = url.replace('&flag=clash', '').replace('?flag=clash', '')
                    analysis_result = self._dual_analyze_subscription(original_clean_url, response.content)
                    result['node_analysis'] = analysis_result
                    result['traffic_info'] = analysis_result.get('traffic_info', {})
                    
                    logger.info(f"èŠ‚ç‚¹åˆ†æç»“æœ: {analysis_result['total_nodes']} ä¸ªèŠ‚ç‚¹")
                    
                    # è®°å½•æµé‡ä¿¡æ¯åˆ°æ—¥å¿—
                    traffic_info = analysis_result.get('traffic_info', {})
                    if traffic_info.get('total_traffic'):
                        logger.info(f"æ€»æµé‡: {traffic_info['total_traffic']} {traffic_info['traffic_unit']}")
                    if traffic_info.get('used_traffic'):
                        logger.info(f"å·²ç”¨æµé‡: {traffic_info['used_traffic']} {traffic_info['traffic_unit']}")
                    if traffic_info.get('remaining_traffic'):
                        logger.info(f"å‰©ä½™æµé‡: {traffic_info['remaining_traffic']} {traffic_info['traffic_unit']}")
                    if traffic_info.get('expire_date'):
                        logger.info(f"è¿‡æœŸæ—¶é—´: {traffic_info['expire_date']}")
                    
                    # å‘é€é’‰é’‰é€šçŸ¥
                    logger.info(f"å‘é€é’‰é’‰é€šçŸ¥: {url}")
                    notification_success = self.send_dingtalk_notification(result)
                    if notification_success:
                        logger.info("âœ… é’‰é’‰é€šçŸ¥å‘é€æˆåŠŸ")
                    else:
                        logger.warning("âŒ é’‰é’‰é€šçŸ¥å‘é€å¤±è´¥")
                else:
                    result['status'] = 'invalid_content'
                    result['error'] = "å“åº”å†…å®¹æ— æ•ˆ"
                    logger.warning(f"è®¢é˜…é“¾æ¥å†…å®¹æ— æ•ˆ: {url}")
            else:
                result['status'] = 'http_error'
                result['error'] = f"HTTPçŠ¶æ€ç : {response.status_code}"
                logger.warning(f"è®¢é˜…é“¾æ¥HTTPé”™è¯¯: {url}, çŠ¶æ€ç : {response.status_code}")
                
        except requests.exceptions.Timeout:
            result['status'] = 'timeout'
            result['error'] = "è¯·æ±‚è¶…æ—¶"
            logger.error(f"è®¢é˜…é“¾æ¥è¯·æ±‚è¶…æ—¶: {url}")
        except requests.exceptions.ConnectionError:
            result['status'] = 'connection_error'
            result['error'] = "è¿æ¥é”™è¯¯"
            logger.error(f"è®¢é˜…é“¾æ¥è¿æ¥é”™è¯¯: {url}")
        except requests.exceptions.RequestException as e:
            result['status'] = 'request_error'
            result['error'] = str(e)
            logger.error(f"è®¢é˜…é“¾æ¥è¯·æ±‚é”™è¯¯: {url}, é”™è¯¯: {e}")
        except Exception as e:
            result['status'] = 'unknown_error'
            result['error'] = str(e)
            logger.error(f"è®¢é˜…é“¾æ¥æ£€æµ‹æœªçŸ¥é”™è¯¯: {url}, é”™è¯¯: {e}")
        
        return result
    
    def _is_valid_subscription_content(self, content: bytes) -> bool:
        """
        åˆ¤æ–­è®¢é˜…å†…å®¹æ˜¯å¦æœ‰æ•ˆ
        
        Args:
            content: å“åº”å†…å®¹
            
        Returns:
            bool: å†…å®¹æ˜¯å¦æœ‰æ•ˆ
        """
        try:
            # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            content_str = content.decode('utf-8', errors='ignore')
            
            # æ·»åŠ è°ƒè¯•ä¿¡æ¯
            logger.debug(f"å†…å®¹é•¿åº¦: {len(content_str)}")
            logger.debug(f"å†…å®¹é¢„è§ˆ: {content_str[:200]}...")
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å¸¸è§çš„è®¢é˜…æ ¼å¼æ ‡è¯†
            valid_indicators = [
                'vmess://', 'vless://', 'trojan://', 'ss://', 'ssr://',
                'http://', 'https://', 'socks5://',
                'server=', 'port=', 'password=',
                'vmess', 'vless', 'trojan', 'shadowsocks'
            ]
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«æœ‰æ•ˆå†…å®¹ï¼ˆæ”¾å®½è¦æ±‚ï¼‰
            if len(content_str.strip()) < 5:
                logger.debug("å†…å®¹é•¿åº¦ä¸è¶³5å­—ç¬¦")
                return False
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«è®¢é˜…æ ¼å¼æ ‡è¯†
            has_valid_format = any(indicator in content_str.lower() for indicator in valid_indicators)
            logger.debug(f"åŒ…å«æœ‰æ•ˆæ ¼å¼æ ‡è¯†: {has_valid_format}")
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«æ˜æ˜¾çš„é”™è¯¯ä¿¡æ¯
            error_indicators = [
                'error', 'not found', '404', '403', '500', '502', '503',
                'access denied', 'forbidden', 'unauthorized'
            ]
            has_error = any(error in content_str.lower() for error in error_indicators)
            logger.debug(f"åŒ…å«é”™è¯¯ä¿¡æ¯: {has_error}")
            
            # å¦‚æœå†…å®¹é•¿åº¦è¶³å¤Ÿä¸”æ²¡æœ‰é”™è¯¯ä¿¡æ¯ï¼Œå°±è®¤ä¸ºæ˜¯æœ‰æ•ˆçš„
            # æ”¾å®½æ ¼å¼è¦æ±‚ï¼Œå› ä¸ºæœ‰äº›è®¢é˜…å¯èƒ½ä½¿ç”¨è‡ªå®šä¹‰æ ¼å¼
            if len(content_str.strip()) > 10 and not has_error:
                logger.debug("å†…å®¹é•¿åº¦è¶³å¤Ÿä¸”æ— é”™è¯¯ä¿¡æ¯ï¼Œè®¤ä¸ºæœ‰æ•ˆ")
                return True
            
            result = has_valid_format and not has_error
            logger.debug(f"æœ€ç»ˆéªŒè¯ç»“æœ: {result}")
            return result
            
        except Exception as e:
            logger.warning(f"å†…å®¹éªŒè¯å¤±è´¥: {e}")
            return False
    
    def _try_base64_decode(self, content: str) -> Optional[str]:
        """
        å°è¯•Base64è§£ç å†…å®¹
        
        Args:
            content: åŸå§‹å†…å®¹å­—ç¬¦ä¸²
            
        Returns:
            Optional[str]: è§£ç åçš„å†…å®¹ï¼Œå¦‚æœè§£ç å¤±è´¥è¿”å›None
        """
        try:
            import base64
            
            # å»é™¤æ‰€æœ‰ç©ºç™½å­—ç¬¦å’Œæ¢è¡Œç¬¦
            cleaned_content = content.replace('\n', '').replace('\r', '').replace(' ', '').replace('\t', '').strip()
            
            # æ£€æŸ¥æ˜¯å¦å¯èƒ½æ˜¯Base64ç¼–ç 
            if not self._looks_like_base64(content):
                logger.debug("å†…å®¹çœ‹èµ·æ¥ä¸åƒBase64ç¼–ç ")
                return None
            
            # å°è¯•è§£ç 
            decoded_bytes = base64.b64decode(cleaned_content)
            decoded_str = decoded_bytes.decode('utf-8', errors='ignore')
            
            logger.debug(f"Base64è§£ç æˆåŠŸï¼ŒåŸå§‹é•¿åº¦: {len(cleaned_content)}, è§£ç åé•¿åº¦: {len(decoded_str)}")
            return decoded_str
            
        except Exception as e:
            logger.debug(f"Base64è§£ç å¤±è´¥: {e}")
            return None
    
    def _looks_like_base64(self, content: str) -> bool:
        """
        åˆ¤æ–­å†…å®¹æ˜¯å¦çœ‹èµ·æ¥åƒBase64ç¼–ç 
        
        Args:
            content: å†…å®¹å­—ç¬¦ä¸²
            
        Returns:
            bool: æ˜¯å¦åƒBase64ç¼–ç 
        """
        if not content:
            return False
        
        # å»é™¤ç©ºç™½å­—ç¬¦åå†æ£€æŸ¥
        cleaned = content.replace('\n', '').replace('\r', '').replace(' ', '').replace('\t', '')
        
        if len(cleaned) < 20:  # å¤ªçŸ­å¯èƒ½ä¸æ˜¯Base64
            return False
        
        # Base64ç¼–ç é€šå¸¸åŒ…å«å­—æ¯ã€æ•°å­—ã€+ã€/ã€=
        valid_chars = set('ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/=')
        content_chars = set(cleaned)
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ— æ•ˆå­—ç¬¦ï¼ˆå…è®¸å°‘é‡æ— æ•ˆå­—ç¬¦ï¼Œå¯èƒ½æ˜¯æ¢è¡Œç­‰ï¼‰
        invalid_chars = content_chars - valid_chars
        if len(invalid_chars) > 0 and len(invalid_chars) / len(content_chars) > 0.1:
            return False
        
        # æ£€æŸ¥Base64ç‰¹å¾ï¼šå¤§éƒ¨åˆ†å­—ç¬¦åº”è¯¥æ˜¯å­—æ¯æ•°å­—
        alphanumeric_count = sum(1 for c in cleaned if c.isalnum())
        if alphanumeric_count / len(cleaned) < 0.6:
            return False
        
        return True
    
    def _is_valid_node_line(self, line: str) -> bool:
        """
        åˆ¤æ–­ä¸€è¡Œæ˜¯å¦åŒ…å«æœ‰æ•ˆçš„èŠ‚ç‚¹ä¿¡æ¯
        
        Args:
            line: è¡Œå†…å®¹
            
        Returns:
            bool: æ˜¯å¦åŒ…å«æœ‰æ•ˆèŠ‚ç‚¹ä¿¡æ¯
        """
        if not line or line.startswith('#'):
            return False
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¸¸è§çš„èŠ‚ç‚¹é…ç½®å‚æ•°
        node_indicators = [
            'server=', 'port=', 'password=', 'method=', 'protocol=',
            'obfs=', 'obfs_param=', 'remarks=', 'group=',
            'name=', 'type=', 'uuid=', 'path=', 'host='
        ]
        
        # å¦‚æœåŒ…å«å¤šä¸ªèŠ‚ç‚¹æŒ‡ç¤ºç¬¦ï¼Œè®¤ä¸ºæ˜¯æœ‰æ•ˆçš„èŠ‚ç‚¹è¡Œ
        indicator_count = sum(1 for indicator in node_indicators if indicator in line)
        return indicator_count >= 2
    
    def _analyze_subscription_content(self, content: bytes) -> Dict:
        """
        åˆ†æè®¢é˜…å†…å®¹ï¼Œæå–èŠ‚ç‚¹æ•°é‡å’Œæµé‡ä¿¡æ¯
        
        Args:
            content: å“åº”å†…å®¹
            
        Returns:
            Dict: åŒ…å«åˆ†æç»“æœçš„å­—å…¸
        """
        try:
            content_str = content.decode('utf-8', errors='ignore')
            logger.debug(f"åŸå§‹å†…å®¹é•¿åº¦: {len(content_str)}")
            logger.debug(f"åŸå§‹å†…å®¹é¢„è§ˆ: {content_str[:200]}...")
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºClash YAMLæ ¼å¼
            if self._is_clash_yaml_format(content_str):
                logger.info("æ£€æµ‹åˆ°Clash YAMLæ ¼å¼ï¼Œä½¿ç”¨ä¸“ç”¨è§£æå™¨")
                return self._analyze_clash_yaml_content(content_str)
            
            # å°è¯•Base64è§£ç 
            decoded_content = self._try_base64_decode(content_str)
            if decoded_content:
                logger.debug(f"Base64è§£ç æˆåŠŸï¼Œè§£ç åé•¿åº¦: {len(decoded_content)}")
                logger.debug(f"è§£ç åå†…å®¹é¢„è§ˆ: {decoded_content[:200]}...")
                # ä½¿ç”¨è§£ç åçš„å†…å®¹è¿›è¡Œåˆ†æ
                content_to_analyze = decoded_content
            else:
                logger.debug("Base64è§£ç å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å†…å®¹")
                content_to_analyze = content_str
            
            # ç»Ÿè®¡èŠ‚ç‚¹æ•°é‡
            node_count = 0
            node_types = {
                'vmess': 0,
                'vless': 0,
                'trojan': 0,
                'ss': 0,
                'ssr': 0,
                'hysteria': 0,
                'hysteria2': 0,
                'http': 0,
                'https': 0,
                'socks5': 0,
                'other': 0
            }
            
            # ç»Ÿè®¡å„ç§åè®®èŠ‚ç‚¹
            for line in content_to_analyze.split('\n'):
                line = line.strip()
                if not line:
                    continue
                    
                if line.startswith('vmess://'):
                    node_count += 1
                    node_types['vmess'] += 1
                elif line.startswith('vless://'):
                    node_count += 1
                    node_types['vless'] += 1
                elif line.startswith('trojan://'):
                    node_count += 1
                    node_types['trojan'] += 1
                elif line.startswith('ss://'):
                    node_count += 1
                    node_types['ss'] += 1
                elif line.startswith('ssr://'):
                    node_count += 1
                    node_types['ssr'] += 1
                elif line.startswith('hysteria2://'):
                    node_count += 1
                    node_types['hysteria2'] += 1
                elif line.startswith('hysteria://'):
                    node_count += 1
                    node_types['hysteria'] += 1
                elif line.startswith('http://'):
                    node_count += 1
                    node_types['http'] += 1
                elif line.startswith('https://'):
                    node_count += 1
                    node_types['https'] += 1
                elif line.startswith('socks5://'):
                    node_count += 1
                    node_types['socks5'] += 1
                elif 'server=' in line and 'port=' in line:
                    # å¯èƒ½æ˜¯é…ç½®æ–‡ä»¶æ ¼å¼
                    node_count += 1
                    node_types['other'] += 1
                elif self._is_valid_node_line(line):
                    # å…¶ä»–æœ‰æ•ˆçš„èŠ‚ç‚¹è¡Œ
                    node_count += 1
                    node_types['other'] += 1
            
            logger.debug(f"æ£€æµ‹åˆ°èŠ‚ç‚¹æ•°é‡: {node_count}")
            logger.debug(f"èŠ‚ç‚¹ç±»å‹åˆ†å¸ƒ: {node_types}")
            
            # å°è¯•æå–æµé‡ä¿¡æ¯
            traffic_info = self._extract_traffic_info(content_to_analyze)
            
            return {
                'total_nodes': node_count,
                'node_types': node_types,
                'traffic_info': traffic_info,
                'content_preview': content_to_analyze[:200] + '...' if len(content_to_analyze) > 200 else content_to_analyze,
                'is_base64_decoded': decoded_content is not None,
                'is_clash_format': False
            }
            
        except Exception as e:
            logger.warning(f"å†…å®¹åˆ†æå¤±è´¥: {e}")
            return {
                'total_nodes': 0,
                'node_types': {},
                'traffic_info': {},
                'content_preview': 'å†…å®¹è§£æå¤±è´¥',
                'is_base64_decoded': False,
                'is_clash_format': False
            }
    
    def _extract_traffic_info(self, content: str) -> Dict:
        """
        ä»è®¢é˜…å†…å®¹ä¸­æå–æµé‡ä¿¡æ¯
        
        Args:
            content: è®¢é˜…å†…å®¹å­—ç¬¦ä¸²
            
        Returns:
            Dict: æµé‡ä¿¡æ¯å­—å…¸
        """
        traffic_info = {
            'total_traffic': None,
            'used_traffic': None,
            'remaining_traffic': None,
            'traffic_unit': 'GB',
            'expire_date': None,
            'reset_date': None
        }
        
        try:
            # å…ˆå°è¯•URLè§£ç ï¼Œä»¥å¤„ç†ç¼–ç åçš„ä¸­æ–‡
            import urllib.parse
            decoded_content = urllib.parse.unquote(content)
            
            content_lower = decoded_content.lower()
            logger.debug(f"å¼€å§‹æå–æµé‡ä¿¡æ¯ï¼Œå†…å®¹é•¿åº¦: {len(content)}")
            logger.debug(f"URLè§£ç åå†…å®¹é¢„è§ˆ: {decoded_content[:500]}...")
            
            # æŸ¥æ‰¾æµé‡ç›¸å…³ä¿¡æ¯
            import re
            
            # åŒ¹é…æ€»æµé‡ (å¦‚: 100GB, 500MB, 1TB)
            total_patterns = [
                r'æ€»æµé‡[ï¼š:]\s*(\d+(?:\.\d+)?)\s*(gb|mb|tb|b)',
                r'total[:\s]*(\d+(?:\.\d+)?)\s*(gb|mb|tb|b)',
                r'(\d+(?:\.\d+)?)\s*(gb|mb|tb|b)\s*æ€»æµé‡',
                r'(\d+(?:\.\d+)?)\s*(gb|mb|tb|b)\s*total',
                r'æµé‡[ï¼š:]\s*(\d+(?:\.\d+)?)\s*(gb|mb|tb|b)',
                r'bandwidth[:\s]*(\d+(?:\.\d+)?)\s*(gb|mb|tb|b)',
                r'(\d+(?:\.\d+)?)\s*(gb|mb|tb|b)',
                r'(\d+(?:\.\d+)?)\s*(gb|mb|tb|b)\s*æµé‡',
                # æ·»åŠ æ›´å¤šClashè®¢é˜…ä¸­å¸¸è§çš„æµé‡æ ¼å¼
                r'upload[:\s]*(\d+(?:\.\d+)?)\s*(gb|mb|tb|b)',
                r'download[:\s]*(\d+(?:\.\d+)?)\s*(gb|mb|tb|b)',
                r'quota[:\s]*(\d+(?:\.\d+)?)\s*(gb|mb|tb|b)'
            ]
            
            for pattern in total_patterns:
                match = re.search(pattern, content_lower)
                if match:
                    value = float(match.group(1))
                    unit = match.group(2).upper()
                    traffic_info['total_traffic'] = value
                    traffic_info['traffic_unit'] = unit
                    logger.debug(f"æ‰¾åˆ°æ€»æµé‡: {value} {unit}")
                    break
            
            # å·²ç”¨æµé‡
            used_patterns = [
                r'å·²ç”¨æµé‡[ï¼š:]\s*(\d+(?:\.\d+)?)\s*(gb|mb|tb|b)',
                r'used[:\s]*(\d+(?:\.\d+)?)\s*(gb|mb|tb|b)',
                r'(\d+(?:\.\d+)?)\s*(gb|mb|tb|b)\s*å·²ç”¨',
                r'(\d+(?:\.\d+)?)\s*(gb|mb|tb|b)\s*used',
                r'æ¶ˆè€—[ï¼š:]\s*(\d+(?:\.\d+)?)\s*(gb|mb|tb|b)',
                r'consumed[:\s]*(\d+(?:\.\d+)?)\s*(gb|mb|tb|b)',
                # æ·»åŠ æ›´å¤šæ ¼å¼
                r'uploaded[:\s]*(\d+(?:\.\d+)?)\s*(gb|mb|tb|b)',
                r'downloaded[:\s]*(\d+(?:\.\d+)?)\s*(gb|mb|tb|b)'
            ]
            
            for pattern in used_patterns:
                match = re.search(pattern, content_lower)
                if match:
                    value = float(match.group(1))
                    unit = match.group(2).upper()
                    traffic_info['used_traffic'] = value
                    logger.debug(f"æ‰¾åˆ°å·²ç”¨æµé‡: {value} {unit}")
                    break
            
            # å‰©ä½™æµé‡ - å¢å¼ºå¯¹URLç¼–ç ä¸­æ–‡çš„æ”¯æŒ
            remaining_patterns = [
                r'å‰©ä½™æµé‡[ï¼š:]\s*(\d+(?:\.\d+)?)\s*(gb|mb|tb|b)',
                r'remaining[:\s]*(\d+(?:\.\d+)?)\s*(gb|mb|tb|b)',
                r'(\d+(?:\.\d+)?)\s*(gb|mb|tb|b)\s*å‰©ä½™',
                r'(\d+(?:\.\d+)?)\s*(gb|mb|tb|b)\s*remaining',
                r'å¯ç”¨[ï¼š:]\s*(\d+(?:\.\d+)?)\s*(gb|mb|tb|b)',
                r'available[:\s]*(\d+(?:\.\d+)?)\s*(gb|mb|tb|b)',
                r'(\d+(?:\.\d+)?)\s*(gb|mb|tb|b)\s*å¯ç”¨',
                r'(\d+(?:\.\d+)?)\s*(gb|mb|tb|b)\s*available',
                # æ·»åŠ æ›´å¤šæ ¼å¼
                r'left[:\s]*(\d+(?:\.\d+)?)\s*(gb|mb|tb|b)',
                r'balance[:\s]*(\d+(?:\.\d+)?)\s*(gb|mb|tb|b)',
                # æ”¯æŒURLç¼–ç åçš„ä¸­æ–‡
                r'%E5%89%A9%E4%BD%99%E6%B5%81%E9%87%8F.*?(\d+(?:\.\d+)?)\s*(gb|mb|tb|b)',
                # æ”¯æŒç›´æ¥ä»èŠ‚ç‚¹åç§°ä¸­æå–æµé‡ä¿¡æ¯
                r'#.*?(\d+(?:\.\d+)?)\s*(gb|mb|tb|b)',
                r'#.*å‰©ä½™.*?(\d+(?:\.\d+)?)\s*(gb|mb|tb|b)'
            ]
            
            for pattern in remaining_patterns:
                match = re.search(pattern, content_lower)
                if match:
                    value = float(match.group(1))
                    unit = match.group(2).upper()
                    traffic_info['remaining_traffic'] = value
                    logger.debug(f"æ‰¾åˆ°å‰©ä½™æµé‡: {value} {unit}")
                    break
            
            # æ™ºèƒ½è®¡ç®—ç¼ºå¤±çš„æµé‡ä¿¡æ¯
            self._calculate_missing_traffic_info(traffic_info)
            
            # æŸ¥æ‰¾è¿‡æœŸæ—¶é—´ - æ”¯æŒæ›´å¤šæ ¼å¼
            expire_patterns = [
                r'è¿‡æœŸæ—¶é—´[ï¼š:]\s*(\d{4}[-/]\d{1,2}[-/]\d{1,2})',
                r'expire[:\s]*(\d{4}[-/]\d{1,2}[-/]\d{1,2})',
                r'åˆ°æœŸæ—¶é—´[ï¼š:]\s*(\d{4}[-/]\d{1,2}[-/]\d{1,2})',
                r'(\d{4}[-/]\d{1,2}[-/]\d{1,2})\s*è¿‡æœŸ',
                r'(\d{4}[-/]\d{1,2}[-/]\d{1,2})\s*åˆ°æœŸ',
                r'æœ‰æ•ˆæœŸ[ï¼š:]\s*(\d{4}[-/]\d{1,2}[-/]\d{1,2})',
                r'valid[:\s]*(\d{4}[-/]\d{1,2}[-/]\d{1,2})',
                # æ·»åŠ æ›´å¤šæ ¼å¼
                r'expires[:\s]*(\d{4}[-/]\d{1,2}[-/]\d{1,2})',
                r'valid_until[:\s]*(\d{4}[-/]\d{1,2}[-/]\d{1,2})',
                r'end_date[:\s]*(\d{4}[-/]\d{1,2}[-/]\d{1,2})'
            ]
            
            for pattern in expire_patterns:
                match = re.search(pattern, content_lower)
                if match:
                    traffic_info['expire_date'] = match.group(1)
                    logger.debug(f"æ‰¾åˆ°è¿‡æœŸæ—¶é—´: {match.group(1)}")
                    break
            
            # æŸ¥æ‰¾é‡ç½®æ—¶é—´
            reset_patterns = [
                r'é‡ç½®æ—¶é—´[ï¼š:]\s*(\d{4}[-/]\d{1,2}[-/]\d{1,2})',
                r'reset[:\s]*(\d{4}[-/]\d{1,2}[-/]\d{1,2})',
                r'(\d{4}[-/]\d{1,2}[-/]\d{1,2})\s*é‡ç½®',
                r'æµé‡é‡ç½®[ï¼š:]\s*(\d{4}[-/]\d{1,2}[-/]\d{1,2})',
                # æ·»åŠ æ›´å¤šæ ¼å¼
                r'reset_date[:\s]*(\d{4}[-/]\d{1,2}[-/]\d{1,2})',
                r'next_reset[:\s]*(\d{4}[-/]\d{1,2}[-/]\d{1,2})'
            ]
            
            for pattern in reset_patterns:
                match = re.search(pattern, content_lower)
                if match:
                    traffic_info['reset_date'] = match.group(1)
                    logger.debug(f"æ‰¾åˆ°é‡ç½®æ—¶é—´: {match.group(1)}")
                    break
            
            # è®°å½•æå–ç»“æœ
            logger.debug(f"æµé‡ä¿¡æ¯æå–ç»“æœ: {traffic_info}")
            
        except Exception as e:
            logger.warning(f"æµé‡ä¿¡æ¯æå–å¤±è´¥: {e}")
        
        return traffic_info
    
    def _calculate_missing_traffic_info(self, traffic_info: Dict):
        """
        æ™ºèƒ½è®¡ç®—ç¼ºå¤±çš„æµé‡ä¿¡æ¯ï¼Œå¤„ç†å•ä½è½¬æ¢å’Œè®¡ç®—é€»è¾‘
        
        Args:
            traffic_info: æµé‡ä¿¡æ¯å­—å…¸
        """
        try:
            # è·å–æ‰€æœ‰å¯ç”¨çš„æµé‡ä¿¡æ¯
            total = traffic_info.get('total_traffic')
            used = traffic_info.get('used_traffic')
            remaining = traffic_info.get('remaining_traffic')
            unit = traffic_info.get('traffic_unit', 'GB')
            
            logger.debug(f"å¼€å§‹è®¡ç®—ç¼ºå¤±æµé‡ä¿¡æ¯: æ€»æµé‡={total}, å·²ç”¨={used}, å‰©ä½™={remaining}, å•ä½={unit}")
            
            # å¦‚æœä¸‰ä¸ªå€¼éƒ½æœ‰ï¼ŒéªŒè¯ä¸€è‡´æ€§
            if total is not None and used is not None and remaining is not None:
                # æ£€æŸ¥å•ä½æ˜¯å¦ä¸€è‡´ï¼Œå¦‚æœä¸ä¸€è‡´éœ€è¦è½¬æ¢
                if self._validate_traffic_consistency(float(total), float(used), float(remaining), unit):
                    logger.debug("æµé‡ä¿¡æ¯ä¸€è‡´ï¼Œæ— éœ€è®¡ç®—")
                    return
                else:
                    logger.warning("æµé‡ä¿¡æ¯ä¸ä¸€è‡´ï¼Œå°è¯•é‡æ–°è®¡ç®—")
            
            # è®¡ç®—ç¼ºå¤±çš„æµé‡ä¿¡æ¯
            if total is not None and used is not None and remaining is None:
                # è®¡ç®—å‰©ä½™æµé‡
                if self._is_same_unit(total, used, unit):
                    traffic_info['remaining_traffic'] = total - used
                    logger.debug(f"è®¡ç®—å¾—åˆ°å‰©ä½™æµé‡: {traffic_info['remaining_traffic']} {unit}")
                else:
                    # å•ä½ä¸åŒï¼Œéœ€è¦è½¬æ¢
                    converted_used = self._convert_to_unit(used, unit)
                    traffic_info['remaining_traffic'] = total - converted_used
                    logger.debug(f"å•ä½è½¬æ¢åè®¡ç®—å‰©ä½™æµé‡: {traffic_info['remaining_traffic']} {unit}")
            
            elif total is not None and remaining is not None and used is None:
                # è®¡ç®—å·²ç”¨æµé‡
                if self._is_same_unit(total, remaining, unit):
                    traffic_info['used_traffic'] = total - remaining
                    logger.debug(f"è®¡ç®—å¾—åˆ°å·²ç”¨æµé‡: {traffic_info['used_traffic']} {unit}")
                else:
                    # å•ä½ä¸åŒï¼Œéœ€è¦è½¬æ¢
                    converted_remaining = self._convert_to_unit(remaining, unit)
                    traffic_info['used_traffic'] = total - converted_remaining
                    logger.debug(f"å•ä½è½¬æ¢åè®¡ç®—å·²ç”¨æµé‡: {traffic_info['used_traffic']} {unit}")
            
            elif used is not None and remaining is not None and total is None:
                # è®¡ç®—æ€»æµé‡
                if self._is_same_unit(used, remaining, unit):
                    traffic_info['total_traffic'] = used + remaining
                    logger.debug(f"è®¡ç®—å¾—åˆ°æ€»æµé‡: {traffic_info['total_traffic']} {unit}")
                else:
                    # å•ä½ä¸åŒï¼Œéœ€è¦è½¬æ¢
                    converted_remaining = self._convert_to_unit(remaining, unit)
                    traffic_info['total_traffic'] = used + converted_remaining
                    logger.debug(f"å•ä½è½¬æ¢åè®¡ç®—æ€»æµé‡: {traffic_info['total_traffic']} {unit}")
            
            # ç»Ÿä¸€å•ä½åˆ°æ ‡å‡†å•ä½ï¼ˆGBï¼‰
            self._normalize_traffic_units(traffic_info)
            
        except Exception as e:
            logger.warning(f"æµé‡è®¡ç®—å¤±è´¥: {e}")
    
    def _validate_traffic_consistency(self, total: float, used: float, remaining: float, unit: str) -> bool:
        """
        éªŒè¯æµé‡ä¿¡æ¯çš„ä¸€è‡´æ€§
        
        Args:
            total: æ€»æµé‡
            used: å·²ç”¨æµé‡
            remaining: å‰©ä½™æµé‡
            unit: å•ä½
            
        Returns:
            bool: æ˜¯å¦ä¸€è‡´
        """
        try:
            # æ£€æŸ¥æ˜¯å¦æ»¡è¶³: total = used + remaining
            tolerance = 0.01  # å…è®¸1%çš„è¯¯å·®
            expected_total = used + remaining
            difference = abs(total - expected_total)
            
            if difference <= tolerance * total:
                logger.debug(f"æµé‡ä¿¡æ¯ä¸€è‡´: {total} = {used} + {remaining}")
                return True
            else:
                logger.warning(f"æµé‡ä¿¡æ¯ä¸ä¸€è‡´: {total} != {used} + {remaining}, å·®å€¼: {difference}")
                return False
                
        except Exception as e:
            logger.debug(f"æµé‡ä¸€è‡´æ€§éªŒè¯å¤±è´¥: {e}")
            return False
    
    def _is_same_unit(self, value1: float, value2: float, unit: str) -> bool:
        """
        æ£€æŸ¥ä¸¤ä¸ªå€¼æ˜¯å¦ä½¿ç”¨ç›¸åŒå•ä½
        
        Args:
            value1: ç¬¬ä¸€ä¸ªå€¼
            value2: ç¬¬äºŒä¸ªå€¼
            unit: å•ä½
            
        Returns:
            bool: æ˜¯å¦ç›¸åŒå•ä½
        """
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå‡è®¾å¦‚æœéƒ½ä½¿ç”¨ç›¸åŒçš„å•ä½å­—æ®µï¼Œå°±æ˜¯ç›¸åŒå•ä½
        return True
    
    def _convert_to_unit(self, value: float, target_unit: str) -> float:
        """
        å°†æµé‡å€¼è½¬æ¢åˆ°ç›®æ ‡å•ä½
        
        Args:
            value: åŸå§‹å€¼
            target_unit: ç›®æ ‡å•ä½
            
        Returns:
            float: è½¬æ¢åçš„å€¼
        """
        # è¿™é‡Œå¯ä»¥æ ¹æ®éœ€è¦å®ç°å•ä½è½¬æ¢é€»è¾‘
        # æš‚æ—¶è¿”å›åŸå€¼ï¼Œé¿å…å¤æ‚çš„å•ä½è½¬æ¢
        return value
    
    def _normalize_traffic_units(self, traffic_info: Dict):
        """
        å°†æµé‡å•ä½ç»Ÿä¸€åˆ°æ ‡å‡†å•ä½ï¼ˆGBï¼‰
        
        Args:
            traffic_info: æµé‡ä¿¡æ¯å­—å…¸
        """
        try:
            unit = traffic_info.get('traffic_unit', 'GB')
            
            # å¦‚æœå·²ç»æ˜¯GBï¼Œæ— éœ€è½¬æ¢
            if unit == 'GB':
                return
            
            # è½¬æ¢ç³»æ•°
            conversion_factors = {
                'B': 1 / (1024**3),      # B to GB
                'KB': 1 / (1024**2),     # KB to GB
                'MB': 1 / 1024,          # MB to GB
                'GB': 1,                 # GB to GB
                'TB': 1024               # TB to GB
            }
            
            factor = conversion_factors.get(unit.upper(), 1)
            
            # è½¬æ¢æµé‡å€¼
            if traffic_info.get('total_traffic') is not None:
                traffic_info['total_traffic'] = round(traffic_info['total_traffic'] * factor, 2)
            
            if traffic_info.get('used_traffic') is not None:
                traffic_info['used_traffic'] = round(traffic_info['used_traffic'] * factor, 2)
            
            if traffic_info.get('remaining_traffic') is not None:
                traffic_info['remaining_traffic'] = round(traffic_info['remaining_traffic'] * factor, 2)
            
            # æ›´æ–°å•ä½
            traffic_info['traffic_unit'] = 'GB'
            logger.debug(f"æµé‡å•ä½å·²ç»Ÿä¸€åˆ°GB")
            
        except Exception as e:
            logger.warning(f"æµé‡å•ä½ç»Ÿä¸€å¤±è´¥: {e}")
    
    def _is_clash_yaml_format(self, content: str) -> bool:
        """
        åˆ¤æ–­å†…å®¹æ˜¯å¦ä¸ºClash YAMLæ ¼å¼
        
        Args:
            content: å†…å®¹å­—ç¬¦ä¸²
            
        Returns:
            bool: æ˜¯å¦ä¸ºClash YAMLæ ¼å¼
        """
        try:
            content_lower = content.lower()
            
            # Clash YAMLæ ¼å¼çš„ç‰¹å¾
            clash_indicators = [
                'proxies:',
                'proxy-groups:',
                'rules:',
                'mixed-port:',
                'allow-lan:',
                'mode:',
                'log-level:',
                'external-controller:',
                'secret:',
                'external-ui:'
            ]
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å¤šä¸ªClashç‰¹å¾
            indicator_count = sum(1 for indicator in clash_indicators if indicator in content_lower)
            is_clash = indicator_count >= 2
            
            logger.debug(f"Clash YAMLæ ¼å¼æ£€æµ‹: æ‰¾åˆ° {indicator_count} ä¸ªç‰¹å¾ï¼Œåˆ¤æ–­ä¸º: {is_clash}")
            return is_clash
            
        except Exception as e:
            logger.debug(f"Clash YAMLæ ¼å¼æ£€æµ‹å¤±è´¥: {e}")
            return False
    
    def _analyze_clash_yaml_content(self, content: str) -> Dict:
        """
        åˆ†æClash YAMLæ ¼å¼å†…å®¹ï¼Œæå–èŠ‚ç‚¹å’Œæµé‡ä¿¡æ¯
        
        Args:
            content: Clash YAMLå†…å®¹å­—ç¬¦ä¸²
            
        Returns:
            Dict: åŒ…å«åˆ†æç»“æœçš„å­—å…¸
        """
        try:
            logger.info("å¼€å§‹è§£æClash YAMLæ ¼å¼å†…å®¹")
            logger.debug(f"åŸå§‹å†…å®¹é•¿åº¦: {len(content)}")
            logger.debug(f"å†…å®¹é¢„è§ˆ: {content[:500]}...")
            
            # ç»Ÿè®¡èŠ‚ç‚¹æ•°é‡
            node_count = 0
            node_types = {
                'vmess': 0,
                'vless': 0,
                'trojan': 0,
                'ss': 0,
                'ssr': 0,
                'http': 0,
                'https': 0,
                'socks5': 0,
                'other': 0
            }
            
            # ä½¿ç”¨æ›´å¼ºå¤§çš„YAMLè§£æé€»è¾‘
            lines = content.split('\n')
            in_proxies_section = False
            current_proxy = {}
            indent_level = 0
            
            for i, line in enumerate(lines):
                original_line = line
                line = line.rstrip()
                
                # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Š
                if not line or line.startswith('#'):
                    continue
                
                # è®¡ç®—ç¼©è¿›çº§åˆ«
                current_indent = len(line) - len(line.lstrip())
                
                # æ£€æŸ¥æ˜¯å¦è¿›å…¥proxieséƒ¨åˆ†
                if line.strip() == 'proxies:':
                    in_proxies_section = True
                    indent_level = current_indent
                    logger.debug(f"è¿›å…¥proxieséƒ¨åˆ†ï¼Œç¼©è¿›çº§åˆ«: {indent_level}")
                    continue
                
                # å¦‚æœä¸åœ¨proxieséƒ¨åˆ†ï¼Œè·³è¿‡
                if not in_proxies_section:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦ç¦»å¼€proxieséƒ¨åˆ†
                if current_indent <= indent_level and line.strip() != 'proxies:':
                    # æ£€æŸ¥æ˜¯å¦æ˜¯å…¶ä»–é¡¶çº§é…ç½®é¡¹
                    if ':' in line and not line.endswith(':'):
                        key = line.split(':')[0].strip()
                        if key in ['proxy-groups', 'rules', 'mixed-port', 'allow-lan', 'mode', 'log-level', 'dns', 'tun', 'experimental']:
                            in_proxies_section = False
                            logger.debug(f"ç¦»å¼€proxieséƒ¨åˆ†ï¼Œé‡åˆ°é…ç½®é¡¹: {key}")
                            continue
                
                # è§£æä»£ç†é…ç½®
                if in_proxies_section and current_indent > indent_level:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°çš„ä»£ç†èŠ‚ç‚¹
                    if line.strip().startswith('- name:'):
                        # ç»Ÿè®¡å‰ä¸€ä¸ªä»£ç†èŠ‚ç‚¹
                        if current_proxy:
                            self._count_proxy_node(current_proxy, node_types)
                            node_count += 1
                            logger.debug(f"è§£æä»£ç†èŠ‚ç‚¹: {current_proxy.get('name', 'unknown')}")
                        
                        # å¼€å§‹æ–°çš„ä»£ç†èŠ‚ç‚¹
                        current_proxy = {'name': line.split(':', 1)[1].strip().strip('"\'')}
                    elif current_proxy and ':' in line:
                        # è§£æä»£ç†å±æ€§
                        key_value = line.strip().split(':', 1)
                        if len(key_value) == 2:
                            key = key_value[0].strip()
                            value = key_value[1].strip().strip('"\'')
                            current_proxy[key] = value
            
            # ç»Ÿè®¡æœ€åä¸€ä¸ªä»£ç†èŠ‚ç‚¹
            if current_proxy:
                self._count_proxy_node(current_proxy, node_types)
                node_count += 1
                logger.debug(f"è§£ææœ€åä¸€ä¸ªä»£ç†èŠ‚ç‚¹: {current_proxy.get('name', 'unknown')}")
            
            logger.info(f"Clash YAMLè§£æå®Œæˆï¼Œæ£€æµ‹åˆ°èŠ‚ç‚¹æ•°é‡: {node_count}")
            logger.debug(f"èŠ‚ç‚¹ç±»å‹åˆ†å¸ƒ: {node_types}")
            
            # å°è¯•æå–æµé‡ä¿¡æ¯
            traffic_info = self._extract_traffic_info(content)
            
            # å¦‚æœæ²¡æœ‰ä»å†…å®¹ä¸­æå–åˆ°æµé‡ä¿¡æ¯ï¼Œå°è¯•ä»URLå‚æ•°ä¸­æå–
            if not traffic_info.get('total_traffic') and not traffic_info.get('remaining_traffic'):
                # è¿™é‡Œå¯ä»¥æ·»åŠ ä»URLå‚æ•°ä¸­æå–æµé‡ä¿¡æ¯çš„é€»è¾‘
                pass
            
            return {
                'total_nodes': node_count,
                'node_types': node_types,
                'traffic_info': traffic_info,
                'content_preview': content[:200] + '...' if len(content) > 200 else content,
                'is_base64_decoded': False,
                'is_clash_format': True
            }
            
        except Exception as e:
            logger.warning(f"Clash YAMLå†…å®¹åˆ†æå¤±è´¥: {e}")
            import traceback
            logger.debug(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return {
                'total_nodes': 0,
                'node_types': {},
                'traffic_info': {},
                'content_preview': 'Clash YAMLè§£æå¤±è´¥',
                'is_base64_decoded': False,
                'is_clash_format': True
            }
    
    def _count_proxy_node(self, proxy: Dict, node_types: Dict):
        """
        ç»Ÿè®¡ä»£ç†èŠ‚ç‚¹ç±»å‹
        
        Args:
            proxy: ä»£ç†èŠ‚ç‚¹é…ç½®å­—å…¸
            node_types: èŠ‚ç‚¹ç±»å‹ç»Ÿè®¡å­—å…¸
        """
        try:
            proxy_type = proxy.get('type', '').lower()
            
            if proxy_type == 'vmess':
                node_types['vmess'] += 1
            elif proxy_type == 'vless':
                node_types['vless'] += 1
            elif proxy_type == 'trojan':
                node_types['trojan'] += 1
            elif proxy_type == 'ss':
                node_types['ss'] += 1
            elif proxy_type == 'ssr':
                node_types['ssr'] += 1
            elif proxy_type == 'http':
                node_types['http'] += 1
            elif proxy_type == 'https':
                node_types['https'] += 1
            elif proxy_type == 'socks5':
                node_types['socks5'] += 1
            else:
                node_types['other'] += 1
                
        except Exception as e:
            logger.debug(f"ç»Ÿè®¡ä»£ç†èŠ‚ç‚¹ç±»å‹å¤±è´¥: {e}")
            node_types['other'] += 1
    
    def _dual_analyze_subscription(self, original_url: str, original_content: bytes) -> Dict:
        """
        åŒé‡åˆ†æè®¢é˜…å†…å®¹ï¼šæ¯”è¾ƒåŸå§‹base64å’Œclashæ ¼å¼ï¼Œå¦‚æœä¸ä¸€è‡´åˆ™ä½¿ç”¨è®¢é˜…è½¬æ¢
        
        Args:
            original_url: åŸå§‹è®¢é˜…URL
            original_content: åŸå§‹å“åº”å†…å®¹
            
        Returns:
            Dict: åŒ…å«åˆ†æç»“æœçš„å­—å…¸
        """
        logger.info("å¼€å§‹åŒé‡åˆ†æè®¢é˜…å†…å®¹")
        
        # æ­¥éª¤1ï¼šåˆ†æåŸå§‹base64å†…å®¹ - é‡æ–°è¯·æ±‚åŸå§‹URL
        logger.info("æ­¥éª¤1ï¼šåˆ†æåŸå§‹base64å†…å®¹")
        original_decoded_content = None
        base64_result = {}
        
        try:
            # é‡æ–°è¯·æ±‚åŸå§‹URLï¼ˆä¸å¸¦clashæ ‡å¿—ï¼‰è·å–çœŸæ­£çš„åŸå§‹å†…å®¹
            logger.info(f"é‡æ–°è¯·æ±‚åŸå§‹URLè·å–base64å†…å®¹: {original_url}")
            response = self.session.get(
                original_url,
                timeout=REQUEST_TIMEOUT,
                allow_redirects=True,
                verify=False
            )
            
            if response.status_code == 200:
                original_content_str = response.content.decode('utf-8', errors='ignore')
                logger.info(f"åŸå§‹å†…å®¹é•¿åº¦: {len(original_content_str)}")
                logger.info(f"åŸå§‹å†…å®¹é¢„è§ˆ: {original_content_str[:200]}...")
                
                original_decoded_content = self._try_base64_decode(original_content_str)
                if original_decoded_content:
                    logger.info(f"åŸå§‹å†…å®¹base64è§£ç æˆåŠŸï¼Œé•¿åº¦: {len(original_decoded_content)}")
                    logger.info(f"è§£ç åå†…å®¹é¢„è§ˆ: {original_decoded_content[:500]}...")
                    # ç›´æ¥åˆ†æè§£ç åçš„å†…å®¹
                    base64_result = self._analyze_decoded_content(original_decoded_content)
                else:
                    logger.info("åŸå§‹å†…å®¹ä¸æ˜¯base64æ ¼å¼æˆ–è§£ç å¤±è´¥")
                    original_decoded_content = original_content_str
                    base64_result = self._analyze_subscription_content(response.content)
            else:
                logger.error(f"é‡æ–°è¯·æ±‚åŸå§‹URLå¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                base64_result = self._analyze_subscription_content(original_content)
                
        except Exception as e:
            logger.error(f"åŸå§‹å†…å®¹å¤„ç†å¤±è´¥: {e}")
            original_decoded_content = ""
            base64_result = self._analyze_subscription_content(original_content)
        
        # æ­¥éª¤2ï¼šè·å–clashæ ¼å¼å†…å®¹
        logger.info("æ­¥éª¤2ï¼šè·å–clashæ ¼å¼å†…å®¹")
        clash_result, clash_content = self._analyze_with_clash_flag_and_content(original_url)
        
        # æ­¥éª¤3ï¼šæ¯”è¾ƒä¸¤ç§å†…å®¹æ˜¯å¦ä¸€è‡´
        content_identical = False
        if clash_content and original_decoded_content:
            # ç®€å•æ¯”è¾ƒå†…å®¹é•¿åº¦å’Œå‰100å­—ç¬¦
            content_identical = (
                abs(len(clash_content) - len(original_decoded_content)) < 100 and
                clash_content[:100] == original_decoded_content[:100]
            )
            logger.info(f"å†…å®¹æ¯”è¾ƒç»“æœ: {'ä¸€è‡´' if content_identical else 'ä¸ä¸€è‡´'}")
            logger.info(f"åŸå§‹å†…å®¹é•¿åº¦: {len(original_decoded_content)}, Clashå†…å®¹é•¿åº¦: {len(clash_content) if clash_content else 0}")
        
        # æ­¥éª¤4ï¼šæ ¹æ®æ¯”è¾ƒç»“æœé€‰æ‹©å¤„ç†æ–¹å¼
        if not content_identical and clash_content:
            logger.info("å†…å®¹ä¸ä¸€è‡´ï¼Œä½¿ç”¨è®¢é˜…åœ°å€è½¬æ¢æœåŠ¡")
            converted_result = self._convert_subscription_with_service(original_url, original_decoded_content)
            if converted_result and converted_result.get('total_nodes', 0) > 0:
                logger.info(f"è®¢é˜…è½¬æ¢æˆåŠŸï¼Œæ‰¾åˆ° {converted_result['total_nodes']} ä¸ªèŠ‚ç‚¹")
                converted_result['analysis_method'] = 'subscription_converter'
                return converted_result
        
        # æ­¥éª¤5ï¼šé€‰æ‹©æœ€ä½³ç»“æœ
        if clash_result and clash_result.get('total_nodes', 0) > 0:
            logger.info(f"ä½¿ç”¨Clashæ ¼å¼ç»“æœï¼Œæ‰¾åˆ° {clash_result['total_nodes']} ä¸ªèŠ‚ç‚¹")
            clash_result['analysis_method'] = 'clash_flag'
            return clash_result
        elif base64_result and base64_result.get('total_nodes', 0) > 0:
            logger.info(f"ä½¿ç”¨base64è§£ç ç»“æœï¼Œæ‰¾åˆ° {base64_result['total_nodes']} ä¸ªèŠ‚ç‚¹")
            base64_result['analysis_method'] = 'base64_decode'
            return base64_result
        
        # éƒ½æ²¡æœ‰æ‰¾åˆ°èŠ‚ç‚¹ï¼Œè¿”å›clashç»“æœï¼ˆå¯èƒ½åŒ…å«æµé‡ä¿¡æ¯ï¼‰
        if clash_result:
            logger.info("æœªæ‰¾åˆ°èŠ‚ç‚¹ï¼Œè¿”å›clashæ ¼å¼ç»“æœï¼ˆå¯èƒ½åŒ…å«æµé‡ä¿¡æ¯ï¼‰")
            clash_result['analysis_method'] = 'clash_flag_fallback'
            return clash_result
        
        # æœ€åè¿”å›base64ç»“æœ
        logger.warning("æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œè¿”å›base64åˆ†æç»“æœ")
        base64_result['analysis_method'] = 'base64_fallback'
        return base64_result
    
    def _analyze_with_clash_flag_and_content(self, original_url: str) -> tuple:
        """
        ä½¿ç”¨&flag=clashå‚æ•°è·å–YAMLæ ¼å¼è¿›è¡Œåˆ†æï¼ŒåŒæ—¶è¿”å›å†…å®¹
        
        Args:
            original_url: åŸå§‹è®¢é˜…URL
            
        Returns:
            tuple: (åˆ†æç»“æœ, å†…å®¹å­—ç¬¦ä¸²)
        """
        try:
            # æ„å»ºclash URL
            if '&flag=clash' not in original_url and '?flag=clash' not in original_url:
                separator = '&' if '?' in original_url else '?'
                clash_url = f"{original_url}{separator}flag=clash"
            else:
                clash_url = original_url
            
            logger.info(f"å°è¯•è·å–Clashæ ¼å¼: {clash_url}")
            
            # è¯·æ±‚clashæ ¼å¼
            response = self.session.get(
                clash_url,
                timeout=REQUEST_TIMEOUT,
                allow_redirects=True,
                verify=False
            )
            
            if response.status_code == 200:
                content_str = response.content.decode('utf-8', errors='ignore')
                logger.info(f"è·å–åˆ°Clashæ ¼å¼å†…å®¹ï¼Œé•¿åº¦: {len(content_str)}")
                
                # åˆ†æYAMLå†…å®¹
                if self._is_clash_yaml_format(content_str):
                    analysis_result = self._analyze_clash_yaml_content(content_str)
                    return analysis_result, content_str
                else:
                    logger.warning("è¿”å›çš„å†…å®¹ä¸æ˜¯æœ‰æ•ˆçš„Clash YAMLæ ¼å¼")
                    return {}, content_str
            else:
                logger.warning(f"è·å–Clashæ ¼å¼å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return {}, None
                
        except Exception as e:
            logger.error(f"ä½¿ç”¨&flag=clashåˆ†æå¤±è´¥: {e}")
            return {}, None
    
    def _convert_subscription_with_service(self, original_url: str, original_content: str) -> Dict:
        """
        ä½¿ç”¨è®¢é˜…è½¬æ¢æœåŠ¡è½¬æ¢è®¢é˜…é“¾æ¥
        
        Args:
            original_url: åŸå§‹è®¢é˜…URL
            original_content: åŸå§‹å†…å®¹
            
        Returns:
            Dict: è½¬æ¢åçš„åˆ†æç»“æœ
        """
        try:
            logger.info("å°è¯•ä½¿ç”¨è®¢é˜…è½¬æ¢æœåŠ¡")
            
            # å¸¸ç”¨çš„è®¢é˜…è½¬æ¢æœåŠ¡
            converter_services = [
                "https://sub.xeton.dev/sub",
                "https://api.dler.io/sub",
                "https://subweb.s3.fr-par.scw.cloud/sub"
            ]
            
            for service_url in converter_services:
                try:
                    # æ„å»ºè½¬æ¢è¯·æ±‚
                    convert_url = f"{service_url}?target=clash&url={original_url}&insert=false&config=https://raw.githubusercontent.com/ACL4SSR/ACL4SSR/master/Clash/config/ACL4SSR_Online.ini"
                    
                    logger.info(f"å°è¯•è½¬æ¢æœåŠ¡: {service_url}")
                    
                    # è¯·æ±‚è½¬æ¢
                    response = self.session.get(
                        convert_url,
                        timeout=REQUEST_TIMEOUT * 2,  # è½¬æ¢æœåŠ¡å¯èƒ½æ¯”è¾ƒæ…¢
                        allow_redirects=True,
                        verify=False
                    )
                    
                    if response.status_code == 200:
                        converted_content = response.content.decode('utf-8', errors='ignore')
                        logger.info(f"è½¬æ¢æœåŠ¡æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(converted_content)}")
                        
                        # åˆ†æè½¬æ¢åçš„å†…å®¹
                        if self._is_clash_yaml_format(converted_content):
                            result = self._analyze_clash_yaml_content(converted_content)
                            if result.get('total_nodes', 0) > 0:
                                logger.info(f"è½¬æ¢æœåŠ¡ {service_url} æˆåŠŸï¼Œæ‰¾åˆ° {result['total_nodes']} ä¸ªèŠ‚ç‚¹")
                                return result
                        else:
                            logger.warning(f"è½¬æ¢æœåŠ¡ {service_url} è¿”å›çš„å†…å®¹ä¸æ˜¯æœ‰æ•ˆçš„Clashæ ¼å¼")
                    else:
                        logger.warning(f"è½¬æ¢æœåŠ¡ {service_url} è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                        
                except Exception as e:
                    logger.error(f"è½¬æ¢æœåŠ¡ {service_url} å¤±è´¥: {e}")
                    continue
            
            logger.warning("æ‰€æœ‰è½¬æ¢æœåŠ¡éƒ½å¤±è´¥")
            return {}
            
        except Exception as e:
            logger.error(f"è®¢é˜…è½¬æ¢å¤±è´¥: {e}")
            return {}
    
    def _analyze_decoded_content(self, content: str) -> Dict:
        """
        ç›´æ¥åˆ†æè§£ç åçš„å†…å®¹ï¼Œä¸“é—¨ç”¨äºèŠ‚ç‚¹é“¾æ¥æ ¼å¼
        
        Args:
            content: è§£ç åçš„å†…å®¹å­—ç¬¦ä¸²
            
        Returns:
            Dict: åŒ…å«åˆ†æç»“æœçš„å­—å…¸
        """
        try:
            logger.info(f"å¼€å§‹åˆ†æè§£ç åå†…å®¹ï¼Œé•¿åº¦: {len(content)}")
            
            # ç»Ÿè®¡èŠ‚ç‚¹æ•°é‡
            node_count = 0
            node_types = {
                'vmess': 0,
                'vless': 0,
                'trojan': 0,
                'ss': 0,
                'ssr': 0,
                'hysteria': 0,
                'hysteria2': 0,
                'http': 0,
                'https': 0,
                'socks5': 0,
                'other': 0
            }
            
            # ç»Ÿè®¡å„ç§åè®®èŠ‚ç‚¹
            for line in content.split('\n'):
                line = line.strip()
                if not line:
                    continue
                    
                if line.startswith('vmess://'):
                    node_count += 1
                    node_types['vmess'] += 1
                elif line.startswith('vless://'):
                    node_count += 1
                    node_types['vless'] += 1
                elif line.startswith('trojan://'):
                    node_count += 1
                    node_types['trojan'] += 1
                elif line.startswith('ss://'):
                    node_count += 1
                    node_types['ss'] += 1
                elif line.startswith('ssr://'):
                    node_count += 1
                    node_types['ssr'] += 1
                elif line.startswith('hysteria2://'):
                    node_count += 1
                    node_types['hysteria2'] += 1
                    logger.debug(f"å‘ç°hysteria2èŠ‚ç‚¹: {line[:100]}...")
                elif line.startswith('hysteria://'):
                    node_count += 1
                    node_types['hysteria'] += 1
                    logger.debug(f"å‘ç°hysteriaèŠ‚ç‚¹: {line[:100]}...")
                elif line.startswith('http://'):
                    node_count += 1
                    node_types['http'] += 1
                elif line.startswith('https://'):
                    node_count += 1
                    node_types['https'] += 1
                elif line.startswith('socks5://'):
                    node_count += 1
                    node_types['socks5'] += 1
                elif self._is_valid_node_line(line):
                    # å…¶ä»–æœ‰æ•ˆçš„èŠ‚ç‚¹è¡Œ
                    node_count += 1
                    node_types['other'] += 1
            
            logger.info(f"è§£ç å†…å®¹æ£€æµ‹åˆ°èŠ‚ç‚¹æ•°é‡: {node_count}")
            logger.info(f"èŠ‚ç‚¹ç±»å‹åˆ†å¸ƒ: {node_types}")
            
            # å°è¯•æå–æµé‡ä¿¡æ¯
            traffic_info = self._extract_traffic_info(content)
            
            return {
                'total_nodes': node_count,
                'node_types': node_types,
                'traffic_info': traffic_info,
                'content_preview': content[:200] + '...' if len(content) > 200 else content,
                'is_base64_decoded': True,
                'is_clash_format': False,
                'analysis_method': 'base64_decode'
            }
            
        except Exception as e:
            logger.error(f"è§£ç å†…å®¹åˆ†æå¤±è´¥: {e}")
            return {
                'total_nodes': 0,
                'node_types': {},
                'traffic_info': {},
                'content_preview': 'è§£ç å†…å®¹åˆ†æå¤±è´¥',
                'is_base64_decoded': True,
                'is_clash_format': False,
                'analysis_method': 'base64_decode_error'
            }

    def send_dingtalk_notification(self, result: Dict) -> bool:
        """
        å‘é€é’‰é’‰é€šçŸ¥ï¼ˆç²¾ç®€ç‰ˆï¼‰
        
        Args:
            result: æ£€æµ‹ç»“æœå­—å…¸
            
        Returns:
            bool: å‘é€æ˜¯å¦æˆåŠŸ
        """
        try:
            # æå–å¹¶æ¸…ç†è®¢é˜…é“¾æ¥
            raw_url = result['url']
            
            # ä½¿ç”¨URLæå–å™¨è¿›è¡Œæ¸…ç†
            from url_extractor import URLExtractor
            extractor = URLExtractor()
            
            # æå–è®¢é˜…é“¾æ¥
            urls = extractor.extract_subscription_urls(raw_url)
            if urls:
                # é€‰æ‹©æœ€é•¿çš„URLï¼ˆé€šå¸¸åŒ…å«æ›´å¤šå‚æ•°ï¼‰
                clean_url = max(urls, key=len)
            else:
                # å¦‚æœæå–å¤±è´¥ï¼Œä½¿ç”¨ç®€å•çš„æ¸…ç†æ–¹æ³•
                clean_url = raw_url
                # å…ˆå¤„ç†HTMLå®ä½“ç¼–ç 
                import html
                clean_url = html.unescape(clean_url)
                # ç§»é™¤HTMLæ ‡ç­¾
                clean_url = re.sub(r'<[^>]+>', '', clean_url)
                # ç§»é™¤å¤šä½™æ–‡æœ¬
                clean_url = re.sub(r'^[^h]*?(https?://)', r'\1', clean_url)
                clean_url = re.sub(r'<br/?>.*$', '', clean_url)
                clean_url = re.sub(r'<div[^>]*>.*$', '', clean_url)
                clean_url = clean_url.strip()
            
            # ç§»é™¤clashæ ‡å¿—
            if '&flag=clash' in clean_url:
                clean_url = clean_url.replace('&flag=clash', '')
            if '?flag=clash' in clean_url:
                clean_url = clean_url.replace('?flag=clash', '')
            
            # æ ‡å‡†åŒ–URLç”¨äºé‡å¤æ£€æŸ¥
            normalized_url = self.normalize_url(clean_url)
            
            # æ£€æŸ¥æ˜¯å¦å·²å‘é€è¿‡é€šçŸ¥ï¼ˆä½¿ç”¨æ ‡å‡†åŒ–URLï¼‰
            if normalized_url in self.notified_urls:
                logger.info(f"è®¢é˜…é“¾æ¥å·²å‘é€è¿‡é’‰é’‰é€šçŸ¥ï¼Œè·³è¿‡: {normalized_url}")
                return True
            
            # åªå‘é€å¯ç”¨çš„è®¢é˜…é“¾æ¥é€šçŸ¥
            if not result['available']:
                logger.debug(f"è®¢é˜…é“¾æ¥ä¸å¯ç”¨ï¼Œè·³è¿‡é’‰é’‰é€šçŸ¥: {clean_url}")
                return True
            
            # æ„å»ºæµé‡ä¿¡æ¯
            traffic_text = "æœªçŸ¥"
            total_traffic_text = "æœªçŸ¥"
            if result.get('traffic_info'):
                traffic = result['traffic_info']
                if traffic.get('remaining_traffic'):
                    traffic_text = f"å‰©ä½™ {traffic['remaining_traffic']} {traffic.get('traffic_unit', 'GB')}"
                if traffic.get('total_traffic'):
                    total_traffic_text = f"æ€»é‡ {traffic['total_traffic']} {traffic.get('traffic_unit', 'GB')}"
                    if not traffic.get('remaining_traffic'):
                        traffic_text = total_traffic_text
            
            # æ„å»ºèŠ‚ç‚¹ä¿¡æ¯å’Œåè®®ä¿¡æ¯
            node_count = 0
            protocols_text = "æœªçŸ¥"
            if result.get('node_analysis'):
                analysis = result['node_analysis']
                node_count = analysis.get('total_nodes', 0)
                
                # è·å–åè®®ç»Ÿè®¡
                node_types = analysis.get('node_types', {})
                if node_types:
                    protocol_list = []
                    for protocol, count in node_types.items():
                        if count > 0:
                            protocol_list.append(f"{protocol}({count})")
                    protocols_text = ", ".join(protocol_list) if protocol_list else "æœªçŸ¥"
            
            title = "âœ… å‘ç°å¯ç”¨è®¢é˜…"
            
            # è·å–åˆ†ææ–¹æ³•ä¿¡æ¯
            analysis_method = "æœªçŸ¥"
            if result.get('node_analysis'):
                method = result['node_analysis'].get('analysis_method', 'unknown')
                if method == 'clash_flag':
                    analysis_method = "Clashæ ¼å¼"
                elif method == 'base64_decode':
                    analysis_method = "Base64è§£ç "
                elif method == 'subscription_converter':
                    analysis_method = "è®¢é˜…è½¬æ¢"
                elif method in ['clash_flag_fallback', 'base64_fallback']:
                    analysis_method = f"å¤‡ç”¨æ–¹æ¡ˆ({method.split('_')[0]})"
            
            # åˆ†æˆä¸‰æ¡æ¶ˆæ¯å‘é€ï¼š1. Linkå¡ç‰‡  2. @allæé†’  3. åˆ†æç»“æœ
            
            # ç¬¬ä¸€æ¡æ¶ˆæ¯ï¼šLinkå¡ç‰‡ï¼ˆæ”¯æŒå¤åˆ¶åŠŸèƒ½ï¼‰
            link_message = {
                "msgtype": "link",
                "link": {
                    "title": "âœ… å‘ç°å¯ç”¨è®¢é˜…",
                    "text": f"èŠ‚ç‚¹: {node_count} ä¸ª | åè®®: {protocols_text} | å‰©ä½™: {traffic_text}",
                    "messageUrl": clean_url,
                    "picUrl": "https://img.alicdn.com/tfs/TB1NwmBEL9TBuNjy1zbXXXpepXa-2400-1218.png"
                }
            }
            
            # ç¬¬äºŒæ¡æ¶ˆæ¯ï¼š@allæé†’ï¼ˆåŒ…å«URLï¼‰
            url_message = {
                "msgtype": "text",
                "text": {
                    "content": f":{clean_url}"
                },
                "at": {
                    "isAtAll": True  # @æ‰€æœ‰äººï¼Œæé†’ç¡®è®¤æ”¶åˆ°
                }
            }
            
            # ç¬¬ä¸‰æ¡æ¶ˆæ¯ï¼šåˆ†æç»“æœï¼ˆæ·»åŠ æ—¶é—´ä¿¡æ¯ï¼‰
            from datetime import datetime
            current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            
            info_content = f"""{title}

ğŸ“Š åˆ†æç»“æœï¼š
â€¢ çŠ¶æ€: âœ… å¯ç”¨
â€¢ èŠ‚ç‚¹: {node_count} ä¸ª
â€¢ åè®®: {protocols_text}
â€¢ å‰©ä½™: {traffic_text}
â€¢ æ€»é‡: {total_traffic_text}
â€¢ æ–¹å¼: {analysis_method}
â€¢ å“åº”: {result.get('status_code', 'N/A')}

â° å‘ç°æ—¶é—´: {current_time}
ğŸ¤– è‡ªåŠ¨æ£€æµ‹ç³»ç»Ÿ"""
            
            info_message = {
                "msgtype": "text",
                "text": {
                    "content": info_content
                },
                "at": {
                    "isAtAll": False
                }
            }
            
            # å‘é€ç¬¬ä¸€æ¡æ¶ˆæ¯ï¼ˆLinkå¡ç‰‡ï¼‰
            response1 = requests.post(
                DINGTALK_WEBHOOK,
                json=link_message,
                timeout=10,
                headers={'Content-Type': 'application/json'}
            )
            
            # ç¨ä½œå»¶è¿Ÿï¼Œç¡®ä¿æ¶ˆæ¯é¡ºåº
            import time
            time.sleep(0.5)
            
            # å‘é€ç¬¬äºŒæ¡æ¶ˆæ¯ï¼ˆ@allæé†’ï¼‰
            response2 = requests.post(
                DINGTALK_WEBHOOK,
                json=url_message,
                timeout=10,
                headers={'Content-Type': 'application/json'}
            )
            
            # ç¨ä½œå»¶è¿Ÿï¼Œç¡®ä¿æ¶ˆæ¯é¡ºåº
            time.sleep(0.5)
            
            # å‘é€ç¬¬ä¸‰æ¡æ¶ˆæ¯ï¼ˆåˆ†æç»“æœï¼‰
            response3 = requests.post(
                DINGTALK_WEBHOOK,
                json=info_message,
                timeout=10,
                headers={'Content-Type': 'application/json'}
            )
            
            # æ£€æŸ¥ä¸‰æ¡æ¶ˆæ¯çš„å‘é€ç»“æœ
            success1 = response1.status_code == 200 and response1.json().get('errcode') == 0
            success2 = response2.status_code == 200 and response2.json().get('errcode') == 0
            success3 = response3.status_code == 200 and response3.json().get('errcode') == 0
            
            if success1 and success2 and success3:
                logger.info("é’‰é’‰é€šçŸ¥å‘é€æˆåŠŸï¼ˆLinkå¡ç‰‡ + @allæé†’ + åˆ†æç»“æœï¼‰")
                logger.info(f"Linkå¡ç‰‡æ”¯æŒå¤åˆ¶ï¼Œ@allæé†’ç¡®è®¤æ”¶åˆ°")
                
                # ä½¿ç”¨ä¸“ç”¨çš„æ—¥å¿—è®°å½•æ–¹æ³•è®°å½•è¯¦ç»†ä¿¡æ¯
                daily_logger.log_subscription_found(clean_url, result)
                daily_logger.log_dingtalk_sent(clean_url, True)
                
                # è®°å½•å·²å‘é€é€šçŸ¥çš„URLå¹¶ä¿å­˜åˆ°æ–‡ä»¶ï¼ˆä½¿ç”¨æ ‡å‡†åŒ–URLï¼‰
                self.notified_urls.add(normalized_url)
                self._save_notified_urls()
                logger.info(f"å·²è®°å½•é€šçŸ¥URL: {normalized_url}")
                
                return True
            else:
                if not success1:
                    logger.error(f"Linkå¡ç‰‡æ¶ˆæ¯å‘é€å¤±è´¥: {response1.status_code} - {response1.json() if response1.status_code == 200 else 'HTTPé”™è¯¯'}")
                
                if not success2:
                    logger.error(f"@allæé†’æ¶ˆæ¯å‘é€å¤±è´¥: {response2.status_code} - {response2.json() if response2.status_code == 200 else 'HTTPé”™è¯¯'}")
                
                if not success3:
                    logger.error(f"åˆ†æç»“æœæ¶ˆæ¯å‘é€å¤±è´¥: {response3.status_code} - {response3.json() if response3.status_code == 200 else 'HTTPé”™è¯¯'}")
                
                return False
                
        except Exception as e:
            logger.error(f"å‘é€é’‰é’‰é€šçŸ¥å¤±è´¥: {e}")
            return False
    
    def send_serpapi_usage_notification(self, current_usage: int, total_quota: int, quotas_detail: List[Dict] = None) -> bool:
        """
        å‘é€SerpAPIä½¿ç”¨é‡é˜ˆå€¼é€šçŸ¥
        
        Args:
            current_usage: å½“å‰å·²ä½¿ç”¨é‡
            total_quota: æ€»é…é¢
            quotas_detail: è¯¦ç»†é…é¢ä¿¡æ¯åˆ—è¡¨
            
        Returns:
            bool: æ˜¯å¦å‘é€æˆåŠŸ
        """
        try:
            from datetime import datetime
            current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            
            # è®¡ç®—ä½¿ç”¨ç‡
            usage_percentage = (current_usage / total_quota * 100) if total_quota > 0 else 0
            
            # å®šä¹‰é˜ˆå€¼åˆ—è¡¨ (10%, 20%, 30%, ...)
            thresholds = [10, 20, 30, 40, 50, 60, 70, 80, 90, 95]
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æŸä¸ªé˜ˆå€¼
            reached_threshold = None
            for threshold in thresholds:
                if usage_percentage >= threshold:
                    reached_threshold = threshold
                else:
                    break
            
            # åªæœ‰è¾¾åˆ°é˜ˆå€¼æ‰å‘é€é€šçŸ¥
            if reached_threshold is None:
                logger.debug(f"SerpAPIä½¿ç”¨ç‡ {usage_percentage:.1f}% æœªè¾¾åˆ°é€šçŸ¥é˜ˆå€¼")
                return True
            
            # è®¡ç®—å¯†é’¥ç»Ÿè®¡ä¿¡æ¯
            total_keys = 0
            available_keys = 0
            failed_keys = 0
            key_details = []
            
            if quotas_detail and isinstance(quotas_detail, list):
                total_keys = len(quotas_detail)
                for i, quota_info in enumerate(quotas_detail, 1):
                    if quota_info and isinstance(quota_info, dict):
                        used = quota_info.get('this_month_usage', 0)
                        quota = quota_info.get('searches_per_month', 0)
                        
                        if quota > 0:
                            available_keys += 1
                            usage_rate = (used / quota * 100) if quota > 0 else 0
                            remaining = quota - used
                            
                            # è®¡ç®—ä¸‹æ¬¡é‡ç½®æ—¶é—´
                            next_reset_date = self._calculate_next_reset_date(quota_info, i)
                            
                            # æ ¹æ®ä½¿ç”¨æƒ…å†µæ·»åŠ çŠ¶æ€æ ‡è¯†
                            if usage_rate >= 90:
                                status = "âš ï¸ å³å°†è€—å°½"
                            elif usage_rate >= 70:
                                status = "âš ï¸ ä½¿ç”¨è¾ƒå¤š"
                            else:
                                status = "âœ… æ­£å¸¸ä½¿ç”¨"
                            
                            key_details.append(f"â€¢ å¯†é’¥{i}: {used}/{quota} ({usage_rate:.1f}% å·²ç”¨) - {status}")
                            key_details.append(f"  ğŸ“… ä¸‹æ¬¡é‡ç½®: {next_reset_date}")
                        else:
                            failed_keys += 1
                            key_details.append(f"â€¢ å¯†é’¥{i}: âŒ å¤±æ•ˆæˆ–æ— æ³•è®¿é—®")
            
            # æ„å»ºé€šçŸ¥å†…å®¹
            usage_content = f"""{current_time}
ğŸ¯ SerpAPIä½¿ç”¨é‡æé†’
ä½¿ç”¨ç‡å·²è¾¾åˆ° {reached_threshold}% é˜ˆå€¼

ğŸ“ˆ æ±‡æ€»ä¿¡æ¯
â€¢ âœ… å¯ç”¨å¯†é’¥: {available_keys}/{total_keys} {'(å…¨éƒ¨å¯ç”¨)' if available_keys == total_keys else ''}
â€¢ âŒ å¤±æ•ˆå¯†é’¥: {failed_keys}/{total_keys} {'(æ— å¤±æ•ˆå¯†é’¥)' if failed_keys == 0 else ''}

ğŸ’° é¢åº¦æ±‡æ€»
â€¢ ğŸ¯ æ€»å‰©ä½™æœç´¢æ¬¡æ•°: {total_quota - current_usage}æ¬¡
â€¢ ğŸ“… æ€»æœˆåº¦é™åˆ¶: {total_quota}æ¬¡
â€¢ ğŸ“Š æ€»å·²ä½¿ç”¨: {current_usage}æ¬¡
â€¢ ğŸ“ˆ æ€»ä½“ä½¿ç”¨ç‡: {usage_percentage:.1f}%"""

            if key_details:
                usage_content += "\n\nğŸ”‘ å„å¯†é’¥ä½¿ç”¨æƒ…å†µ\n" + "\n".join(key_details)

            usage_content += f"""

âš ï¸ ä½¿ç”¨é‡é˜ˆå€¼æé†’
â€¢ å½“å‰ä½¿ç”¨ç‡å·²è¾¾åˆ° {reached_threshold}% ç›‘æ§é˜ˆå€¼
â€¢ è¯·æ³¨æ„APIé…é¢ä½¿ç”¨æƒ…å†µ
â€¢ å»ºè®®åˆç†å®‰æ’æœç´¢é¢‘ç‡

ğŸ¤– è‡ªåŠ¨ç›‘æ§ç³»ç»Ÿ"""

            # æ„å»ºé’‰é’‰æ¶ˆæ¯
            usage_message = {
                "msgtype": "text",
                "text": {
                    "content": f":{usage_content}"
                },
                "at": {
                    "isAtAll": False
                }
            }

            # å‘é€é’‰é’‰é€šçŸ¥
            import requests
            response = requests.post(
                self.dingtalk_webhook,
                json=usage_message,
                timeout=10
            )
            
            if response.status_code == 200:
                result = response.json()
                if result.get('errcode') == 0:
                    logger.info(f"âœ… SerpAPIä½¿ç”¨é‡é˜ˆå€¼é€šçŸ¥å‘é€æˆåŠŸ (ä½¿ç”¨ç‡: {usage_percentage:.1f}%, é˜ˆå€¼: {reached_threshold}%)")
                    return True
                else:
                    logger.error(f"âŒ SerpAPIä½¿ç”¨é‡é˜ˆå€¼é€šçŸ¥å‘é€å¤±è´¥: {result}")
                    return False
            else:
                logger.error(f"âŒ SerpAPIä½¿ç”¨é‡é˜ˆå€¼é€šçŸ¥HTTPé”™è¯¯: {response.status_code}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ SerpAPIä½¿ç”¨é‡é˜ˆå€¼é€šçŸ¥å‘é€å¼‚å¸¸: {e}")
            return False
    
    def check_multiple_subscriptions(self, urls: List[str]) -> List[Dict]:
        """
        æ‰¹é‡æ£€æµ‹å¤šä¸ªè®¢é˜…é“¾æ¥
        
        Args:
            urls: è®¢é˜…é“¾æ¥åˆ—è¡¨
            
        Returns:
            List[Dict]: æ£€æµ‹ç»“æœåˆ—è¡¨
        """
        # å»é™¤é‡å¤URL
        unique_urls, duplicate_mapping = self.remove_duplicate_urls(urls)
        
        # æ‰“å°é‡å¤åˆ†æç»“æœ
        self.print_duplicate_analysis(urls, duplicate_mapping)
        
        if not unique_urls:
            logger.warning("å»é‡åæ²¡æœ‰æœ‰æ•ˆçš„è®¢é˜…é“¾æ¥")
            return []
        
        results = []
        
        for i, url in enumerate(unique_urls, 1):
            logger.info(f"æ­£åœ¨æ£€æµ‹ç¬¬ {i}/{len(unique_urls)} ä¸ªè®¢é˜…é“¾æ¥ (å»é‡å)")
            result = self.check_subscription_url(url.strip())
            results.append(result)
            
            # å¦‚æœå¯ç”¨ï¼Œå‘é€é’‰é’‰é€šçŸ¥
            if result['available']:
                self.send_dingtalk_notification(result)
            
            # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            if i < len(unique_urls):
                time.sleep(1)
        
        return results
    
    def save_results(self, results: List[Dict], filename: Optional[str] = None):
        """
        ä¿å­˜æ£€æµ‹ç»“æœåˆ°æ–‡ä»¶
        
        Args:
            results: æ£€æµ‹ç»“æœåˆ—è¡¨
            filename: æ–‡ä»¶åï¼ˆå¯é€‰ï¼‰
        """
        if filename is None:
            timestamp = time.strftime('%Y%m%d_%H%M%S')
            filename = f'subscription_check_results_{timestamp}.json'
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            logger.info(f"æ£€æµ‹ç»“æœå·²ä¿å­˜åˆ°: {filename}")
        except Exception as e:
            logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("è®¢é˜…é“¾æ¥å¯ç”¨æ€§æ£€æµ‹å·¥å…·")
    print("=" * 60)
    
    # åˆ›å»ºæ£€æµ‹å™¨
    checker = SubscriptionChecker()
    
    # æµ‹è¯•ä»£ç†è¿æ¥
    proxy_test_result = checker.test_proxy()
    checker.print_proxy_test_result(proxy_test_result)

    # è·å–ç”¨æˆ·è¾“å…¥
    print("\nè¯·è¾“å…¥è®¢é˜…é“¾æ¥ï¼ˆæ¯è¡Œä¸€ä¸ªï¼Œè¾“å…¥ç©ºè¡Œç»“æŸï¼‰:")
    urls = []
    
    while True:
        url = input().strip()
        if not url:
            break
        urls.append(url)
    
    if not urls:
        print("æœªè¾“å…¥ä»»ä½•è®¢é˜…é“¾æ¥ï¼Œç¨‹åºé€€å‡ºã€‚")
        return
    
    print(f"\nå¼€å§‹æ£€æµ‹ {len(urls)} ä¸ªè®¢é˜…é“¾æ¥...")
    
    # æ‰§è¡Œæ£€æµ‹ï¼ˆè‡ªåŠ¨å»é‡ï¼‰
    results = checker.check_multiple_subscriptions(urls)
    
    if not results:
        print("æ²¡æœ‰æœ‰æ•ˆçš„è®¢é˜…é“¾æ¥éœ€è¦æ£€æµ‹ã€‚")
        return
    
    # æ˜¾ç¤ºç»“æœæ‘˜è¦
    print("\n" + "=" * 60)
    print("æ£€æµ‹ç»“æœæ‘˜è¦")
    print("=" * 60)
    
    available_count = sum(1 for r in results if r['available'])
    total_count = len(results)
    
    print(f"æ£€æµ‹çš„é“¾æ¥æ•°: {total_count} (å·²è‡ªåŠ¨å»é‡)")
    print(f"å¯ç”¨é“¾æ¥: {available_count}")
    print(f"ä¸å¯ç”¨é“¾æ¥: {total_count - available_count}")
    
    # æ˜¾ç¤ºè¯¦ç»†ç»“æœ
    print("\nè¯¦ç»†ç»“æœ:")
    for i, result in enumerate(results, 1):
        status_icon = "âœ…" if result['available'] else "âŒ"
        print(f"{i}. {status_icon} {result['url']}")
        print(f"   çŠ¶æ€: {result['status']}")
        if result['error']:
            print(f"   é”™è¯¯: {result['error']}")
        if result['available']:
            print(f"   å“åº”æ—¶é—´: {result['response_time']}ç§’")
            
            # æ˜¾ç¤ºèŠ‚ç‚¹ä¿¡æ¯
            if result.get('node_analysis'):
                analysis = result['node_analysis']
                print(f"   èŠ‚ç‚¹æ•°é‡: {analysis.get('total_nodes', 0)} ä¸ª")
                
                # æ˜¾ç¤ºèŠ‚ç‚¹ç±»å‹åˆ†å¸ƒ
                node_types = analysis.get('node_types', {})
                active_types = [f"{k.upper()}:{v}" for k, v in node_types.items() if v > 0]
                if active_types:
                    print(f"   èŠ‚ç‚¹ç±»å‹: {', '.join(active_types)}")
                
                # æ˜¾ç¤ºæ ¼å¼ä¿¡æ¯
                if analysis.get('is_clash_format'):
                    print(f"   æ ¼å¼: Clash YAML")
                elif analysis.get('is_base64_decoded'):
                    print(f"   æ ¼å¼: Base64ç¼–ç ")
                else:
                    print(f"   æ ¼å¼: åŸå§‹æ ¼å¼")
            
            # æ˜¾ç¤ºæµé‡ä¿¡æ¯
            if result.get('traffic_info'):
                traffic = result['traffic_info']
                traffic_details = []
                
                if traffic.get('total_traffic'):
                    traffic_details.append(f"æ€»æµé‡: {traffic['total_traffic']} {traffic['traffic_unit']}")
                if traffic.get('remaining_traffic'):
                    traffic_details.append(f"å‰©ä½™æµé‡: {traffic['remaining_traffic']} {traffic['traffic_unit']}")
                if traffic.get('expire_date'):
                    traffic_details.append(f"è¿‡æœŸæ—¶é—´: {traffic['expire_date']}")
                
                if traffic_details:
                    print(f"   æµé‡ä¿¡æ¯: {' | '.join(traffic_details)}")
        
        print()
    
    # ä¿å­˜ç»“æœ
    checker.save_results(results)
    
    print("æ£€æµ‹å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°JSONæ–‡ä»¶ä¸­ã€‚")


if __name__ == "__main__":
    main()
